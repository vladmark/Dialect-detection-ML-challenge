{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8Ot0frmdm5dW",
        "t69nd7sWplFh",
        "GzUph8STwuJx",
        "bO8_aQS5wwqC",
        "fQSfLRIn5Zj0",
        "97FmWzaD4Fvq",
        "6-fzuIaZiPBi",
        "v6HB2fLhR9Aj",
        "vagDjDyEhUp-",
        "-AXUclj3bPon"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOXqQqmm6DjXXH7kEbnJUvz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vladmark/Dialect-detection-ML-challenge/blob/main/Dialect%20Detection%20Challenge%20Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1_jwDvM2r4X"
      },
      "source": [
        "# Preliminarii: imports, gdrive setup etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWSJpsyKqHjH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "c25ba5fd-b408-458d-cc21-1cbcceb816e7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeJt9GIdGMJQ"
      },
      "source": [
        "# ! pip install torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import json\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import preprocessing\n",
        "import torch.nn.functional as F\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "# nltk.download()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoznneWrTkWw"
      },
      "source": [
        "\n",
        "torch.manual_seed(1)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bayiXxMv22of"
      },
      "source": [
        "# Partea I: Aducerea de date la un format cu care se poate lucra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKnOuY3w1z9q"
      },
      "source": [
        "##**Procesare de raw data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtblVl1jVZnw"
      },
      "source": [
        "Functia de procesat date din fisier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtshr8_PVfqW"
      },
      "source": [
        "def get_data(location, type ='samplefile'):\n",
        "    with open(location, encoding=\"utf8\") as file:\n",
        "        if(type=='samplefile'):\n",
        "            data={}\n",
        "        elif(type=='labelfile'):\n",
        "            data=torch.empty(1,2)\n",
        "            i=0\n",
        "        elif(type == 'labelfile_no_OneHot'):\n",
        "            data={}\n",
        "        for line in file:\n",
        "            id = line[:line.find(\"\t\")]\n",
        "            text = line[line.find(\"\t\")+1:]\n",
        "            if(type=='samplefile'):\n",
        "                # text = re.sub(\"(\\\\d|\\\\W)+\", \" \", text)\n",
        "                # text = text.lower()\n",
        "                # text = text.split()\n",
        "                # text = list(text)\n",
        "                # for word in stopwords.words('romanian'):\n",
        "                #     if (word in text):\n",
        "                #         text.remove(word)\n",
        "                # text = ' '.join(text)\n",
        "                # id = int(id)\n",
        "                # text=text.replace(\"+\", \" \")\n",
        "                # text=text.replace(\"-\", \" \")\n",
        "                # text=text.replace(\":\", \" \")\n",
        "                # text=text.replace(\";\", \" \")\n",
        "                # text=text.split(\" \")\n",
        "                data[id] = text\n",
        "            elif (type=='labelfile'):\n",
        "                if i==0:\n",
        "                    data[i,:]=torch.Tensor([int(id), int(text)])\n",
        "                    i = i + 1\n",
        "                else:\n",
        "                    data=torch.cat((data, torch.Tensor([[int(id), int(text)]]) ), dim=0)\n",
        "            elif (type == 'labelfile_no_OneHot'):\n",
        "                data[id]=int(text)\n",
        "        if (type == 'labelfile'):\n",
        "            enc = preprocessing.OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "            encoded_labels = enc.fit_transform(data[:, 1].type(torch.IntTensor).reshape(-1,1) )\n",
        "            dict={}\n",
        "            for i in range(data.shape[0]):\n",
        "                dict[int(data[i, 0].item())]=[int(encoded_labels[i,0]), int(encoded_labels[i,1])]\n",
        "            return(dict)\n",
        "        elif (type == 'labelfile_no_OneHot'):\n",
        "            return data\n",
        "        elif (type == 'samplefile'):\n",
        "            return data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51d1CC09Vivt"
      },
      "source": [
        "Functii de manipulat json:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nw-0P_wZVlzy"
      },
      "source": [
        "def write_to_file(data, filename):\n",
        "    jsonloc = json.dumps(data)\n",
        "    f = open(filename,\"w\")\n",
        "    f.write(jsonloc)\n",
        "    f.close()\n",
        "\n",
        "def get_data_from_file(filename):\n",
        "    f=open(filename,\"r\")\n",
        "    data=json.load(f)\n",
        "    f.close()\n",
        "    return(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo3ieYdkkF9c"
      },
      "source": [
        "basepath='/content/drive/My Drive/Proiect IA/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFCRiwNLVoqu"
      },
      "source": [
        "Creeare de jsonuri **daca nu sunt create deja**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAwsjKAlVqWt"
      },
      "source": [
        "train_data=get_data(basepath+'train_samples.txt')\n",
        "dict_train_labels=get_data(basepath+'train_labels.txt', type='labelfile_no_OneHot')\n",
        "write_to_file(train_data, basepath+\"dict_train_data.json\")\n",
        "write_to_file(dict_train_labels, basepath+\"dict_train_labels.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnHKrengBcS7"
      },
      "source": [
        "validation_data = get_data(basepath+'validation_samples.txt')\n",
        "dict_validation_labels = get_data(basepath+'validation_labels.txt', type='labelfile_no_OneHot')\n",
        "write_to_file(validation_data, basepath+\"dict_validation_data.json\")\n",
        "write_to_file(dict_validation_labels, basepath+\"dict_validation_labels.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxuZCrEjCvew"
      },
      "source": [
        "test_data=get_data(basepath+'test_samples.txt')\n",
        "write_to_file(test_data, basepath+\"dict_test_data.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIKybeqJXhZc"
      },
      "source": [
        "Incarcare din jsonuri **daca exista deja**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wY6nHvXEWkNG"
      },
      "source": [
        "train_data=get_data_from_file(basepath+\"dict_train_data.json\")\n",
        "validation_data = get_data_from_file(basepath+\"dict_validation_data.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgQp0wIm5v4r"
      },
      "source": [
        "dict_train_labels=get_data_from_file(basepath+\"dict_train_labels.json\")\n",
        "dict_validation_labels = get_data_from_file(basepath+\"dict_validation_labels.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ndt918OC4Zv"
      },
      "source": [
        "test_data=get_data_from_file(basepath+\"dict_test_data.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-GFKWWJY9IR"
      },
      "source": [
        "\n",
        "Procesare labeluri."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNONaCViY8cy"
      },
      "source": [
        "train_labels = torch.LongTensor([dict_train_labels[key] for key in dict_train_labels.keys()])\n",
        "validation_labels = torch.LongTensor([dict_validation_labels[key] for key in dict_validation_labels.keys()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwHkvTUGNQOm"
      },
      "source": [
        "train_textlist=[train_data[key].split() for key in train_data.keys()]\n",
        "validation_textlist=[validation_data[key].split() for key in validation_data.keys()]\n",
        "test_textlist=[test_data[key].split() for key in test_data.keys()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ot0frmdm5dW"
      },
      "source": [
        "##**Observatii pe date**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t69nd7sWplFh"
      },
      "source": [
        "###Basics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_N7MMYJNpoDn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "27d11ad1-2c33-4819-9d76-01c2d9f99316"
      },
      "source": [
        "print(f'numarul de texte de train: {len(train_textlist)}')\n",
        "print(f'numarul de texte de validare: {len(validation_textlist)}')\n",
        "print(f'numarul de texte de test: {len(test_textlist)}')\n",
        "print(f'exemplu de text: {train_textlist[46]}' )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numarul de texte de train: 7757\n",
            "numarul de texte de validare: 2656\n",
            "numarul de texte de test: 2623\n",
            "exemplu de text: ['|eF!', 'a:Z@', 'pg', '@<wtq', 'c=.HUz', 'fhk#', 'XBvC', 'rl#:', '|@UD', '&e', 'RElTp', 'em@#', 'op!', 'AxZF', 'i&:', 'mk@kchk', 'gkea&hk', 'pyXA', \"'Bh\", 'hkm}hf@m', 'mghZ', 'AgXwB', '*cp>h', '@:fh']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzUph8STwuJx"
      },
      "source": [
        "### Statistici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muYon5YyDKdY"
      },
      "source": [
        "Numarul de exemple din fiecare clasa pentru setul de train si setul de validare."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNv1vy-dnBCO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d9d8845b-08ef-4ab9-80e4-f560081d52f0"
      },
      "source": [
        "no_labels_train=torch.FloatTensor([torch.sum(train_labels == 1.), torch.sum(train_labels == 0.)])*(1/train_labels.shape[0])\n",
        "no_labels_validation=torch.FloatTensor([torch.sum(validation_labels == 1.), torch.sum(validation_labels == 0.)])*(1/validation_labels.shape[0])\n",
        "print(f'distributia claselor pe train: {no_labels_train}')\n",
        "print(f'distributia claselor pe validare: {no_labels_validation}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "distributia claselor pe train: tensor([0.5068, 0.4932])\n",
            "distributia claselor pe validare: tensor([0.5102, 0.4898])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya8vITQfDYyI"
      },
      "source": [
        "Lungimile textelor de train/validare/test; lungimiile medii ale textelor si variantele lungimilor textelor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLFPgVYtorPJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "7dd71e38-b157-4f9a-c134-b865a37072c5"
      },
      "source": [
        "train_text_lengths=torch.FloatTensor([len(train_data[key]) for key in train_data.keys()])\n",
        "validation_text_lengths=torch.FloatTensor([len(validation_data[key]) for key in validation_data.keys()])\n",
        "test_text_lengths=torch.FloatTensor([len(test_data[key]) for key in test_data.keys()])\n",
        "print(f'lungimile textelor de train: {train_text_lengths}')\n",
        "print(f'lungimile textelor de validare: {validation_text_lengths}')\n",
        "print(f'lungimile textelor de test: {test_text_lengths}')\n",
        "print(f'media lungimilor textelor de train: {torch.mean(train_text_lengths)}')\n",
        "print(f'media lungimilor textelor de validare: {torch.mean(validation_text_lengths)}')\n",
        "print(f'media lungimilor textelor de test: {torch.mean(test_text_lengths)}')\n",
        "print(f'deviatia standard a textelor de train: {torch.std(train_text_lengths)}')\n",
        "print(f'deviatia standard a textelor de validare: {torch.std(validation_text_lengths)}')\n",
        "print(f'deviatia standard a lungimilor textelor de test: {torch.std(test_text_lengths)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lungimile textelor de train: tensor([349.,  90., 169.,  ..., 785.,  88., 207.])\n",
            "lungimile textelor de validare: tensor([165., 214., 205.,  ..., 165., 305., 180.])\n",
            "lungimile textelor de test: tensor([124., 273., 233.,  ...,  63., 148., 211.])\n",
            "media lungimilor textelor de train: 182.95204162597656\n",
            "media lungimilor textelor de validare: 186.8911895751953\n",
            "media lungimilor textelor de test: 176.7014923095703\n",
            "deviatia standard a textelor de train: 192.9701690673828\n",
            "deviatia standard a textelor de validare: 192.45263671875\n",
            "deviatia standard a lungimilor textelor de test: 149.76451110839844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myp89Q4xEP6r"
      },
      "source": [
        "Lungimile medii ale cuvintelor per fiecare text din fiecare dataset, apoi media lungimilor medii ale cuvintelor pe fiecare dataset si deviata satandard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgJva0grn0uM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "ac92fe6d-0af9-4283-bef6-54503241c924"
      },
      "source": [
        "#average of len of words of each text\n",
        "average_train_word_lengths_per_text=[np.array(([len(train_textlist[i][j]) for j in range(len(train_textlist[i]))]) ).mean() for i in range(len(train_textlist))]\n",
        "average_validation_word_lengths_per_text=[np.array(([len(validation_textlist[i][j]) for j in range(len(validation_textlist[i]))]) ).mean() for i in range(len(validation_textlist))]\n",
        "average_test_word_lengths_per_text=[np.array(([len(test_textlist[i][j]) for j in range(len(test_textlist[i]))]) ).mean() for i in range(len(test_textlist))]\n",
        "\n",
        "average_train_word_lengths_per_text=torch.FloatTensor(average_train_word_lengths_per_text)\n",
        "average_validation_word_lengths_per_text=torch.FloatTensor(average_validation_word_lengths_per_text)\n",
        "average_test_word_lengths_per_text=torch.FloatTensor(average_test_word_lengths_per_text)\n",
        "print(f'mediile lungimiilor cuvintelor per fiecare text de train: {average_train_word_lengths_per_text}')\n",
        "print(f'mediile lungimiilor cuvintelor per fiecare text de validare: {average_validation_word_lengths_per_text}')\n",
        "print(f'mediile lungimiilor cuvintelor per fiecare text de test: {average_test_word_lengths_per_text}')\n",
        "print(f'media mediilor lungimiilor cuvintelor per text de train: {torch.mean(average_train_word_lengths_per_text)}')\n",
        "print(f'media mediilor lungimiilor cuvintelor per text de validare: {torch.mean(average_validation_word_lengths_per_text)}')\n",
        "print(f'media mediilor lungimiilor cuvintelor per text de test: {torch.mean(average_test_word_lengths_per_text)}')\n",
        "print(f'deviatia standard a mediilor lungimiilor cuvintelor per text de train: {torch.std(average_train_word_lengths_per_text)}')\n",
        "print(f'deviatia standard a mediilor lungimiilor cuvintelor per text de validare: {torch.std(average_validation_word_lengths_per_text)}')\n",
        "print(f'deviatia standard a mediilor lungimiilor cuvintelor per text de test: {torch.std(average_test_word_lengths_per_text)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mediile lungimiilor cuvintelor per fiecare text de train: tensor([5.0172, 4.0000, 4.6333,  ..., 4.9470, 4.5000, 4.4474])\n",
            "mediile lungimiilor cuvintelor per fiecare text de validare: tensor([4.3226, 4.7838, 4.1250,  ..., 4.5000, 4.6481, 4.8065])\n",
            "mediile lungimiilor cuvintelor per fiecare text de test: tensor([4.6364, 3.9636, 4.6829,  ..., 4.7273, 4.2857, 4.1463])\n",
            "media mediilor lungimiilor cuvintelor per text de train: 4.512233734130859\n",
            "media mediilor lungimiilor cuvintelor per text de validare: 4.498715400695801\n",
            "media mediilor lungimiilor cuvintelor per text de test: 4.51282262802124\n",
            "deviatia standard a mediilor lungimiilor cuvintelor per text de train: 0.3247920274734497\n",
            "deviatia standard a mediilor lungimiilor cuvintelor per text de validare: 0.32956647872924805\n",
            "deviatia standard a mediilor lungimiilor cuvintelor per text de test: 0.3234666883945465\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO8_aQS5wwqC"
      },
      "source": [
        "### Cuvinte care apar in plus pe testul de validare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95WCyfWgHGwq"
      },
      "source": [
        "Cream trei word counturi, pentru datele de train, validare si test respectiv. \n",
        "\n",
        "Vrem sa vedem daca apar cuvinte diferite intre dataseturi, deoarece un anumit cuvant care apare intr-un text ar putea conditiona apartenenta textului la dialect, deci poate fi un feature important.\n",
        "\n",
        "`train_textlist` si celelalte textlisturi sunt deja splituite in cuvinte, deci ca tokenizer apelez o functie dummy care doar returneaza lista cu cuvintele din document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gdncTyBH15Z"
      },
      "source": [
        "def dummy_fun(doc):\n",
        "  return doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5gpILz8FFYn"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "wordcounter_train=CountVectorizer(max_df=1., min_df=0, lowercase=False, \n",
        "    tokenizer=dummy_fun, preprocessor=dummy_fun, \n",
        "    token_pattern=None, binary=True)\n",
        "wordcounter_validation=CountVectorizer(max_df=1., min_df=0, lowercase=False, \n",
        "    tokenizer=dummy_fun, preprocessor=dummy_fun, \n",
        "    token_pattern=None, binary=True)\n",
        "wordcounter_test=CountVectorizer(max_df=1., min_df=0, lowercase=False, \n",
        "    tokenizer=dummy_fun, preprocessor=dummy_fun, \n",
        "    token_pattern=None, binary=True)\n",
        "wordcounter_train=wordcounter_train.fit(train_textlist)\n",
        "wordcounter_validation=wordcounter_validation.fit(validation_textlist)\n",
        "wordcounter_test=wordcounter_test.fit(test_textlist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdO7QCa-JJmx"
      },
      "source": [
        "Cheile dictionarului construit de fiecare wordcounter vor fi cuvintele din fiecare dataset. Construim multimi ca sa facem operatii usoare cu ele."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYpdst_4IhY6"
      },
      "source": [
        "train_vocab_set=set([key for key in wordcounter_train.vocabulary_.keys()])\n",
        "validation_vocab_set=set([key for key in wordcounter_validation.vocabulary_.keys()])\n",
        "test_vocab_set=set([key for key in wordcounter_test.vocabulary_.keys()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9S_KXr8JXpH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7a2ff721-b882-450b-8810-90968f713481"
      },
      "source": [
        "print(len(train_vocab_set))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32874\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZ2sb4hNyeIP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "28ee68eb-e5d8-4180-da30-a1f6c64aee58"
      },
      "source": [
        "print(f'numar total de cuvinte din train: {len(train_vocab_set)}')\n",
        "print(f'numar total de cuvinte din validation: {len(validation_vocab_set)}')\n",
        "print(f'numar total de cuvinte din test: {len(test_vocab_set)}')\n",
        "print(f'numar de cuvinte care sunt in validation si nu sunt in train: \\\n",
        "{len(validation_vocab_set - train_vocab_set)}\\\n",
        "; procentual: {len(validation_vocab_set - train_vocab_set)/len(validation_vocab_set)}')\n",
        "print(f'numar de cuvinte care sunt in test si nu sunt in train: \\\n",
        "{len(test_vocab_set - train_vocab_set)}\\\n",
        "; procentual: {len(test_vocab_set - train_vocab_set)/len(test_vocab_set)}')\n",
        "print(f'numar de cuvinte care sunt in test si nu sunt nici in validation si nici in train: \\\n",
        "{len(test_vocab_set - (train_vocab_set | validation_vocab_set) )}\\\n",
        "; procentual: {len(test_vocab_set - (train_vocab_set | validation_vocab_set))/len(test_vocab_set)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numar total de cuvinte din train: 32874\n",
            "numar total de cuvinte din validation: 18912\n",
            "numar total de cuvinte din test: 18104\n",
            "numar de cuvinte care sunt in validation si nu sunt in train: 5507; procentual: 0.2911907783417936\n",
            "numar de cuvinte care sunt in test si nu sunt in train: 5158; procentual: 0.284909412284578\n",
            "numar de cuvinte care sunt in test si nu sunt nici in validation si nici in train: 4457; procentual: 0.24618868758285462\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQSfLRIn5Zj0"
      },
      "source": [
        "### Cuvinte care apar in datele de antrenament doar pe o eticheta versus cuvinte care apar in validare doar pe eticheta respectiva"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aldb8l1QuxFq"
      },
      "source": [
        "#pastram textele de train si validare care corespund fiecarei etichete\n",
        "train_label0=[train_textlist[i] for i in range(len(train_textlist)) if train_labels[i]==0]\n",
        "train_label1=[train_textlist[i] for i in range(len(train_textlist)) if train_labels[i]==1]\n",
        "validation_label0=[validation_textlist[i] for i in range(len(validation_textlist)) if validation_labels[i]==0]\n",
        "validation_label1=[validation_textlist[i] for i in range(len(validation_textlist)) if validation_labels[i]==1]\n",
        "assert len(train_label1)+len(train_label0) == len(train_textlist), \"Nu obtin toate textele de train\"\n",
        "assert len(validation_label1)+len(validation_label0) == len(validation_textlist), \"Nu obtin toate textele de validare\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCrROeSfvwMx"
      },
      "source": [
        "#obtinem vocabularele fiecarei clase din fiecare dataset\n",
        "\n",
        "wordcounter_train_label0=CountVectorizer(max_df=1., min_df=0, lowercase=False, \n",
        "    tokenizer=dummy_fun, preprocessor=dummy_fun, \n",
        "    token_pattern=None, binary=True)\n",
        "wordcounter_train_label1=CountVectorizer(max_df=1., min_df=0, lowercase=False, \n",
        "    tokenizer=dummy_fun, preprocessor=dummy_fun, \n",
        "    token_pattern=None, binary=True)\n",
        "wordcounter_val_label0=CountVectorizer(max_df=1., min_df=0, lowercase=False, \n",
        "    tokenizer=dummy_fun, preprocessor=dummy_fun, \n",
        "    token_pattern=None, binary=True)\n",
        "wordcounter_val_label1=CountVectorizer(max_df=1., min_df=0, lowercase=False, \n",
        "    tokenizer=dummy_fun, preprocessor=dummy_fun, \n",
        "    token_pattern=None, binary=True)\n",
        "\n",
        "wordcounter_train_label0=wordcounter_train_label0.fit(train_label0)\n",
        "wordcounter_train_label1=wordcounter_train_label1.fit(train_label1)\n",
        "wordcounter_val_label0=wordcounter_val_label0.fit(validation_label0)\n",
        "wordcounter_val_label1=wordcounter_val_label1.fit(validation_label1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-54rk117wZNN"
      },
      "source": [
        "#obtinem seturi de cuvinte din fiecare clasa din fiecare dataset\n",
        "train_label0_vocab_set=set([key for key in wordcounter_train_label0.vocabulary_.keys()])\n",
        "train_label1_vocab_set=set([key for key in wordcounter_train_label1.vocabulary_.keys()])\n",
        "val_label0_vocab_set=set([key for key in wordcounter_val_label0.vocabulary_.keys()])\n",
        "val_label1_vocab_set=set([key for key in wordcounter_val_label1.vocabulary_.keys()])\n",
        "assert len(train_label0_vocab_set | train_label1_vocab_set) == len(train_vocab_set),\\\n",
        "       \"Cele doua clase din train nu dau tot vocabularul\"\n",
        "assert len(val_label0_vocab_set | val_label1_vocab_set) == len(validation_vocab_set),\\\n",
        "       \"Cele doua clase din validare nu dau tot vocabularul\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XynU1cOUwwzs"
      },
      "source": [
        "#obtinem seturi de cuvinte unice fiecarei clase in interiorul fiecarui dataset\n",
        "train_label0_minus_label1=train_label0_vocab_set - train_label1_vocab_set\n",
        "train_label1_minus_label0=train_label1_vocab_set - train_label0_vocab_set\n",
        "val_label0_minus_label1=val_label0_vocab_set - val_label1_vocab_set\n",
        "val_label1_minus_label0=val_label1_vocab_set - val_label0_vocab_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0xqNE1oR5RD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "outputId": "396cc176-e544-423d-d479-a38b169b4b47"
      },
      "source": [
        "print(f'numar de cuvinte care apar in datele de train in texte cu eticheta 0 \\n si nu apar in niciun text (de train)  \\\n",
        " cu eticheta 1: {len(train_label0_minus_label1)};\\n procentual din toate cuvintele care apar in train cu eticheta 0: \\\n",
        "{len(train_label0_minus_label1)/len(train_label0_vocab_set)}\\n')\n",
        "\n",
        "print(f'numar de cuvinte care apar in datele de train in texte cu eticheta 1 \\n si nu apar in niciun text (de train) \\\n",
        "cu eticheta 0: {len(train_label1_minus_label0)}; \\n procentual din toate cuvintele care apar in train cu eticheta 0: \\\n",
        "{len(train_label1_minus_label0)/len(train_label1_vocab_set)}\\n')\n",
        "\n",
        "print(f'numar de cuvinte care apar in datele de validare in texte cu eticheta 0 \\n si nu apar in niciun text (de validare)\\\n",
        "cu eticheta 1: {len(val_label0_minus_label1)}; \\n procentual din toate cuvintele care apar in validare cu eticheta 0: \\\n",
        "{len(val_label0_minus_label1)/len(val_label0_vocab_set)}\\n')\n",
        "\n",
        "print(f'numar de cuvinte care apar in datele de validare in texte cu eticheta 1 \\n si nu apar in niciun text (de validare)\\\n",
        " cu eticheta 0: {len(val_label1_minus_label0)}; \\n procentual din toate cuvintele care apar in validare cu eticheta 0: \\\n",
        "{len(val_label1_minus_label0)/len(val_label1_vocab_set)}\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numar de cuvinte care apar in datele de train in texte cu eticheta 0 \n",
            " si nu apar in niciun text (de train)   cu eticheta 1: 11740;\n",
            " procentual din toate cuvintele care apar in train cu eticheta 0: 0.5129325410695561\n",
            "\n",
            "numar de cuvinte care apar in datele de train in texte cu eticheta 1 \n",
            " si nu apar in niciun text (de train) cu eticheta 0: 9986; \n",
            " procentual din toate cuvintele care apar in train cu eticheta 0: 0.47250875366707673\n",
            "\n",
            "numar de cuvinte care apar in datele de validare in texte cu eticheta 0 \n",
            " si nu apar in niciun text (de validare)cu eticheta 1: 6603; \n",
            " procentual din toate cuvintele care apar in validare cu eticheta 0: 0.4932397101665795\n",
            "\n",
            "numar de cuvinte care apar in datele de validare in texte cu eticheta 1 \n",
            " si nu apar in niciun text (de validare) cu eticheta 0: 5525; \n",
            " procentual din toate cuvintele care apar in validare cu eticheta 0: 0.4488585587781298\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ok6sZXBE7sSA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "21afd94c-52fe-4522-8a40-0bebb0559302"
      },
      "source": [
        "print(f'numar de cuvinte care apar in datele de train exclusiv in texte cu eticheta 0 \\n\\\n",
        "si apar si in datele de validare exclusiv in texte cu eticheta 0: \\\n",
        "{len(train_label0_minus_label1 & val_label0_minus_label1)}; \\n procentual din toate cuvintele de train \\\n",
        "care apar in texte cu eticheta 1: {len(train_label0_minus_label1 & val_label0_minus_label1)/len(train_label0_vocab_set)}; \\n\\\n",
        "procentual din toate cuvintele de train care apar exclusiv \\nin texte de train cu eticheta 0: \\\n",
        "{len(train_label0_minus_label1 & val_label0_minus_label1)/len(train_label0_minus_label1)};\\nprocentual din toate \\\n",
        "cuvintele de validare care apar \\nin texte de validare cu eticheta 0: \\\n",
        "{len(train_label0_minus_label1 & val_label0_minus_label1)/len(val_label0_vocab_set)};\\nprocentual din toate \\\n",
        "cuvintele de validare care apar exclusiv \\nin texte de validare cu eticheta 0: \\\n",
        "{len(train_label0_minus_label1 & val_label0_minus_label1)/len(val_label0_minus_label1)}')\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(f'numar de cuvinte care apar in datele de train exclusiv in texte cu eticheta 1 \\n\\\n",
        "si apar si in datele de validare exclusiv in texte cu eticheta 1: \\\n",
        "{len(train_label1_minus_label0 & val_label1_minus_label0)}; \\n procentual din toate cuvintele de train \\\n",
        "care apar in texte cu eticheta 1: {len(train_label1_minus_label0 & val_label1_minus_label0)/len(train_label1_vocab_set)}; \\n\\\n",
        "procentual din toate cuvintele de train care apar exclusiv \\nin texte de train cu eticheta 0: \\\n",
        "{len(train_label1_minus_label0 & val_label1_minus_label0)/len(train_label1_minus_label0)};\\nprocentual din toate \\\n",
        "cuvintele de validare care apar \\nin texte de validare cu eticheta 0: \\\n",
        "{len(train_label1_minus_label0 & val_label1_minus_label0)/len(val_label1_vocab_set)};\\nprocentual din toate \\\n",
        "cuvintele de validare care apar exclusiv \\nin texte de validare cu eticheta 0: \\\n",
        "{len(train_label1_minus_label0 & val_label1_minus_label0)/len(val_label1_minus_label0)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numar de cuvinte care apar in datele de train exclusiv in texte cu eticheta 0 \n",
            "si apar si in datele de validare exclusiv in texte cu eticheta 0: 1665; \n",
            " procentual din toate cuvintele de train care apar in texte cu eticheta 1: 0.07274554351625306; \n",
            "procentual din toate cuvintele de train care apar exclusiv \n",
            "in texte de train cu eticheta 0: 0.1418228279386712;\n",
            "procentual din toate cuvintele de validare care apar \n",
            "in texte de validare cu eticheta 0: 0.1243743930679017;\n",
            "procentual din toate cuvintele de validare care apar exclusiv \n",
            "in texte de validare cu eticheta 0: 0.2521581099500227\n",
            "\n",
            "\n",
            "numar de cuvinte care apar in datele de train exclusiv in texte cu eticheta 1 \n",
            "si apar si in datele de validare exclusiv in texte cu eticheta 1: 1209; \n",
            " procentual din toate cuvintele de train care apar in texte cu eticheta 1: 0.05720639727453393; \n",
            "procentual din toate cuvintele de train care apar exclusiv \n",
            "in texte de train cu eticheta 0: 0.1210694972962147;\n",
            "procentual din toate cuvintele de validare care apar \n",
            "in texte de validare cu eticheta 0: 0.09822081403850841;\n",
            "procentual din toate cuvintele de validare care apar exclusiv \n",
            "in texte de validare cu eticheta 0: 0.2188235294117647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIPrnsjL1uAo"
      },
      "source": [
        "##**Feature extraction din training si validare**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ydn-t4p8yNs8"
      },
      "source": [
        "Vocabular imbunatatit (nu contine cuvintele comune celor 2 labeluri)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rD8HkDEPyRhi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "17381e37-eb39-4d65-fbdb-794478c88153"
      },
      "source": [
        "improved_vocabulary=vocab_label0_not_label1+vocab_label1_not_label0\n",
        "print(len(improved_vocabulary))\n",
        "print(len(vocab_label0_not_label1))\n",
        "print(len(vocab_label1_not_label0))\n",
        "assert len(improved_vocabulary)==len(vocab_label1_not_label0)+len(vocab_label0_not_label1), print(\"Ceva n-a mers la extindere\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25901\n",
            "13830\n",
            "12071\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1niTehlbV_da"
      },
      "source": [
        "### **VAR 1: TfIdf**\n",
        "\n",
        "La validarea sursa o sa convertesc in dense matrix pentru ca e un fisier de 4 ori mai mic si sper sa am destula memorie.\n",
        "\n",
        "Salvarea matricelor sparse rezultate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCYu0dhMFOxs"
      },
      "source": [
        "**Daca s-a facut deja, pot incarca din fisier in codul de mai jos**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nxt3-2Pj-m8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "c0a47867-3ef0-486a-dc0c-0314046074fb"
      },
      "source": [
        "#teste (cum arata textlisturile)\n",
        "print(train_textlist[0])\n",
        "print(train_textlist[1])\n",
        "print(validation_textlist[0])\n",
        "print(validation_textlist[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[';%fE', 'mr#&', 'crmx', 'temjc@m', \"%'wb:\", '}hHAm@@m', 'ykm=aa', 'Eje@', 'Ejh=', 'EcrZk', 's}lZ$', 'rhfh', '}h@kofe@mk', 'RgWE<', '>mfor@m', '@#@', 'm=hkaa', 'TFr>o*', 'h}Ah', 'EHfm}e@mHk', 'e#hj@', 'j&}k', 'gAmaH', 'mgmkafe', 'cmT:', 'k>.h', 'XH(q!', '}FW', '@*oDgB', '#Sx.W', 'hZ', 'jh=', 'chrZ', '}k#h', 'svcNt', 'ejmc@m', 'gYAmZ', 'efke@m', 'h}Ah', 'g@@m', '>m&', '}%k', 'tr(:', ';wxq', 'Ere', 'E*}ga', 'hgZ', 'h$mhr@m', 'tkafe@m', 't@A', '%#sE', '=hkaa@m', 'm*gH', 'E@he=@m', 'wk}hX', 'Ejhr=@m', 'Ejhr=@m', 'h@mg:@']\n",
            "['sAFW', 'K#xk}t', 'fH@ae', 'm&Xd', '>h&', '@#', 'l@Rd}a', '@Hc', 'liT', 'ehAr@m', 'Xgmz', '!}a', '}eAr@m', 'Be', 'g@@m', 'efH', 'RB(D', 'Ehk&']\n",
            "['E*Zjv', 'mj=', 'Ee($', 'A@mpe', 'hZ', '.ycn;', '*gjlU', 'H&Zn;', '}kh@Hm}', 'jgA', '}e', 'mj=', 'E@hkx', 'EraZ', '}e', 'mj=', 'hB#*;', 'Eehjck', 'hHrA@m', 'be|KB', \"*l'qy\", 'e@mA@m', '*|XE', 'Ae', 'mj=', 'H!d', 'h*jx', 'Eejcae', 'mHkrk:', \">c'WY\", 'A@mpe']\n",
            "[\"Xea'\", '}hrAH@m', 'mnSb(', 'Bg:z', 'HA&@@', 'Erkp@m', 'g*g', '}k@pmkae', 'A&ymz', 'tgmY', 'ekh@m', 'Ex&}:@m', 'g*g', \"*xa@D'\", '@m@(', 'r$m', '}e', 'ejca', 'ha@m', 'amxm&}@mk', 'EjAmk', '*WAEh', 'EjAmk', 'gzmNW', \"*q'oF\", '&jKn*', 'EjAmk', '<pAmE', 'Yze', 'EjAmk', 'E@eA', 'n>lAx', '}A', 'hX!FT', 'gxl|*', 'E>TCa', 'm}Aef']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OOL3J4jeal4"
      },
      "source": [
        "def dummy_fun(doc):\n",
        "  return doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY2JXC-BWDwU"
      },
      "source": [
        "#use_idf=false, binary=True, norm=False pentru a obtine un vector binar\n",
        "#old one: get only whether word is present\n",
        "# cv=TfidfVectorizer(\n",
        "#     lowercase=False,\n",
        "#     token_pattern='[^\\s]+')\n",
        "cv=TfidfVectorizer(\n",
        "    max_df=0.8, min_df=0, lowercase=False,\n",
        "    token_pattern='[^\\s]+', analyzer='word')\n",
        "# cv=TfidfVectorizer(\n",
        "#     max_df=0.8, min_df=0, lowercase=False,\n",
        "#     token_pattern='[^\\s]+', analyzer='char_wb', ngram_range=(1,4))\n",
        "\n",
        "# cv=TfidfVectorizer(analyzer='char_wb', binary=True, decode_error='replace',\n",
        "#          encoding='utf-8', input='content',\n",
        "#         max_df=0.7, min_df=1,\n",
        "#         ngram_range=(10,12), norm='l2',\n",
        "#         strip_accents='ascii', sublinear_tf=False,\n",
        "#                    use_idf=True)\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "# cv=CountVectorizer(max_df=1., min_df=0, lowercase=False, \n",
        "#     tokenizer=dummy_fun, preprocessor=dummy_fun, \n",
        "#     token_pattern=None, binary=True, vocabulary=improved_vocabulary)\n",
        "# word_count_train=cv.fit_transform([train_data[key] for key in train_data.keys()])\n",
        "cv.fit([train_data[key] for key in train_data.keys()])\n",
        "word_count_train=cv.transform([train_data[key] for key in train_data.keys()])\n",
        "word_count_validation=cv.transform([validation_data[key] for key in validation_data.keys()])\n",
        "# print(type(word_count_train_ngrams))\n",
        "# print(type(word_count_validation_ngrams))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNL7WKB3Fftf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "57da4368-9af5-45e4-fe0c-26fd56cbf8a6"
      },
      "source": [
        "# print([train_data[key] for key in train_data.keys()])\n",
        "print(word_count_train_ngrams.shape)\n",
        "print(word_count_validation_ngrams.shape)\n",
        "# print(word_count_train[:3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7757, 36871)\n",
            "(2656, 36871)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXcHzfJwFeBf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "246bb51b-6e7d-455a-94c7-12deef87138d"
      },
      "source": [
        "# print([train_data[key] for key in train_data.keys()])\n",
        "print(word_count_train.shape)\n",
        "print(word_count_validation.shape)\n",
        "# print(word_count_train[:3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7757, 32874)\n",
            "(2656, 32874)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12KZznr5-poM"
      },
      "source": [
        "print(word_count_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2jVPL9K9wpQ"
      },
      "source": [
        "import scipy\n",
        "print(word_count_train[:50, :10000].sum(axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFL2rX4mknxN"
      },
      "source": [
        "import scipy\n",
        "scipy.sparse.save_npz(basepath+'word_count_train', word_count_train, compressed=True)\n",
        "scipy.sparse.save_npz(basepath+'word_count_validation', word_count_validation, compressed=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFb_DIeKuNU_"
      },
      "source": [
        "#### Loading of wordcounts and definition of batch-getting function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwJUY9UyiL1D"
      },
      "source": [
        "Incarc word_counturile **daca exista deja**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30e03Ysait6e"
      },
      "source": [
        "import scipy\n",
        "word_count_train=scipy.sparse.load_npz(basepath+'word_count_train.npz')\n",
        "word_count_validation=scipy.sparse.load_npz(basepath+'word_count_validation.npz')\n",
        "# word_count_target=scipy.sparse.load_npz(basepath+'word_count_target.npz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jByOy4lKpNAx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6b3f9d02-3b88-4dc1-84a0-462fdb1e5206"
      },
      "source": [
        "print(word_count_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7757, 25901)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjVlQCVEgZlu"
      },
      "source": [
        "def binarize(features):\n",
        "  #supposed to be a non-sparse matrix\n",
        "  features=torch.Tensor(features)\n",
        "  if (features.shape[0]==1):\n",
        "    features=features.squeeze()\n",
        "    features=[features>0]\n",
        "    features=features[0]\n",
        "    features=torch.Tensor([int(i) for i in features])\n",
        "    features=features.reshape(1, features.shape[0])\n",
        "  else:\n",
        "    for i in range(features.shape[0]):\n",
        "      processing=[features[i,:]>0]\n",
        "      processing=processing[0]\n",
        "      processing=torch.Tensor([int(i) for i in processing])\n",
        "      features[i, :]=processing\n",
        "  return(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfB1F6gbWHWI"
      },
      "source": [
        "Functie care-mi da un batch **pentru TfIdf**. Inputul e asteptat ca sparse matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1G9RgG6WK-B"
      },
      "source": [
        "def get_batch_tfidf(train_features, train_labels, batch_size, batch_index):\n",
        "    batch_features=torch.Tensor( train_features[batch_index].todense()).to(device)\n",
        "    batch_labels=[train_labels[batch_index]]\n",
        "    for i in range(1,min(batch_index+batch_size, train_features.shape[0])):\n",
        "        percent=torch.Tensor(train_features[i].todense())\n",
        "        batch_features=torch.cat((batch_features, percent), dim=0)\n",
        "        #la fel cu reshape-ul si aici, pentru ca trebuie ca tensorii sa aiba aceeasi dimensiune pentru cat, dar train_labels[i] are o singura dimensiune\n",
        "        # batch_labels=torch.cat((batch_labels, train_labels[i]))\n",
        "        batch_labels.append(train_labels[i])\n",
        "    # print(f'The batch features have shape: {batch_features.shape}')\n",
        "    return(batch_features, torch.LongTensor(batch_labels).to(device))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97FmWzaD4Fvq"
      },
      "source": [
        "### **VAR 2: Word2Vec**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLUlRabPB4LA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "cd75063a-8c43-409c-bb4b-a6888de0e44f"
      },
      "source": [
        "#average of len of words of each text\n",
        "print([np.array(([len(train_textlist[i][j]) for j in range(len(train_textlist[i]))]) ).mean() for i in range(len(train_textlist))])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5.051724137931035, 4.111111111111111, 4.7, 4.3478260869565215, 4.226415094339623, 4.478260869565218, 4.157894736842105, 4.469387755102041, 4.421052631578948, 4.230769230769231, 5.05, 4.543478260869565, 4.666666666666667, 4.7272727272727275, 5.896551724137931, 5.25, 4.5, 4.75, 4.521739130434782, 4.566666666666666, 5.052631578947368, 4.194444444444445, 4.357142857142857, 5.2, 4.571428571428571, 4.981481481481482, 4.56, 4.631578947368421, 4.705882352941177, 5.2, 4.840909090909091, 4.428571428571429, 4.6923076923076925, 4.365384615384615, 4.4, 4.9523809523809526, 4.380952380952381, 4.782608695652174, 4.4, 4.333333333333333, 5.25, 5.7, 4.391304347826087, 3.984375, 4.142857142857143, 4.458333333333333, 4.458333333333333, 4.350877192982456, 4.536332179930795, 4.575757575757576, 4.222222222222222, 4.526315789473684, 4.333333333333333, 4.454545454545454, 4.896551724137931, 3.909090909090909, 4.6891891891891895, 4.954545454545454, 5.0, 4.333333333333333, 4.428571428571429, 4.864864864864865, 4.5, 5.0, 4.647058823529412, 4.371428571428571, 4.255813953488372, 4.3076923076923075, 4.181818181818182, 4.464285714285714, 4.862068965517241, 4.931818181818182, 4.714285714285714, 4.636363636363637, 4.409090909090909, 4.466666666666667, 4.602739726027397, 4.9375, 4.733333333333333, 4.53125, 4.620689655172414, 4.666666666666667, 5.357142857142857, 4.631578947368421, 3.8181818181818183, 4.832258064516129, 4.454545454545454, 4.380952380952381, 4.095238095238095, 4.473684210526316, 4.315789473684211, 4.625, 4.566666666666666, 4.9393939393939394, 4.166666666666667, 4.450980392156863, 4.821428571428571, 4.75, 4.8125, 4.578947368421052, 4.818181818181818, 4.695652173913044, 4.470588235294118, 4.9375, 4.6, 4.304347826086956, 4.451327433628318, 4.633333333333334, 4.5, 5.2, 4.775, 4.5, 4.357142857142857, 4.321428571428571, 4.0, 5.5, 4.705882352941177, 4.560975609756097, 4.976744186046512, 4.32258064516129, 4.333333333333333, 4.659574468085107, 4.871794871794871, 4.5, 4.852941176470588, 4.702127659574468, 4.8076923076923075, 5.2105263157894735, 4.5476190476190474, 4.863636363636363, 4.3, 5.090909090909091, 4.444444444444445, 4.909090909090909, 4.575757575757576, 5.333333333333333, 4.05, 4.846153846153846, 4.527777777777778, 4.133333333333334, 4.918918918918919, 4.5, 4.333333333333333, 4.521739130434782, 4.5, 4.7894736842105265, 5.111111111111111, 4.280898876404494, 4.68, 4.904761904761905, 4.8, 4.884615384615385, 4.931034482758621, 4.482758620689655, 4.533333333333333, 5.157894736842105, 3.911392405063291, 4.769230769230769, 4.041666666666667, 4.333333333333333, 5.0, 4.45, 5.137931034482759, 4.67741935483871, 4.461538461538462, 4.4361702127659575, 4.428571428571429, 4.36, 4.533333333333333, 4.6, 4.15, 4.545454545454546, 4.433333333333334, 4.611111111111111, 4.6, 4.4375, 3.9, 4.8125, 4.222222222222222, 5.416666666666667, 5.0, 4.584905660377358, 5.642857142857143, 4.2727272727272725, 4.645161290322581, 4.484848484848484, 4.166666666666667, 4.46875, 5.0, 4.285714285714286, 4.366666666666666, 4.5476190476190474, 4.484848484848484, 5.46875, 4.555555555555555, 4.608695652173913, 4.476190476190476, 4.773809523809524, 4.367088607594937, 4.7, 4.384615384615385, 4.478260869565218, 4.392857142857143, 4.9818181818181815, 4.966666666666667, 4.514285714285714, 4.571428571428571, 4.25, 5.0588235294117645, 4.409090909090909, 4.933333333333334, 5.041666666666667, 4.538461538461538, 4.122448979591836, 4.0588235294117645, 4.454545454545454, 4.857142857142857, 5.222222222222222, 4.565217391304348, 4.588235294117647, 4.773584905660377, 4.925531914893617, 4.769230769230769, 4.366666666666666, 4.25, 4.454545454545454, 5.142857142857143, 4.6923076923076925, 4.606060606060606, 4.421052631578948, 4.545454545454546, 4.3, 4.37037037037037, 4.916666666666667, 4.869565217391305, 4.571428571428571, 4.471698113207547, 4.8441558441558445, 4.678571428571429, 4.485148514851486, 4.315789473684211, 5.666666666666667, 4.512820512820513, 4.390243902439025, 5.043478260869565, 4.7073170731707314, 4.277777777777778, 4.346938775510204, 5.369230769230769, 4.6923076923076925, 4.814814814814815, 4.416666666666667, 4.28, 3.8333333333333335, 4.863636363636363, 4.4, 4.3, 4.266666666666667, 4.348837209302325, 5.094827586206897, 4.6, 4.142857142857143, 4.625, 4.882352941176471, 5.066666666666666, 4.4, 4.703703703703703, 4.5, 4.7727272727272725, 4.333333333333333, 4.48936170212766, 4.444444444444445, 4.176470588235294, 4.529411764705882, 4.314285714285714, 4.068181818181818, 5.05, 4.413793103448276, 4.342105263157895, 4.571428571428571, 4.413793103448276, 5.045454545454546, 4.3478260869565215, 4.583333333333333, 4.804878048780488, 4.6, 4.56, 4.454545454545454, 4.454545454545454, 4.571428571428571, 6.285714285714286, 4.568181818181818, 4.28, 4.530612244897959, 4.530612244897959, 3.9545454545454546, 4.8, 4.666666666666667, 4.555555555555555, 4.739130434782608, 4.25, 4.444444444444445, 4.2592592592592595, 3.736842105263158, 5.096774193548387, 4.8125, 4.428571428571429, 4.260869565217392, 5.0, 4.62962962962963, 4.333333333333333, 4.6, 4.791666666666667, 4.894736842105263, 4.470588235294118, 4.666666666666667, 4.533333333333333, 5.04, 4.333333333333333, 4.714285714285714, 4.428571428571429, 4.75, 4.8, 4.88, 4.3076923076923075, 5.606060606060606, 4.4, 4.625, 4.190476190476191, 5.303030303030303, 4.5, 4.578947368421052, 4.222222222222222, 4.680851063829787, 4.854166666666667, 4.7, 5.0476190476190474, 4.382352941176471, 4.75, 4.96969696969697, 4.181818181818182, 4.666666666666667, 4.4, 4.619047619047619, 4.285714285714286, 4.590909090909091, 4.266666666666667, 4.696969696969697, 4.444444444444445, 4.5, 4.791666666666667, 4.3478260869565215, 4.683333333333334, 3.9, 4.75, 4.9, 4.52, 4.55, 4.108108108108108, 4.7, 4.448275862068965, 4.631578947368421, 4.2, 4.875, 4.6571428571428575, 4.916666666666667, 3.5833333333333335, 4.115537848605578, 5.130434782608695, 4.785714285714286, 4.75, 4.735849056603773, 4.666666666666667, 4.076923076923077, 4.027027027027027, 4.846153846153846, 4.535714285714286, 4.533333333333333, 5.0, 4.625, 4.166666666666667, 4.333333333333333, 3.9444444444444446, 5.041666666666667, 4.5, 5.6923076923076925, 3.8333333333333335, 4.625, 4.590909090909091, 4.788461538461538, 4.40625, 5.2105263157894735, 4.473684210526316, 4.68, 4.32, 4.1, 5.0, 4.833333333333333, 4.666666666666667, 4.5636363636363635, 4.9375, 4.8125, 4.393939393939394, 5.555555555555555, 4.220338983050848, 4.448275862068965, 4.428571428571429, 4.588235294117647, 4.625, 4.5, 4.409090909090909, 4.703703703703703, 4.461538461538462, 4.862068965517241, 4.4, 4.023255813953488, 5.439024390243903, 4.255813953488372, 4.5, 4.28, 4.516129032258065, 4.395833333333333, 4.318518518518519, 5.214285714285714, 4.405405405405405, 4.866666666666666, 5.7368421052631575, 4.75, 4.568807339449541, 4.5, 4.533333333333333, 4.366666666666666, 4.944444444444445, 4.6571428571428575, 5.2, 4.0625, 4.0588235294117645, 5.208333333333333, 4.0625, 5.3, 4.2272727272727275, 4.222222222222222, 4.451612903225806, 4.833333333333333, 4.5, 4.620689655172414, 4.222222222222222, 4.8478260869565215, 4.714285714285714, 4.516129032258065, 4.166666666666667, 4.555555555555555, 4.0, 4.411764705882353, 4.25, 4.125, 4.782608695652174, 4.388888888888889, 4.7368421052631575, 5.588235294117647, 4.3125, 4.875, 5.05, 4.277777777777778, 4.764705882352941, 4.380952380952381, 4.5, 4.714285714285714, 4.4, 4.114285714285714, 5.0, 4.619047619047619, 4.545454545454546, 4.906666666666666, 5.117647058823529, 4.916666666666667, 4.5, 4.56, 4.615384615384615, 4.827586206896552, 4.371428571428571, 4.681818181818182, 4.590909090909091, 4.305084745762712, 4.4411764705882355, 4.8, 4.7844827586206895, 4.833333333333333, 4.39622641509434, 4.888888888888889, 4.578947368421052, 4.828125, 5.115384615384615, 4.689655172413793, 4.695652173913044, 5.217391304347826, 4.555555555555555, 4.818181818181818, 4.324324324324325, 4.454545454545454, 4.606060606060606, 4.615384615384615, 4.875, 4.052631578947368, 4.0, 4.464285714285714, 4.7969543147208125, 4.8, 4.277777777777778, 4.602564102564102, 5.133333333333334, 4.8, 4.888888888888889, 5.0, 4.780487804878049, 4.55, 4.0, 3.8541666666666665, 4.5, 4.675, 5.074074074074074, 4.2727272727272725, 4.176470588235294, 4.695035460992908, 4.291666666666667, 4.470588235294118, 5.444444444444445, 5.0, 4.742857142857143, 4.375, 4.333333333333333, 4.722222222222222, 4.851851851851852, 4.7894736842105265, 4.456310679611651, 4.454545454545454, 4.5, 4.857142857142857, 5.105263157894737, 4.176470588235294, 4.428571428571429, 4.375, 4.534246575342466, 5.418181818181818, 4.0, 4.277777777777778, 4.586206896551724, 4.47191011235955, 4.722222222222222, 4.529411764705882, 4.354838709677419, 5.344827586206897, 4.833333333333333, 4.166666666666667, 4.7, 4.214285714285714, 4.428571428571429, 4.419354838709677, 5.0, 4.491803278688525, 4.533333333333333, 4.407407407407407, 5.115384615384615, 4.9, 4.703703703703703, 4.8125, 4.763636363636364, 5.214285714285714, 4.475806451612903, 4.2, 4.666666666666667, 4.8, 4.863636363636363, 4.323529411764706, 4.8125, 4.30188679245283, 4.230769230769231, 5.090909090909091, 4.439024390243903, 4.357142857142857, 4.886792452830188, 4.523809523809524, 4.352941176470588, 4.475, 4.655172413793103, 4.415094339622642, 4.5, 4.71875, 5.571428571428571, 4.815789473684211, 3.8823529411764706, 4.772058823529412, 5.066666666666666, 4.425, 4.1875, 4.411764705882353, 4.72, 4.764705882352941, 4.4, 4.653846153846154, 4.076923076923077, 4.35, 4.4411764705882355, 4.893805309734513, 4.1875, 4.22972972972973, 4.636363636363637, 4.1875, 4.339622641509434, 4.555555555555555, 4.689655172413793, 5.388888888888889, 4.645161290322581, 4.4, 4.938775510204081, 4.947368421052632, 4.238095238095238, 4.166666666666667, 4.684210526315789, 4.346153846153846, 4.6, 4.785714285714286, 4.769230769230769, 4.764705882352941, 4.55, 5.363636363636363, 4.777777777777778, 4.67741935483871, 4.022222222222222, 4.666666666666667, 4.457142857142857, 4.586666666666667, 4.777777777777778, 4.36734693877551, 4.434782608695652, 4.753246753246753, 4.565217391304348, 4.714285714285714, 4.6, 4.157894736842105, 5.0, 4.8, 5.066666666666666, 4.111111111111111, 3.9714285714285715, 4.6875, 4.863636363636363, 4.666666666666667, 4.782608695652174, 4.548387096774194, 4.181818181818182, 4.454545454545454, 4.954545454545454, 4.833333333333333, 4.7368421052631575, 4.346153846153846, 5.153846153846154, 4.346153846153846, 4.833333333333333, 4.52, 4.2727272727272725, 4.136363636363637, 4.421052631578948, 4.485714285714286, 4.571428571428571, 4.433333333333334, 4.7, 4.2368421052631575, 4.131578947368421, 4.526315789473684, 4.8, 4.513513513513513, 4.357142857142857, 4.086956521739131, 4.607407407407408, 4.5, 4.736363636363636, 4.637254901960785, 4.096774193548387, 4.959183673469388, 5.125, 4.904761904761905, 4.818181818181818, 4.875, 5.142857142857143, 4.4, 4.526315789473684, 4.913043478260869, 5.066666666666666, 5.177777777777778, 4.290322580645161, 4.565217391304348, 5.180327868852459, 4.242424242424242, 4.631578947368421, 4.904761904761905, 4.84375, 4.107142857142857, 4.611111111111111, 5.111111111111111, 4.37037037037037, 4.852941176470588, 4.6521739130434785, 4.317073170731708, 4.3076923076923075, 4.588235294117647, 4.583333333333333, 5.130434782608695, 4.0, 4.2631578947368425, 4.666666666666667, 4.809523809523809, 4.3559322033898304, 4.380952380952381, 5.052631578947368, 4.818181818181818, 4.5, 4.958333333333333, 4.0, 4.5, 5.1, 4.0, 5.222222222222222, 4.45945945945946, 4.470588235294118, 4.294117647058823, 4.810344827586207, 4.888888888888889, 4.208333333333333, 4.4, 4.46, 4.615384615384615, 4.548387096774194, 4.680851063829787, 4.0, 5.125, 4.5, 4.823529411764706, 4.914634146341464, 4.697183098591549, 4.818181818181818, 4.28125, 5.0, 4.44, 4.675675675675675, 4.926829268292683, 4.15625, 4.824074074074074, 4.923076923076923, 4.428571428571429, 4.2, 5.071428571428571, 4.527777777777778, 4.3076923076923075, 4.25, 4.777777777777778, 4.5396825396825395, 4.296296296296297, 4.888888888888889, 4.176470588235294, 4.473684210526316, 4.555555555555555, 4.5675675675675675, 4.7555555555555555, 3.8043478260869565, 3.95, 4.285714285714286, 4.43, 4.171052631578948, 4.384615384615385, 4.923076923076923, 4.545454545454546, 4.632075471698113, 4.706666666666667, 4.444444444444445, 4.830188679245283, 5.3076923076923075, 4.774193548387097, 4.2727272727272725, 4.6, 5.043478260869565, 4.411764705882353, 4.6, 4.247422680412371, 4.885714285714286, 4.818181818181818, 4.25, 4.25, 4.716981132075472, 4.46875, 4.606060606060606, 5.175, 4.3, 4.771428571428571, 4.461538461538462, 4.448275862068965, 5.071428571428571, 4.538461538461538, 4.653846153846154, 5.357142857142857, 4.735294117647059, 4.428571428571429, 4.555555555555555, 4.2105263157894735, 4.352941176470588, 4.666666666666667, 5.208333333333333, 4.722222222222222, 4.672131147540983, 4.333333333333333, 4.09375, 4.357142857142857, 4.783783783783784, 4.391304347826087, 4.608695652173913, 5.235294117647059, 4.25, 4.0, 4.4375, 4.1875, 4.777777777777778, 4.771929824561403, 4.714285714285714, 4.393939393939394, 4.190476190476191, 4.444444444444445, 4.285714285714286, 4.6923076923076925, 5.041666666666667, 4.694444444444445, 5.0, 4.833333333333333, 5.393939393939394, 4.352941176470588, 4.983606557377049, 3.8461538461538463, 4.625, 4.833333333333333, 4.9, 4.375, 4.7631578947368425, 4.488888888888889, 4.476190476190476, 4.357142857142857, 4.454545454545454, 3.6470588235294117, 4.805555555555555, 4.464285714285714, 4.6, 4.25, 4.878787878787879, 4.811320754716981, 5.222222222222222, 4.833333333333333, 4.235294117647059, 5.0, 4.6923076923076925, 4.3, 4.375661375661376, 4.433734939759036, 4.290322580645161, 4.5, 4.32, 4.9375, 4.552, 4.485714285714286, 4.534883720930233, 4.568181818181818, 4.428571428571429, 4.555555555555555, 4.333333333333333, 4.769230769230769, 4.52, 5.0, 4.032258064516129, 4.7592592592592595, 4.833333333333333, 4.206896551724138, 4.68, 5.0, 4.666666666666667, 4.766666666666667, 3.888888888888889, 4.098712446351931, 4.476190476190476, 4.266666666666667, 4.636363636363637, 4.666666666666667, 4.642857142857143, 4.466666666666667, 4.75, 4.533333333333333, 4.954545454545454, 4.714285714285714, 4.194444444444445, 4.75, 4.8125, 4.683168316831683, 4.733333333333333, 4.857142857142857, 4.7, 4.826086956521739, 5.038461538461538, 4.714285714285714, 4.238095238095238, 4.46, 5.0, 4.51063829787234, 4.346153846153846, 4.5, 4.333333333333333, 4.133333333333334, 4.854166666666667, 4.666666666666667, 4.45, 4.375, 4.4, 4.096774193548387, 4.818181818181818, 4.857142857142857, 4.545454545454546, 4.142857142857143, 4.761904761904762, 4.6923076923076925, 4.4, 4.083333333333333, 4.761904761904762, 4.466666666666667, 4.533333333333333, 4.428571428571429, 4.2, 4.545454545454546, 4.9743589743589745, 4.5675675675675675, 4.714285714285714, 4.888888888888889, 4.304347826086956, 4.147058823529412, 4.2, 4.6521739130434785, 4.846153846153846, 4.6923076923076925, 4.431818181818182, 4.266666666666667, 4.84, 4.809523809523809, 4.934782608695652, 4.541666666666667, 4.5, 4.095238095238095, 4.206896551724138, 4.761904761904762, 4.904761904761905, 4.628571428571429, 4.6, 4.4, 4.361702127659575, 4.473684210526316, 3.875, 5.0, 4.790697674418604, 4.333333333333333, 4.571428571428571, 4.52, 4.56, 4.5, 4.5, 4.571428571428571, 4.75, 4.5, 4.733333333333333, 4.333333333333333, 4.478021978021978, 5.6, 4.7272727272727275, 4.928571428571429, 4.666666666666667, 4.745454545454545, 4.512820512820513, 4.6, 4.48, 4.323529411764706, 4.523809523809524, 4.357142857142857, 4.588235294117647, 4.461538461538462, 4.542857142857143, 4.083333333333333, 4.409090909090909, 4.730769230769231, 4.434782608695652, 4.523809523809524, 4.153846153846154, 5.1, 5.0625, 4.052631578947368, 4.882352941176471, 4.285714285714286, 4.3478260869565215, 4.25, 4.4324324324324325, 4.454545454545454, 4.904109589041096, 4.461538461538462, 4.448275862068965, 4.377358490566038, 3.95, 4.55, 4.407407407407407, 4.333333333333333, 4.388888888888889, 4.608247422680412, 4.910714285714286, 4.27906976744186, 4.478260869565218, 4.2926829268292686, 4.782608695652174, 5.071428571428571, 4.9, 5.0, 5.0, 5.083333333333333, 4.0, 4.541666666666667, 4.566666666666666, 4.0, 4.0, 4.59375, 4.6521739130434785, 4.368421052631579, 3.933333333333333, 4.426829268292683, 5.428571428571429, 4.882352941176471, 4.0, 4.625, 4.7368421052631575, 4.416666666666667, 4.470588235294118, 4.769230769230769, 4.636363636363637, 5.066666666666666, 4.666666666666667, 4.75, 5.5, 5.0625, 4.909090909090909, 4.647058823529412, 4.578313253012048, 4.267857142857143, 4.444444444444445, 4.315789473684211, 4.6923076923076925, 4.622641509433962, 4.735294117647059, 4.590909090909091, 4.4375, 4.653846153846154, 5.153846153846154, 4.565217391304348, 4.625, 4.714285714285714, 4.571428571428571, 4.7368421052631575, 4.318181818181818, 4.113636363636363, 5.137931034482759, 4.761904761904762, 4.4, 4.4222222222222225, 5.277777777777778, 4.714285714285714, 4.409090909090909, 4.935483870967742, 4.641025641025641, 4.65625, 4.625, 4.771084337349397, 4.385714285714286, 5.333333333333333, 4.724137931034483, 4.851851851851852, 4.704545454545454, 4.776595744680851, 4.529411764705882, 4.551724137931035, 4.083333333333333, 4.6761904761904765, 4.838383838383838, 3.9444444444444446, 4.527272727272727, 4.0625, 4.642857142857143, 4.632911392405063, 4.25, 4.590909090909091, 3.6363636363636362, 5.0, 4.333333333333333, 4.0, 4.185185185185185, 4.555555555555555, 4.571428571428571, 4.95, 5.083333333333333, 4.625, 4.681159420289855, 3.7454545454545456, 4.346153846153846, 4.625, 5.428571428571429, 4.5, 4.688172043010753, 4.434782608695652, 4.904761904761905, 4.461538461538462, 4.457142857142857, 4.407407407407407, 4.636363636363637, 4.24, 5.333333333333333, 4.8125, 4.5, 4.863636363636363, 4.65, 4.909090909090909, 4.666666666666667, 4.375, 4.4, 4.454545454545454, 4.538461538461538, 4.1891891891891895, 4.363636363636363, 4.44, 3.8181818181818183, 4.473684210526316, 6.0, 4.5, 4.409090909090909, 4.578947368421052, 4.318181818181818, 4.588235294117647, 4.3076923076923075, 4.769230769230769, 4.48, 5.125, 4.633333333333334, 4.705882352941177, 4.6938775510204085, 4.571428571428571, 5.571428571428571, 4.9375, 4.920792079207921, 4.730769230769231, 3.9473684210526314, 4.52, 4.357142857142857, 5.0, 3.9444444444444446, 4.509433962264151, 4.866666666666666, 4.8, 4.666666666666667, 4.516666666666667, 4.2105263157894735, 4.205128205128205, 4.588235294117647, 4.761904761904762, 5.0476190476190474, 4.708333333333333, 4.529411764705882, 4.333333333333333, 4.454545454545454, 4.3125, 4.277777777777778, 4.333333333333333, 4.074074074074074, 4.0, 4.714285714285714, 4.440677966101695, 4.45945945945946, 4.928571428571429, 4.6, 5.142857142857143, 4.574468085106383, 4.818181818181818, 4.833333333333333, 5.125, 4.696969696969697, 4.111111111111111, 4.435897435897436, 4.535714285714286, 4.8, 4.473684210526316, 5.181818181818182, 4.222222222222222, 4.6521739130434785, 4.214285714285714, 4.716814159292035, 4.3478260869565215, 4.411764705882353, 5.0, 4.3, 4.7368421052631575, 3.888888888888889, 4.526315789473684, 5.315789473684211, 4.375, 3.789473684210526, 4.72, 4.639784946236559, 4.318181818181818, 4.846153846153846, 4.7727272727272725, 4.4520547945205475, 5.035087719298246, 4.181818181818182, 4.473684210526316, 4.833333333333333, 4.7272727272727275, 4.642857142857143, 4.666666666666667, 4.357142857142857, 5.5, 5.228571428571429, 4.521739130434782, 4.642857142857143, 4.96, 5.125, 4.43859649122807, 4.861111111111111, 4.117647058823529, 4.557971014492754, 5.019607843137255, 4.125, 4.578947368421052, 4.235294117647059, 4.419354838709677, 4.87248322147651, 4.761363636363637, 4.428571428571429, 5.545454545454546, 4.684210526315789, 4.2727272727272725, 4.619047619047619, 5.071428571428571, 4.769230769230769, 5.416666666666667, 4.904255319148936, 5.333333333333333, 4.545454545454546, 3.8974358974358974, 4.75, 4.875, 4.65, 4.1875, 4.894736842105263, 4.483870967741935, 4.805555555555555, 4.412698412698413, 4.380952380952381, 4.1875, 4.083333333333333, 4.375, 4.872093023255814, 4.666666666666667, 4.583333333333333, 4.818181818181818, 4.577981651376147, 4.875, 4.724137931034483, 4.0, 4.409090909090909, 4.611111111111111, 4.4375, 3.814814814814815, 4.1521739130434785, 4.851851851851852, 4.5576923076923075, 4.333333333333333, 4.666666666666667, 5.25, 6.071428571428571, 4.516129032258065, 4.695652173913044, 4.3076923076923075, 4.782608695652174, 4.7407407407407405, 4.36, 4.571428571428571, 5.28, 4.6923076923076925, 4.928571428571429, 5.066666666666666, 4.611111111111111, 4.8895348837209305, 4.266666666666667, 5.111111111111111, 4.5, 4.375, 4.435483870967742, 4.777777777777778, 4.413793103448276, 4.916666666666667, 5.25, 4.538461538461538, 4.576923076923077, 4.619047619047619, 4.428571428571429, 4.607142857142857, 4.928571428571429, 4.5, 4.515151515151516, 4.9324324324324325, 4.793103448275862, 4.304347826086956, 4.461538461538462, 4.391304347826087, 4.476190476190476, 4.288888888888889, 4.7272727272727275, 4.6875, 4.833333333333333, 4.505813953488372, 4.7407407407407405, 4.366666666666666, 5.75, 4.117647058823529, 4.433333333333334, 4.702127659574468, 4.894736842105263, 4.338235294117647, 4.074074074074074, 4.444444444444445, 4.888888888888889, 4.642857142857143, 4.206896551724138, 4.787878787878788, 5.04, 4.896551724137931, 4.333333333333333, 4.384615384615385, 4.7407407407407405, 4.578947368421052, 5.0, 4.95, 4.4523809523809526, 5.0, 4.466666666666667, 4.685714285714286, 4.714285714285714, 4.44, 4.954545454545454, 4.571428571428571, 4.423076923076923, 5.177777777777778, 4.25, 4.75, 5.6, 5.086956521739131, 4.235294117647059, 4.4, 4.454545454545454, 4.773584905660377, 5.0, 4.461538461538462, 4.7368421052631575, 4.283018867924528, 4.6875, 4.214285714285714, 4.266666666666667, 4.933333333333334, 4.71875, 4.571428571428571, 4.6875, 4.185185185185185, 4.333333333333333, 4.636363636363637, 4.38, 5.166666666666667, 4.753846153846154, 4.538461538461538, 4.2682926829268295, 4.0, 5.333333333333333, 4.409090909090909, 4.416666666666667, 5.0, 5.25, 4.627906976744186, 4.68, 4.833333333333333, 4.3125, 4.518518518518518, 4.741379310344827, 4.85, 4.5, 4.48, 4.64, 5.181818181818182, 5.0, 4.413793103448276, 4.7272727272727275, 5.181818181818182, 4.67578125, 4.333333333333333, 4.294117647058823, 5.03921568627451, 5.1, 4.421052631578948, 4.230769230769231, 4.384615384615385, 4.555555555555555, 4.027777777777778, 4.595238095238095, 5.37037037037037, 4.434782608695652, 4.690476190476191, 4.366666666666666, 4.481481481481482, 4.466666666666667, 4.425531914893617, 4.066666666666666, 4.571428571428571, 4.785714285714286, 4.21875, 4.65, 4.75, 4.714285714285714, 4.333333333333333, 4.4, 4.761904761904762, 4.617647058823529, 4.1, 4.583333333333333, 4.227848101265823, 4.238095238095238, 4.25, 5.033333333333333, 4.3, 4.608695652173913, 4.888888888888889, 4.2727272727272725, 4.714285714285714, 4.642857142857143, 4.9, 4.538461538461538, 5.555555555555555, 4.560975609756097, 4.840579710144928, 4.642857142857143, 4.25, 4.833333333333333, 4.666666666666667, 5.368421052631579, 4.642857142857143, 4.294117647058823, 4.518518518518518, 4.448275862068965, 4.395348837209302, 5.2272727272727275, 4.458333333333333, 4.75, 4.9375, 4.571428571428571, 5.052631578947368, 4.55, 5.083333333333333, 4.357142857142857, 4.714285714285714, 4.6, 4.842105263157895, 5.071428571428571, 4.615384615384615, 4.117647058823529, 4.677685950413223, 4.862068965517241, 4.448275862068965, 4.0625, 4.714285714285714, 4.833333333333333, 4.8, 4.739130434782608, 4.404761904761905, 4.712328767123288, 5.052631578947368, 4.1, 4.166666666666667, 4.733333333333333, 4.785714285714286, 4.546728971962617, 4.645161290322581, 4.892857142857143, 4.217391304347826, 4.76, 4.538461538461538, 4.37037037037037, 5.45, 4.647058823529412, 4.592592592592593, 4.980769230769231, 5.076923076923077, 4.586206896551724, 4.636363636363637, 4.733333333333333, 4.815789473684211, 4.428571428571429, 4.5, 5.116279069767442, 5.066666666666666, 4.666666666666667, 4.961538461538462, 4.48, 5.149253731343284, 4.153846153846154, 4.407407407407407, 3.923076923076923, 5.214285714285714, 5.071428571428571, 4.5, 5.444444444444445, 4.409090909090909, 4.642857142857143, 4.636363636363637, 4.5, 5.5, 4.166666666666667, 4.826086956521739, 4.625, 5.125, 4.436619718309859, 4.1875, 4.035714285714286, 4.875, 4.454545454545454, 4.709677419354839, 4.277777777777778, 4.745762711864407, 4.9, 5.116279069767442, 4.621621621621622, 4.434782608695652, 5.055555555555555, 4.416666666666667, 5.117647058823529, 4.928571428571429, 4.388888888888889, 4.354838709677419, 5.222222222222222, 4.357142857142857, 4.7, 4.909090909090909, 4.095238095238095, 4.833333333333333, 5.0, 5.136363636363637, 5.055555555555555, 4.568965517241379, 4.857142857142857, 4.2, 4.714285714285714, 4.4523809523809526, 4.722222222222222, 4.283018867924528, 4.208333333333333, 4.136363636363637, 4.108695652173913, 4.894736842105263, 4.409090909090909, 5.235294117647059, 4.521739130434782, 4.769230769230769, 5.125, 5.0, 4.774193548387097, 4.568627450980392, 4.565217391304348, 4.078431372549019, 4.148148148148148, 4.771428571428571, 4.5, 4.833333333333333, 4.325581395348837, 4.368421052631579, 4.533333333333333, 4.277777777777778, 4.916666666666667, 4.5, 4.666666666666667, 4.866666666666666, 4.5, 4.75, 4.190476190476191, 4.9375, 4.238095238095238, 4.555555555555555, 4.680851063829787, 4.65, 4.538461538461538, 4.5636363636363635, 4.72972972972973, 4.529411764705882, 4.833333333333333, 4.333333333333333, 4.5675675675675675, 4.369565217391305, 4.338709677419355, 4.454545454545454, 4.2727272727272725, 5.0, 4.866666666666666, 4.631578947368421, 4.733333333333333, 4.708333333333333, 5.2, 4.3649289099526065, 4.684210526315789, 4.76, 4.535714285714286, 4.285714285714286, 4.333333333333333, 4.368421052631579, 4.787878787878788, 4.4, 4.9, 5.045454545454546, 4.6976744186046515, 5.090909090909091, 4.947368421052632, 4.337837837837838, 4.052631578947368, 4.830508474576271, 4.95, 4.733333333333333, 4.12, 4.5, 4.4222222222222225, 4.823529411764706, 4.6923076923076925, 4.230769230769231, 4.842105263157895, 4.413793103448276, 4.352941176470588, 4.739130434782608, 4.181818181818182, 5.3076923076923075, 4.96969696969697, 4.421052631578948, 4.636363636363637, 4.583333333333333, 5.0, 4.346938775510204, 5.071428571428571, 4.111111111111111, 4.7272727272727275, 4.75, 4.833333333333333, 5.4, 4.473684210526316, 4.615384615384615, 5.421052631578948, 4.839160839160839, 4.277777777777778, 4.3, 4.541666666666667, 4.212121212121212, 4.7368421052631575, 4.043478260869565, 4.548387096774194, 4.6521739130434785, 4.571428571428571, 4.361702127659575, 4.454545454545454, 4.619047619047619, 4.3478260869565215, 4.7317073170731705, 4.684210526315789, 4.818181818181818, 4.2727272727272725, 4.675675675675675, 4.363636363636363, 4.904761904761905, 4.684210526315789, 4.545454545454546, 4.529411764705882, 4.380952380952381, 4.615384615384615, 4.576923076923077, 4.416666666666667, 4.333333333333333, 4.380952380952381, 4.52, 4.125, 4.428571428571429, 4.466666666666667, 4.71875, 4.678571428571429, 4.517241379310345, 4.636363636363637, 4.75, 4.5, 4.545454545454546, 4.463414634146342, 4.023255813953488, 4.6, 5.038461538461538, 4.132075471698113, 4.363636363636363, 4.172413793103448, 4.586956521739131, 4.396825396825397, 4.619047619047619, 4.538461538461538, 4.21875, 4.454545454545454, 4.6923076923076925, 4.391304347826087, 4.904761904761905, 4.704761904761905, 4.411764705882353, 4.616666666666666, 4.230769230769231, 4.475609756097561, 4.676470588235294, 4.260869565217392, 4.565217391304348, 4.428571428571429, 4.098039215686274, 4.525, 4.884615384615385, 4.457142857142857, 4.416666666666667, 4.833333333333333, 5.0, 4.619047619047619, 4.3, 4.433333333333334, 4.833333333333333, 4.5777777777777775, 5.0625, 4.916666666666667, 5.2727272727272725, 4.466666666666667, 4.632352941176471, 4.809523809523809, 4.5, 4.84375, 3.8333333333333335, 5.16, 4.583333333333333, 5.1, 4.608695652173913, 5.0, 4.714285714285714, 4.6521739130434785, 4.928571428571429, 5.181818181818182, 4.9411764705882355, 4.666666666666667, 4.32258064516129, 4.352941176470588, 4.533333333333333, 4.490196078431373, 4.6, 4.771428571428571, 4.918918918918919, 4.238095238095238, 5.0, 4.4, 4.59375, 4.619047619047619, 4.829268292682927, 4.541666666666667, 4.166666666666667, 4.384615384615385, 5.005847953216374, 4.631578947368421, 4.523809523809524, 4.833333333333333, 5.166666666666667, 4.609756097560975, 4.545454545454546, 4.625, 5.0, 5.0, 5.25, 4.361111111111111, 4.454545454545454, 4.7317073170731705, 4.428571428571429, 4.137931034482759, 4.7272727272727275, 4.7924528301886795, 4.722222222222222, 4.473684210526316, 4.722222222222222, 4.518518518518518, 4.555555555555555, 4.819444444444445, 4.588235294117647, 5.095238095238095, 4.470588235294118, 4.27906976744186, 4.6923076923076925, 4.62, 4.436619718309859, 4.7894736842105265, 4.517241379310345, 4.486486486486487, 4.212121212121212, 4.555555555555555, 5.0, 4.533333333333333, 4.461538461538462, 4.549019607843137, 4.444444444444445, 4.709677419354839, 4.4, 4.483333333333333, 4.638888888888889, 4.473684210526316, 4.705882352941177, 4.54054054054054, 5.066666666666666, 4.2, 4.842105263157895, 4.925925925925926, 4.518518518518518, 4.5, 4.85, 4.0625, 4.714285714285714, 4.492537313432836, 4.095238095238095, 4.303921568627451, 4.9, 4.176470588235294, 4.921568627450981, 5.0588235294117645, 4.894736842105263, 4.655172413793103, 4.434782608695652, 4.454545454545454, 4.0, 4.857142857142857, 4.861111111111111, 4.368421052631579, 4.1875, 3.8043478260869565, 4.086956521739131, 4.695652173913044, 4.466666666666667, 4.6, 4.666666666666667, 4.476190476190476, 4.545454545454546, 4.888888888888889, 4.7, 4.576271186440678, 4.685714285714286, 4.851851851851852, 4.0, 4.882352941176471, 4.885714285714286, 4.576923076923077, 4.52, 4.833333333333333, 4.62962962962963, 5.1923076923076925, 4.357142857142857, 4.6, 4.5, 4.206896551724138, 4.87292817679558, 4.675, 5.0476190476190474, 4.724137931034483, 4.842105263157895, 4.5, 4.666666666666667, 4.684210526315789, 4.708333333333333, 4.576271186440678, 4.470588235294118, 3.923076923076923, 4.333333333333333, 4.642857142857143, 4.392857142857143, 4.619047619047619, 4.880952380952381, 4.454545454545454, 4.619047619047619, 4.333333333333333, 4.833333333333333, 4.777777777777778, 4.882352941176471, 4.333333333333333, 4.188679245283019, 4.346153846153846, 4.8108108108108105, 4.375, 4.5, 4.428571428571429, 5.092592592592593, 4.391304347826087, 4.706422018348624, 4.704225352112676, 4.6, 4.375, 4.08, 4.741935483870968, 4.5636363636363635, 4.5, 4.846153846153846, 4.606060606060606, 5.0, 4.369565217391305, 4.3125, 4.90625, 4.76, 4.105263157894737, 4.6571428571428575, 5.2631578947368425, 4.909090909090909, 4.5, 4.666666666666667, 4.2727272727272725, 4.608695652173913, 4.6875, 4.363636363636363, 4.428571428571429, 4.611111111111111, 4.32, 4.285714285714286, 4.547169811320755, 4.266666666666667, 4.975609756097561, 4.714285714285714, 4.518518518518518, 4.133333333333334, 5.0588235294117645, 4.551724137931035, 3.9069767441860463, 4.208333333333333, 4.529411764705882, 4.171428571428572, 5.5, 4.7368421052631575, 4.886363636363637, 4.428571428571429, 4.761904761904762, 4.119402985074627, 4.909090909090909, 4.548780487804878, 4.7368421052631575, 5.75, 4.852941176470588, 4.2682926829268295, 4.666666666666667, 4.5, 4.833333333333333, 4.574074074074074, 4.363636363636363, 4.269230769230769, 4.32, 3.9655172413793105, 4.666666666666667, 4.473684210526316, 4.428571428571429, 4.741935483870968, 4.7560975609756095, 3.9565217391304346, 5.157894736842105, 4.448979591836735, 4.896551724137931, 4.375, 4.379310344827586, 5.032258064516129, 4.321428571428571, 4.857142857142857, 5.0344827586206895, 4.777777777777778, 4.485714285714286, 4.1923076923076925, 4.724137931034483, 4.569037656903766, 4.681818181818182, 5.0606060606060606, 4.1395348837209305, 5.090909090909091, 4.265822784810126, 4.2, 4.5, 5.125, 4.125, 5.230769230769231, 4.7, 4.875, 4.333333333333333, 4.425, 5.428571428571429, 4.521739130434782, 4.921052631578948, 4.535714285714286, 4.375, 4.725, 4.82962962962963, 4.875, 5.2, 3.6666666666666665, 5.7, 4.2631578947368425, 4.4, 4.32, 4.764705882352941, 4.7368421052631575, 4.083333333333333, 4.5, 4.2105263157894735, 4.333333333333333, 4.6923076923076925, 5.195121951219512, 4.11864406779661, 4.384615384615385, 4.185185185185185, 4.809523809523809, 4.444444444444445, 4.470588235294118, 4.7272727272727275, 4.5, 3.888888888888889, 4.368421052631579, 4.833333333333333, 4.214285714285714, 4.714285714285714, 4.185185185185185, 4.444444444444445, 4.95, 4.4375, 3.59375, 4.391304347826087, 4.5, 4.315789473684211, 4.25, 4.515151515151516, 4.875, 3.8181818181818183, 4.133333333333334, 4.571428571428571, 4.409090909090909, 4.684210526315789, 4.26, 4.625, 4.681818181818182, 4.722222222222222, 4.166666666666667, 4.964285714285714, 4.714285714285714, 4.466666666666667, 4.761904761904762, 4.85, 4.3023255813953485, 4.454545454545454, 4.310344827586207, 4.3428571428571425, 4.363636363636363, 3.8461538461538463, 4.666666666666667, 4.545454545454546, 4.931034482758621, 4.411764705882353, 4.9411764705882355, 4.8059701492537314, 4.95, 4.25, 4.4375, 4.846153846153846, 5.0, 4.260869565217392, 4.378378378378378, 4.383606557377049, 4.48, 4.340909090909091, 4.5, 4.777777777777778, 4.458333333333333, 5.038461538461538, 4.148148148148148, 5.090909090909091, 4.136363636363637, 4.576923076923077, 4.76, 4.25, 4.25, 4.644444444444445, 4.7407407407407405, 4.769230769230769, 4.717391304347826, 4.866666666666666, 5.117647058823529, 4.46875, 4.642857142857143, 4.357142857142857, 5.055555555555555, 4.363636363636363, 4.65, 4.615384615384615, 4.166666666666667, 4.548387096774194, 4.4, 4.580152671755725, 4.707692307692308, 5.1, 4.8, 4.5, 4.733333333333333, 5.181818181818182, 4.511904761904762, 5.0, 4.380952380952381, 5.071428571428571, 5.294117647058823, 4.647058823529412, 4.333333333333333, 4.142857142857143, 4.4, 4.454545454545454, 4.555555555555555, 4.571428571428571, 4.878787878787879, 4.538461538461538, 4.791666666666667, 4.709302325581396, 4.333333333333333, 4.121951219512195, 4.714285714285714, 5.892857142857143, 4.742857142857143, 4.1875, 5.066666666666666, 4.954545454545454, 5.0476190476190474, 4.3076923076923075, 4.333333333333333, 4.842105263157895, 4.766666666666667, 4.224137931034483, 3.953488372093023, 4.7272727272727275, 4.285714285714286, 4.388888888888889, 4.333333333333333, 4.296296296296297, 4.6, 4.390243902439025, 5.5, 5.5, 4.508771929824562, 4.2592592592592595, 4.222222222222222, 4.217391304347826, 4.833333333333333, 4.65, 4.647058823529412, 4.27027027027027, 5.294117647058823, 4.363636363636363, 5.0, 4.377777777777778, 4.441860465116279, 4.64, 4.703557312252965, 4.368421052631579, 5.176470588235294, 4.402985074626866, 4.1, 4.967032967032967, 4.530612244897959, 5.086956521739131, 4.0, 5.615384615384615, 4.9411764705882355, 4.8, 4.612903225806452, 4.204545454545454, 4.571428571428571, 4.408163265306122, 4.183673469387755, 4.428571428571429, 4.9714285714285715, 4.75, 4.392857142857143, 4.142857142857143, 4.7272727272727275, 4.584745762711864, 4.6875, 4.733333333333333, 4.523809523809524, 5.066666666666666, 4.619047619047619, 4.571428571428571, 4.777777777777778, 5.2272727272727275, 3.7547169811320753, 4.357142857142857, 4.8, 5.095238095238095, 4.681818181818182, 4.458333333333333, 4.357142857142857, 4.321428571428571, 4.4363636363636365, 4.933333333333334, 4.777777777777778, 5.0606060606060606, 4.5, 5.052631578947368, 4.454545454545454, 4.48, 4.875, 4.527777777777778, 4.555555555555555, 4.708333333333333, 5.111111111111111, 4.6, 4.518518518518518, 4.133333333333334, 4.388888888888889, 3.8518518518518516, 4.589285714285714, 4.94, 4.647058823529412, 4.45, 4.753623188405797, 4.304347826086956, 4.647058823529412, 4.5625, 5.035714285714286, 4.379310344827586, 5.0, 4.4411764705882355, 4.384615384615385, 4.333333333333333, 5.0, 5.0, 5.275862068965517, 5.0, 4.928571428571429, 4.40625, 4.157894736842105, 5.8, 4.684210526315789, 4.62962962962963, 4.75, 4.585365853658536, 4.739130434782608, 4.392857142857143, 4.4, 4.414414414414415, 4.67741935483871, 4.5, 4.428571428571429, 4.565217391304348, 4.421052631578948, 5.8, 4.219178082191781, 4.8, 4.641025641025641, 4.8, 4.809523809523809, 4.705882352941177, 4.75, 4.608695652173913, 4.647058823529412, 4.375, 4.5038167938931295, 5.545454545454546, 5.181818181818182, 4.387096774193548, 4.346938775510204, 4.5, 4.7407407407407405, 4.357142857142857, 4.676470588235294, 4.417721518987341, 4.464285714285714, 4.586206896551724, 5.133333333333334, 4.285714285714286, 4.71875, 4.761904761904762, 4.84, 5.0588235294117645, 5.0, 4.365853658536586, 4.25, 4.793103448275862, 4.480769230769231, 4.636363636363637, 4.258064516129032, 4.429245283018868, 5.0, 5.2727272727272725, 4.72, 4.56, 4.1, 4.545454545454546, 4.823529411764706, 4.970588235294118, 4.523809523809524, 4.157894736842105, 4.375, 5.3125, 4.615384615384615, 4.645161290322581, 4.678571428571429, 4.739130434782608, 4.424242424242424, 4.375, 4.4375, 4.410256410256411, 5.45, 4.530612244897959, 4.818181818181818, 4.630434782608695, 5.25, 4.954545454545454, 5.166666666666667, 3.8780487804878048, 4.549450549450549, 5.05, 4.666666666666667, 4.4, 4.71830985915493, 4.944444444444445, 4.875, 4.923076923076923, 4.125, 4.59375, 4.8, 4.9, 4.68, 4.777777777777778, 4.318181818181818, 4.928571428571429, 4.296296296296297, 4.428571428571429, 4.7894736842105265, 4.156424581005586, 4.722222222222222, 4.833333333333333, 5.181818181818182, 4.615384615384615, 4.318181818181818, 4.1923076923076925, 5.583333333333333, 5.0, 4.391304347826087, 4.739130434782608, 4.428571428571429, 4.933333333333334, 4.184210526315789, 4.666666666666667, 4.333333333333333, 5.011111111111111, 4.787878787878788, 5.4, 4.378378378378378, 5.0344827586206895, 4.634408602150538, 4.45, 4.423076923076923, 5.4, 5.222222222222222, 4.454545454545454, 4.533333333333333, 4.25, 4.320754716981132, 4.3125, 4.56, 4.354166666666667, 4.875, 4.36, 4.162790697674419, 4.956521739130435, 4.78125, 4.190476190476191, 5.071428571428571, 4.052631578947368, 4.344827586206897, 4.764705882352941, 4.266666666666667, 4.294117647058823, 4.733333333333333, 4.717948717948718, 4.294117647058823, 4.733333333333333, 4.2272727272727275, 4.764705882352941, 4.75, 4.85, 4.777777777777778, 4.16, 3.9302325581395348, 4.133333333333334, 4.222222222222222, 4.166666666666667, 3.888888888888889, 4.608695652173913, 4.7, 4.40625, 5.0, 4.466666666666667, 4.681818181818182, 4.25, 4.615384615384615, 4.863636363636363, 4.662100456621005, 4.681818181818182, 4.285714285714286, 4.166666666666667, 4.666666666666667, 4.142857142857143, 4.222222222222222, 4.75, 4.785714285714286, 4.275862068965517, 4.482758620689655, 4.4, 4.476190476190476, 4.428571428571429, 4.449275362318841, 4.5, 4.533333333333333, 3.68, 4.444444444444445, 4.675, 4.396551724137931, 3.4615384615384617, 5.0, 4.421052631578948, 4.142857142857143, 4.9411764705882355, 4.428571428571429, 4.125, 4.346153846153846, 4.75, 4.105263157894737, 4.133333333333334, 5.02, 4.666666666666667, 4.5, 4.838709677419355, 4.5, 4.3532934131736525, 5.2, 4.6, 4.238095238095238, 4.304347826086956, 4.842105263157895, 5.2682926829268295, 4.333333333333333, 4.633333333333334, 4.545454545454546, 4.333333333333333, 5.533333333333333, 4.513513513513513, 4.4, 4.5636363636363635, 4.857142857142857, 4.411764705882353, 4.25, 4.85, 4.833333333333333, 4.75, 4.537037037037037, 5.111111111111111, 5.0, 4.5, 5.0, 4.0588235294117645, 4.666666666666667, 4.571428571428571, 4.826086956521739, 4.779411764705882, 4.297872340425532, 4.5625, 4.59375, 4.445103857566766, 5.0, 4.230769230769231, 4.545454545454546, 4.833333333333333, 4.323529411764706, 4.35, 4.419354838709677, 4.6, 4.3, 4.2727272727272725, 4.571428571428571, 5.5, 4.478260869565218, 4.705882352941177, 4.636363636363637, 4.435294117647059, 3.8421052631578947, 4.21875, 4.9655172413793105, 4.578947368421052, 4.320754716981132, 4.521739130434782, 4.285714285714286, 4.214285714285714, 4.4, 4.916666666666667, 4.195121951219512, 4.966101694915254, 4.9, 4.8, 4.608695652173913, 4.545454545454546, 4.424242424242424, 4.111111111111111, 4.714285714285714, 4.5, 5.043478260869565, 4.8, 4.696428571428571, 5.125, 5.157894736842105, 4.75, 4.961538461538462, 4.22, 4.388888888888889, 5.222222222222222, 5.142857142857143, 4.833333333333333, 4.048780487804878, 4.705882352941177, 5.25, 4.0, 4.424242424242424, 4.34375, 4.48, 4.25, 4.777777777777778, 4.4, 5.0, 4.440677966101695, 4.928571428571429, 4.607142857142857, 4.904761904761905, 5.1875, 4.923076923076923, 3.9310344827586206, 4.583333333333333, 4.846153846153846, 5.056603773584905, 4.708333333333333, 4.421052631578948, 3.75, 4.595238095238095, 4.162162162162162, 4.777777777777778, 4.7317073170731705, 4.724137931034483, 4.881443298969073, 4.6, 4.866666666666666, 4.625, 5.083333333333333, 4.333333333333333, 4.0476190476190474, 4.733333333333333, 4.851485148514851, 4.333333333333333, 4.72, 4.75, 4.603174603174603, 4.333333333333333, 5.0, 4.833333333333333, 4.0, 4.8, 4.838709677419355, 4.75, 4.25, 4.62962962962963, 4.526315789473684, 4.516129032258065, 4.666666666666667, 5.529411764705882, 4.7, 5.666666666666667, 4.538461538461538, 4.371428571428571, 4.745454545454545, 5.0, 4.655172413793103, 4.695652173913044, 4.612903225806452, 4.421052631578948, 4.341463414634147, 4.46875, 4.138888888888889, 4.340909090909091, 4.2727272727272725, 4.666666666666667, 4.846153846153846, 4.2272727272727275, 4.647058823529412, 4.214285714285714, 4.7894736842105265, 4.416666666666667, 4.298507462686567, 4.2, 4.3, 4.0476190476190474, 4.7, 4.914285714285715, 4.746753246753247, 4.333333333333333, 4.5, 4.260869565217392, 4.6, 4.506410256410256, 4.115384615384615, 4.407407407407407, 3.896551724137931, 5.0, 5.295454545454546, 4.2727272727272725, 4.5, 5.052631578947368, 5.085106382978723, 5.076923076923077, 4.53125, 4.538461538461538, 4.6, 4.448275862068965, 4.515151515151516, 5.166666666666667, 4.8, 4.319148936170213, 4.838709677419355, 4.5, 4.166666666666667, 4.1875, 4.34375, 4.5, 4.423076923076923, 4.681818181818182, 4.275, 4.852941176470588, 4.48936170212766, 4.583333333333333, 4.409090909090909, 4.827586206896552, 4.78125, 4.813559322033898, 4.766666666666667, 4.28125, 4.538461538461538, 3.892857142857143, 4.695652173913044, 4.382352941176471, 4.796610169491525, 3.5714285714285716, 5.0, 4.627906976744186, 4.434782608695652, 4.16, 4.363636363636363, 4.846153846153846, 4.565217391304348, 4.521739130434782, 4.833333333333333, 4.3125, 4.60377358490566, 4.052631578947368, 4.814814814814815, 3.963636363636364, 5.117647058823529, 4.375, 4.571428571428571, 4.947368421052632, 4.894736842105263, 4.090909090909091, 4.5, 4.565217391304348, 4.9523809523809526, 5.119658119658119, 4.621621621621622, 4.3478260869565215, 4.142857142857143, 4.65, 4.925925925925926, 4.076923076923077, 4.0, 4.611111111111111, 4.615384615384615, 5.0, 4.625, 4.888888888888889, 4.411764705882353, 4.363636363636363, 4.454545454545454, 4.666666666666667, 4.857142857142857, 4.5, 4.866666666666666, 4.285714285714286, 4.590909090909091, 4.611111111111111, 4.2631578947368425, 4.875, 4.5, 5.315789473684211, 4.5625, 4.290322580645161, 4.947368421052632, 4.275862068965517, 4.55, 4.947368421052632, 5.181818181818182, 4.666666666666667, 4.176470588235294, 4.705882352941177, 4.380952380952381, 4.8, 4.5, 4.523809523809524, 5.157894736842105, 4.368421052631579, 4.958333333333333, 4.866666666666666, 4.785714285714286, 4.434210526315789, 4.853658536585366, 4.037037037037037, 4.65, 4.576923076923077, 4.45, 4.909090909090909, 4.5588235294117645, 4.944444444444445, 5.0, 4.666666666666667, 4.233333333333333, 5.384615384615385, 4.595744680851064, 4.78125, 4.371428571428571, 5.175, 4.178571428571429, 4.444444444444445, 4.6875, 4.547169811320755, 4.666666666666667, 4.857142857142857, 4.3478260869565215, 4.454545454545454, 4.576923076923077, 4.611111111111111, 4.538461538461538, 4.388888888888889, 5.117647058823529, 4.8231292517006805, 4.473684210526316, 4.666666666666667, 4.717391304347826, 4.428571428571429, 5.056818181818182, 5.0, 4.85, 4.523809523809524, 3.5625, 4.444444444444445, 3.925925925925926, 4.916666666666667, 4.2, 3.95, 4.083333333333333, 4.2, 5.011111111111111, 4.423076923076923, 5.066666666666666, 5.090909090909091, 4.529411764705882, 4.833333333333333, 4.266666666666667, 5.0, 4.291666666666667, 4.142857142857143, 4.705882352941177, 4.5, 4.6, 4.5, 5.0, 4.833333333333333, 4.818181818181818, 4.909090909090909, 4.405797101449275, 4.0625, 4.388888888888889, 5.1, 4.967741935483871, 4.785714285714286, 4.411764705882353, 4.45, 4.0, 4.538461538461538, 4.95, 4.368421052631579, 4.5, 4.5, 4.7, 4.190476190476191, 5.222222222222222, 4.411764705882353, 4.75, 5.0, 4.19327731092437, 4.015384615384615, 4.428571428571429, 4.944444444444445, 4.947368421052632, 4.285714285714286, 4.714285714285714, 4.9375, 4.636363636363637, 4.333333333333333, 4.764705882352941, 4.0, 4.540650406504065, 4.1521739130434785, 5.137931034482759, 4.529411764705882, 5.071428571428571, 4.5625, 4.541666666666667, 4.684210526315789, 4.571428571428571, 4.8076923076923075, 4.458333333333333, 4.724137931034483, 4.142857142857143, 4.666666666666667, 4.434782608695652, 4.833333333333333, 4.571428571428571, 4.625, 4.3584905660377355, 4.208333333333333, 4.8807339449541285, 4.611111111111111, 4.566666666666666, 5.0, 4.416666666666667, 5.111111111111111, 4.541666666666667, 4.887387387387387, 4.5, 3.9375, 4.743589743589744, 4.333333333333333, 4.456521739130435, 4.090909090909091, 4.904761904761905, 4.777777777777778, 4.7272727272727275, 4.4, 4.363636363636363, 4.45, 4.846153846153846, 4.215189873417722, 4.0, 4.580645161290323, 5.2, 4.821052631578947, 4.208333333333333, 4.130434782608695, 4.733333333333333, 4.818181818181818, 4.771428571428571, 4.966666666666667, 4.45, 4.421052631578948, 4.823529411764706, 4.958333333333333, 4.583333333333333, 5.464285714285714, 4.714285714285714, 4.555555555555555, 4.333333333333333, 4.695652173913044, 4.89, 4.911764705882353, 5.2727272727272725, 5.294117647058823, 4.35, 4.88, 4.266666666666667, 4.238095238095238, 5.041666666666667, 5.0, 4.466666666666667, 4.416666666666667, 4.583333333333333, 4.37037037037037, 4.233333333333333, 4.9, 4.333333333333333, 4.3, 4.777777777777778, 4.363636363636363, 4.767857142857143, 4.3125, 5.333333333333333, 5.095238095238095, 4.628571428571429, 4.368421052631579, 4.8, 4.739130434782608, 4.571428571428571, 4.415094339622642, 4.608695652173913, 4.76, 3.6153846153846154, 4.405063291139241, 4.2, 4.111111111111111, 4.857142857142857, 5.06, 4.833333333333333, 4.277777777777778, 5.25, 5.25, 4.222222222222222, 4.5, 4.342105263157895, 4.517241379310345, 4.928571428571429, 4.7894736842105265, 4.232323232323233, 4.583333333333333, 4.9, 4.636363636363637, 3.891891891891892, 4.865384615384615, 4.423076923076923, 4.238095238095238, 4.666666666666667, 4.608695652173913, 5.12, 4.916666666666667, 5.052631578947368, 4.4375, 4.214285714285714, 4.933333333333334, 4.035714285714286, 4.458064516129032, 4.8, 4.446153846153846, 4.413793103448276, 4.653846153846154, 4.064516129032258, 4.846153846153846, 4.266666666666667, 4.580645161290323, 4.421052631578948, 4.666666666666667, 4.3, 4.115384615384615, 4.384615384615385, 4.434782608695652, 4.512195121951219, 4.5, 4.148148148148148, 4.304347826086956, 4.444444444444445, 4.535714285714286, 4.43010752688172, 4.8, 4.689655172413793, 4.64, 4.655172413793103, 4.576923076923077, 3.642857142857143, 4.443037974683544, 4.666666666666667, 4.755102040816326, 4.346153846153846, 4.551724137931035, 4.357142857142857, 4.5, 4.333333333333333, 4.05, 4.291666666666667, 4.538461538461538, 4.86896551724138, 4.565217391304348, 4.833333333333333, 4.703389830508475, 4.603174603174603, 4.52, 4.6075949367088604, 4.208333333333333, 4.565217391304348, 4.603448275862069, 5.147058823529412, 5.083333333333333, 5.095238095238095, 4.411764705882353, 4.766666666666667, 4.555555555555555, 4.966666666666667, 4.352941176470588, 4.642857142857143, 4.2727272727272725, 5.1, 4.313725490196078, 5.25, 4.168421052631579, 4.0, 4.883116883116883, 4.28, 4.666666666666667, 5.2, 4.8, 4.666666666666667, 5.16, 4.6923076923076925, 4.333333333333333, 4.809523809523809, 4.937853107344632, 4.833333333333333, 4.85, 4.909090909090909, 5.166666666666667, 4.827586206896552, 4.0, 4.586206896551724, 4.473684210526316, 4.633333333333334, 4.526315789473684, 4.586206896551724, 4.8, 4.157894736842105, 4.375, 4.529411764705882, 4.35, 4.636363636363637, 5.388888888888889, 4.533333333333333, 4.607142857142857, 4.68, 5.0, 5.021660649819495, 4.16, 4.648148148148148, 4.34375, 4.7727272727272725, 4.636363636363637, 4.55, 5.292307692307692, 4.533333333333333, 4.619047619047619, 4.592592592592593, 4.785087719298246, 5.764705882352941, 4.380952380952381, 4.394736842105263, 4.9, 4.833333333333333, 4.368421052631579, 4.888888888888889, 4.814634146341463, 4.739130434782608, 4.823529411764706, 4.75, 4.352941176470588, 4.184210526315789, 4.8125, 4.25, 4.305555555555555, 4.787878787878788, 4.948717948717949, 4.9523809523809526, 4.5, 5.142857142857143, 4.6, 4.666666666666667, 4.285714285714286, 4.8474576271186445, 4.954545454545454, 4.352941176470588, 5.428571428571429, 4.8, 5.037037037037037, 3.967741935483871, 4.357142857142857, 4.695652173913044, 4.096774193548387, 4.642857142857143, 3.8125, 4.875, 4.1923076923076925, 4.818181818181818, 5.2105263157894735, 4.606060606060606, 4.604166666666667, 4.785714285714286, 4.7894736842105265, 4.583333333333333, 4.25, 4.7317073170731705, 4.586206896551724, 4.6, 4.631578947368421, 4.326530612244898, 4.872340425531915, 4.6, 4.464285714285714, 4.319148936170213, 4.663366336633663, 5.428571428571429, 4.0, 4.56, 4.928571428571429, 4.3, 4.6, 5.0, 4.6, 4.875, 4.612903225806452, 4.317073170731708, 5.125, 4.555555555555555, 4.7272727272727275, 5.0, 4.6571428571428575, 4.21875, 5.545454545454546, 4.571428571428571, 4.954545454545454, 4.916666666666667, 4.75, 4.6891891891891895, 4.638297872340425, 4.1, 4.583333333333333, 4.4125874125874125, 4.775, 4.545454545454546, 4.513513513513513, 4.928571428571429, 4.375, 4.45, 5.076923076923077, 4.703703703703703, 4.344827586206897, 5.142857142857143, 4.291666666666667, 4.841584158415841, 4.0, 4.916666666666667, 4.409090909090909, 4.574257425742574, 4.133333333333334, 4.136363636363637, 4.807228915662651, 4.761904761904762, 3.727272727272727, 4.311475409836065, 4.576923076923077, 4.4, 4.488372093023256, 4.75, 4.714285714285714, 4.923076923076923, 4.470588235294118, 4.529411764705882, 4.391304347826087, 4.571428571428571, 4.50561797752809, 4.714285714285714, 4.277777777777778, 4.678571428571429, 4.090909090909091, 5.444444444444445, 4.3, 5.0, 4.647058823529412, 4.916666666666667, 4.809523809523809, 5.140350877192983, 4.696969696969697, 4.333333333333333, 4.636363636363637, 4.8, 4.806451612903226, 4.517241379310345, 4.769230769230769, 4.538461538461538, 4.4, 5.086956521739131, 4.46, 4.3076923076923075, 4.5476190476190474, 4.771428571428571, 4.5, 4.206896551724138, 4.705882352941177, 4.725, 5.125, 4.153846153846154, 4.925, 4.6, 4.611111111111111, 4.28, 5.0, 4.705882352941177, 4.416666666666667, 4.615384615384615, 4.346153846153846, 4.947368421052632, 4.846153846153846, 4.0, 4.619047619047619, 4.565217391304348, 4.352941176470588, 4.566666666666666, 4.62962962962963, 5.340425531914893, 4.916666666666667, 5.333333333333333, 4.508196721311475, 4.516483516483516, 4.35, 4.2, 4.730769230769231, 4.6, 4.833333333333333, 4.785714285714286, 4.95, 4.115384615384615, 4.380952380952381, 3.9047619047619047, 4.333333333333333, 4.409090909090909, 5.166666666666667, 4.714285714285714, 4.380952380952381, 4.588235294117647, 5.05, 5.333333333333333, 4.708333333333333, 4.304347826086956, 6.0, 5.0, 4.625, 4.423076923076923, 4.115384615384615, 4.418181818181818, 4.419354838709677, 4.756756756756757, 4.818181818181818, 4.785714285714286, 4.875, 4.777777777777778, 4.2592592592592595, 4.1891891891891895, 4.583333333333333, 3.8461538461538463, 4.428571428571429, 4.826086956521739, 4.538461538461538, 4.48, 4.411764705882353, 4.898305084745763, 4.516129032258065, 4.615384615384615, 4.111111111111111, 4.9375, 4.390243902439025, 4.846153846153846, 5.0, 4.054794520547945, 4.7272727272727275, 4.612903225806452, 5.269230769230769, 4.454545454545454, 5.0, 4.35, 4.791666666666667, 4.375, 4.265625, 4.833333333333333, 4.451612903225806, 4.366666666666666, 5.0, 4.796610169491525, 5.0588235294117645, 4.809523809523809, 5.176470588235294, 4.521739130434782, 4.384615384615385, 4.478260869565218, 4.95, 4.925, 4.5625, 4.5476190476190474, 5.0, 5.044444444444444, 4.625, 4.555555555555555, 4.809523809523809, 5.333333333333333, 4.2272727272727275, 4.904761904761905, 3.763157894736842, 4.4375, 4.75, 4.714285714285714, 4.753623188405797, 4.666666666666667, 4.545454545454546, 4.2439024390243905, 5.421052631578948, 4.4375, 4.533333333333333, 4.4, 4.322033898305085, 4.291139240506329, 4.684210526315789, 4.25, 4.857142857142857, 5.444444444444445, 4.037037037037037, 4.5, 4.947368421052632, 4.666666666666667, 4.625, 4.717948717948718, 4.6, 4.818181818181818, 4.25, 3.9444444444444446, 4.933333333333334, 4.390625, 3.875, 4.464285714285714, 4.591836734693878, 4.680412371134021, 4.380952380952381, 5.103448275862069, 4.275862068965517, 5.142857142857143, 4.5, 4.228571428571429, 4.043478260869565, 4.695652173913044, 4.5, 4.5, 4.473684210526316, 4.4375, 5.0344827586206895, 4.08, 4.5, 4.617647058823529, 4.294117647058823, 4.678571428571429, 4.6, 4.151162790697675, 4.545454545454546, 4.67741935483871, 4.159090909090909, 4.174603174603175, 4.666666666666667, 4.066666666666666, 4.769230769230769, 4.142857142857143, 3.923076923076923, 4.52112676056338, 4.910256410256411, 4.588235294117647, 4.678571428571429, 5.0, 5.0, 4.611111111111111, 4.533333333333333, 4.583333333333333, 4.444444444444445, 4.470588235294118, 4.636363636363637, 4.54320987654321, 4.666666666666667, 4.2272727272727275, 4.7, 4.225, 5.0, 4.3, 4.90625, 4.888888888888889, 4.605263157894737, 4.818181818181818, 4.2, 4.260869565217392, 4.48, 4.464285714285714, 4.3, 4.411764705882353, 4.529411764705882, 5.173913043478261, 4.882352941176471, 4.793103448275862, 4.809523809523809, 4.909090909090909, 4.454545454545454, 4.214285714285714, 5.0, 4.483870967741935, 4.521739130434782, 4.440298507462686, 4.666666666666667, 4.333333333333333, 4.5, 4.714285714285714, 5.105263157894737, 4.481481481481482, 4.56, 4.409090909090909, 4.389380530973451, 4.105263157894737, 4.824324324324325, 5.038461538461538, 4.777777777777778, 4.4, 4.342592592592593, 4.365384615384615, 4.266666666666667, 4.266666666666667, 4.777777777777778, 4.714285714285714, 4.190476190476191, 5.4, 4.578947368421052, 4.777777777777778, 4.303030303030303, 4.55, 4.454545454545454, 4.67741935483871, 4.197183098591549, 4.409090909090909, 4.444444444444445, 4.576923076923077, 4.35, 4.389473684210526, 4.3478260869565215, 4.46, 4.583333333333333, 4.473684210526316, 4.333333333333333, 4.504347826086956, 4.172413793103448, 5.007462686567164, 4.769230769230769, 4.634615384615385, 4.782608695652174, 4.678571428571429, 4.136363636363637, 4.62962962962963, 4.151515151515151, 4.5, 5.0, 5.214285714285714, 4.217391304347826, 4.25, 5.3076923076923075, 4.809523809523809, 4.25, 5.0, 4.6923076923076925, 4.846153846153846, 4.416666666666667, 5.2, 4.462585034013605, 4.652777777777778, 4.7, 4.63265306122449, 4.833333333333333, 4.583333333333333, 4.352941176470588, 4.595505617977528, 4.25, 4.421052631578948, 4.3428571428571425, 5.125, 3.8518518518518516, 5.333333333333333, 4.769230769230769, 4.4, 4.545454545454546, 4.476923076923077, 4.357142857142857, 5.088888888888889, 4.46, 4.423076923076923, 4.642857142857143, 4.5, 4.375, 4.2, 4.6521739130434785, 4.666666666666667, 4.550561797752809, 4.586206896551724, 4.716666666666667, 6.333333333333333, 4.486486486486487, 4.333333333333333, 4.56, 4.705882352941177, 4.578947368421052, 4.777777777777778, 4.285714285714286, 3.823529411764706, 4.904761904761905, 4.476190476190476, 4.714285714285714, 4.631578947368421, 4.777777777777778, 4.3478260869565215, 4.466666666666667, 4.764705882352941, 4.0, 4.489795918367347, 4.980769230769231, 4.631578947368421, 4.44, 5.1, 4.214285714285714, 4.56, 4.285714285714286, 4.363636363636363, 4.888888888888889, 5.0, 4.571428571428571, 4.634146341463414, 5.588235294117647, 5.384615384615385, 4.722222222222222, 5.5, 4.666666666666667, 4.44, 4.458333333333333, 4.653846153846154, 4.238095238095238, 4.0588235294117645, 4.2439024390243905, 4.428571428571429, 3.914285714285714, 4.717948717948718, 4.440217391304348, 4.821917808219178, 4.9411764705882355, 4.482758620689655, 5.076923076923077, 4.327272727272727, 5.085106382978723, 4.730769230769231, 5.0, 4.538461538461538, 4.5, 4.48, 4.787234042553192, 4.346153846153846, 4.1, 4.4, 4.64, 4.5, 4.773584905660377, 4.2875, 4.588235294117647, 4.52, 4.185185185185185, 4.451612903225806, 4.695652173913044, 4.36, 4.8108108108108105, 4.026315789473684, 4.923076923076923, 4.45, 4.523809523809524, 4.166666666666667, 4.242424242424242, 4.193548387096774, 5.543478260869565, 4.470588235294118, 4.529411764705882, 4.7936507936507935, 4.703703703703703, 4.739130434782608, 4.542857142857143, 5.0, 4.481012658227848, 4.824175824175824, 4.5625, 4.886877828054299, 4.25, 4.2631578947368425, 4.8076923076923075, 4.483870967741935, 4.190476190476191, 4.816091954022989, 4.6, 4.0, 4.346153846153846, 4.636363636363637, 4.37037037037037, 5.142857142857143, 4.857142857142857, 4.714285714285714, 4.0, 4.84, 4.478260869565218, 4.894736842105263, 4.3, 5.235294117647059, 4.391304347826087, 4.7, 4.2272727272727275, 4.361111111111111, 4.645161290322581, 4.571428571428571, 4.266666666666667, 4.880952380952381, 4.5, 5.1875, 4.761904761904762, 4.45945945945946, 4.464285714285714, 4.866666666666666, 4.576923076923077, 4.785714285714286, 4.533333333333333, 4.565217391304348, 4.666666666666667, 4.590909090909091, 4.666666666666667, 4.611111111111111, 4.535714285714286, 4.083333333333333, 4.5, 4.166666666666667, 4.777777777777778, 4.416666666666667, 4.863636363636363, 4.181818181818182, 4.804878048780488, 4.8, 4.541666666666667, 4.344827586206897, 5.125, 4.36, 4.8, 4.538461538461538, 4.780821917808219, 4.46875, 4.125, 4.2105263157894735, 4.645161290322581, 4.875862068965517, 4.235294117647059, 4.571428571428571, 4.285714285714286, 4.545454545454546, 4.09375, 4.44, 4.886792452830188, 4.909090909090909, 4.533333333333333, 4.631578947368421, 4.305555555555555, 4.914285714285715, 4.260869565217392, 4.424242424242424, 4.7, 4.62962962962963, 4.388888888888889, 4.3076923076923075, 4.6875, 4.869565217391305, 4.375, 4.695945945945946, 5.095238095238095, 4.0, 5.375, 4.708333333333333, 4.363636363636363, 4.666666666666667, 4.090909090909091, 4.086956521739131, 4.475, 5.0, 4.739130434782608, 4.32, 4.705882352941177, 4.333333333333333, 5.147058823529412, 5.0, 4.8076923076923075, 4.647058823529412, 4.901408450704225, 4.35, 5.352941176470588, 5.0, 4.4, 4.193548387096774, 5.166666666666667, 5.333333333333333, 4.0588235294117645, 4.294117647058823, 4.266666666666667, 4.533333333333333, 4.283783783783784, 4.5, 4.222222222222222, 4.818181818181818, 4.3882352941176475, 5.7, 4.9, 3.935483870967742, 4.818181818181818, 4.285714285714286, 4.7272727272727275, 4.607142857142857, 4.888888888888889, 5.024390243902439, 4.296296296296297, 4.782945736434108, 4.809523809523809, 4.838709677419355, 4.5, 4.173913043478261, 4.352941176470588, 4.45, 5.222222222222222, 5.461538461538462, 4.222222222222222, 4.08, 4.636363636363637, 4.8076923076923075, 5.0, 4.907407407407407, 4.6521739130434785, 4.351351351351352, 4.3023255813953485, 4.523076923076923, 4.5, 4.083333333333333, 4.782608695652174, 4.842105263157895, 4.823529411764706, 4.878048780487805, 4.75, 4.666666666666667, 4.604651162790698, 4.329411764705882, 4.571428571428571, 4.407407407407407, 4.7388059701492535, 4.777777777777778, 5.055555555555555, 4.481481481481482, 5.066666666666666, 4.967741935483871, 4.821428571428571, 4.65, 4.153846153846154, 4.64, 4.821428571428571, 4.733333333333333, 4.64, 4.157894736842105, 4.848, 4.405405405405405, 5.12, 5.15, 5.769230769230769, 3.7142857142857144, 4.5, 4.44, 4.583333333333333, 4.64, 4.642857142857143, 4.444444444444445, 4.875, 4.15, 4.5, 4.666666666666667, 4.469387755102041, 4.391304347826087, 4.6, 4.607142857142857, 4.3, 4.833333333333333, 4.826086956521739, 4.0, 3.9523809523809526, 5.17948717948718, 4.571428571428571, 4.318181818181818, 4.409090909090909, 4.454545454545454, 4.866666666666666, 5.0, 4.913043478260869, 4.486486486486487, 4.458333333333333, 4.464285714285714, 4.818181818181818, 4.733333333333333, 4.875, 3.8947368421052633, 4.379310344827586, 4.846153846153846, 4.8, 4.6, 4.947368421052632, 4.571428571428571, 4.514285714285714, 4.333333333333333, 4.125, 4.117647058823529, 4.604166666666667, 4.5, 4.25, 4.764705882352941, 4.586206896551724, 3.8636363636363638, 4.523809523809524, 4.612903225806452, 4.5, 4.5, 4.375, 3.9545454545454546, 4.8, 4.428571428571429, 4.2631578947368425, 6.076923076923077, 4.607142857142857, 4.450980392156863, 4.379310344827586, 4.375, 4.76, 4.7, 5.25, 4.833333333333333, 4.966666666666667, 5.128571428571429, 4.448979591836735, 4.888888888888889, 5.444444444444445, 4.428571428571429, 4.380952380952381, 4.326315789473684, 4.375, 4.592592592592593, 4.571428571428571, 4.666666666666667, 5.040816326530612, 4.4, 4.954545454545454, 4.5, 4.8108108108108105, 4.333333333333333, 4.222222222222222, 4.96875, 4.588235294117647, 4.483870967741935, 4.7272727272727275, 4.742857142857143, 5.0, 4.217391304347826, 4.48, 4.294117647058823, 4.409090909090909, 4.65, 4.619047619047619, 4.428571428571429, 4.363636363636363, 4.235294117647059, 4.333333333333333, 5.071428571428571, 4.535714285714286, 4.739130434782608, 4.222222222222222, 4.4, 4.913043478260869, 4.571428571428571, 3.9375, 4.902777777777778, 5.012987012987013, 4.461538461538462, 4.6, 4.142857142857143, 4.586206896551724, 4.130434782608695, 4.975609756097561, 4.744186046511628, 4.095238095238095, 4.535714285714286, 4.476190476190476, 4.181818181818182, 4.5, 5.166666666666667, 4.023255813953488, 4.368421052631579, 4.379310344827586, 4.6, 4.8, 4.947368421052632, 5.6, 5.25, 5.21875, 4.767441860465116, 5.095238095238095, 4.875, 4.033333333333333, 4.575, 4.923076923076923, 4.553191489361702, 4.52, 4.185185185185185, 4.925925925925926, 5.222222222222222, 4.478260869565218, 4.612903225806452, 4.636363636363637, 4.7272727272727275, 4.666666666666667, 5.113636363636363, 4.36, 4.25, 4.269230769230769, 4.55, 4.378378378378378, 4.318181818181818, 4.388888888888889, 4.737704918032787, 4.833333333333333, 4.7727272727272725, 4.478260869565218, 4.536585365853658, 4.478260869565218, 4.695652173913044, 4.375, 5.230769230769231, 5.0476190476190474, 5.0588235294117645, 5.071428571428571, 4.492063492063492, 4.571428571428571, 4.454545454545454, 4.555555555555555, 5.125, 4.857142857142857, 4.348837209302325, 4.7894736842105265, 4.925233644859813, 4.909090909090909, 4.5, 4.228571428571429, 4.391304347826087, 5.2, 4.421052631578948, 4.21875, 4.25, 4.529411764705882, 4.457142857142857, 4.944444444444445, 4.066666666666666, 4.6, 4.439024390243903, 5.238095238095238, 4.6521739130434785, 4.615384615384615, 4.323529411764706, 4.473684210526316, 4.476190476190476, 5.054545454545455, 4.52, 4.714285714285714, 4.888888888888889, 4.533333333333333, 4.627906976744186, 5.857142857142857, 4.529411764705882, 4.909090909090909, 4.8125, 4.363636363636363, 4.56, 5.074074074074074, 4.478260869565218, 5.444444444444445, 4.6875, 4.538461538461538, 4.471698113207547, 4.944444444444445, 5.0, 4.84, 4.833333333333333, 4.526315789473684, 4.6, 4.7631578947368425, 4.56140350877193, 4.88, 4.48, 4.285714285714286, 3.9473684210526314, 4.260869565217392, 4.6, 4.548387096774194, 4.541666666666667, 4.45, 4.64, 4.7727272727272725, 5.2727272727272725, 4.169811320754717, 4.520833333333333, 4.64, 4.826923076923077, 4.669172932330827, 5.157894736842105, 4.4, 4.777777777777778, 4.625, 4.75, 4.24, 4.714285714285714, 4.666666666666667, 4.5, 4.5, 4.85, 4.523809523809524, 4.28, 4.25, 4.607142857142857, 5.722222222222222, 4.5, 4.634920634920635, 4.387096774193548, 5.2, 4.75, 4.666666666666667, 4.37037037037037, 4.2727272727272725, 4.387096774193548, 4.6, 4.6, 4.346153846153846, 4.434782608695652, 5.130434782608695, 5.75, 4.125, 5.363636363636363, 4.9, 5.216216216216216, 4.346153846153846, 4.384615384615385, 4.190476190476191, 4.916666666666667, 4.578947368421052, 4.685714285714286, 5.0, 5.0, 4.230769230769231, 4.7, 4.416666666666667, 4.515151515151516, 4.7, 4.2444444444444445, 4.509803921568627, 4.75, 4.758620689655173, 4.65625, 4.465753424657534, 4.785714285714286, 4.791666666666667, 4.285714285714286, 4.068376068376068, 4.464285714285714, 4.785714285714286, 4.291666666666667, 4.4375, 4.777777777777778, 4.714285714285714, 4.558139534883721, 4.666666666666667, 4.615384615384615, 3.857142857142857, 4.578947368421052, 4.5227272727272725, 4.583333333333333, 4.578947368421052, 4.7727272727272725, 4.375, 4.642857142857143, 4.658536585365853, 4.554216867469879, 4.535714285714286, 4.1875, 4.408450704225352, 4.428571428571429, 4.394736842105263, 4.5, 5.0, 4.390977443609023, 4.5, 4.363636363636363, 4.728813559322034, 4.7368421052631575, 4.866666666666666, 5.466666666666667, 4.113636363636363, 4.619047619047619, 4.3076923076923075, 4.53125, 4.7272727272727275, 4.611111111111111, 4.333333333333333, 4.590909090909091, 4.866666666666666, 4.296875, 4.392344497607655, 4.36, 4.8076923076923075, 4.818181818181818, 4.662337662337662, 4.081632653061225, 4.260869565217392, 4.454545454545454, 4.6, 4.3, 6.142857142857143, 5.103448275862069, 4.037037037037037, 4.958333333333333, 5.04, 5.0, 4.6, 4.833333333333333, 3.6666666666666665, 4.472222222222222, 4.666666666666667, 4.608695652173913, 4.565217391304348, 4.571428571428571, 4.258064516129032, 4.333333333333333, 4.157894736842105, 4.571428571428571, 4.52, 4.686746987951807, 4.357142857142857, 4.5777777777777775, 4.368421052631579, 4.517241379310345, 4.478260869565218, 4.444444444444445, 4.6521739130434785, 4.9, 4.333333333333333, 4.454545454545454, 4.466666666666667, 4.2272727272727275, 4.166666666666667, 4.319148936170213, 4.842105263157895, 4.703703703703703, 5.068965517241379, 4.3478260869565215, 5.024390243902439, 4.5, 4.8431372549019605, 4.105263157894737, 4.481481481481482, 4.695652173913044, 4.9523809523809526, 4.375, 4.434782608695652, 4.388888888888889, 4.777777777777778, 4.3076923076923075, 4.521739130434782, 4.888888888888889, 4.6, 4.098360655737705, 4.555555555555555, 4.4, 4.909090909090909, 4.742857142857143, 4.086956521739131, 4.583333333333333, 4.095238095238095, 4.65, 4.51063829787234, 5.333333333333333, 4.615384615384615, 4.538461538461538, 3.933333333333333, 4.395348837209302, 4.331753554502369, 4.182926829268292, 4.9, 4.730769230769231, 4.6521739130434785, 5.0, 4.2, 5.142857142857143, 4.65, 4.458333333333333, 4.5054945054945055, 4.166666666666667, 4.797979797979798, 4.265625, 5.055555555555555, 5.222222222222222, 4.470588235294118, 4.666666666666667, 4.310344827586207, 4.942857142857143, 3.9705882352941178, 4.735294117647059, 4.721153846153846, 4.190476190476191, 4.225806451612903, 4.03125, 4.6875, 4.85, 4.135135135135135, 4.421052631578948, 4.411764705882353, 4.4655172413793105, 4.642857142857143, 4.111111111111111, 4.958333333333333, 4.361111111111111, 4.6923076923076925, 4.595238095238095, 4.608695652173913, 4.439024390243903, 5.111111111111111, 4.570469798657718, 4.620689655172414, 4.773584905660377, 4.545454545454546, 4.714285714285714, 4.571428571428571, 4.416666666666667, 5.037037037037037, 4.411764705882353, 4.536585365853658, 4.702702702702703, 4.625, 4.4, 4.277777777777778, 4.676470588235294, 4.479166666666667, 4.857142857142857, 4.894736842105263, 4.133333333333334, 4.518518518518518, 4.833333333333333, 4.3125, 5.363636363636363, 4.767123287671233, 4.769230769230769, 4.705882352941177, 4.571428571428571, 4.142857142857143, 4.892857142857143, 4.947368421052632, 4.285714285714286, 5.083333333333333, 5.172413793103448, 4.823529411764706, 4.134615384615385, 4.730769230769231, 5.0, 4.581967213114754, 4.608695652173913, 4.608695652173913, 4.485714285714286, 5.166666666666667, 4.71875, 4.333333333333333, 4.466666666666667, 4.507936507936508, 5.0, 4.24793388429752, 4.542857142857143, 5.666666666666667, 4.625850340136054, 5.0, 4.6, 4.35, 4.684210526315789, 3.9523809523809526, 4.909090909090909, 4.357142857142857, 4.823529411764706, 4.6, 4.833333333333333, 4.825, 4.5, 4.428571428571429, 4.588235294117647, 4.155555555555556, 4.666666666666667, 4.552631578947368, 4.7272727272727275, 5.0, 4.615384615384615, 4.520833333333333, 4.782608695652174, 4.875, 4.769230769230769, 4.66, 5.25, 4.774193548387097, 4.75, 4.513513513513513, 4.867924528301887, 4.696969696969697, 4.53125, 5.076923076923077, 4.111111111111111, 4.164179104477612, 4.684210526315789, 5.094594594594595, 4.944444444444445, 4.243243243243243, 4.866666666666666, 4.136363636363637, 4.923076923076923, 4.333333333333333, 4.327272727272727, 4.9655172413793105, 4.702702702702703, 4.360655737704918, 4.638297872340425, 5.166666666666667, 4.5, 4.294117647058823, 4.5625, 4.787878787878788, 4.416666666666667, 4.684210526315789, 4.423076923076923, 4.542857142857143, 4.32, 4.9375, 3.9615384615384617, 4.88, 4.615384615384615, 4.3, 5.117647058823529, 4.5, 4.517241379310345, 4.535714285714286, 4.076923076923077, 4.863636363636363, 4.928571428571429, 5.060869565217391, 4.846153846153846, 4.555555555555555, 4.571428571428571, 4.357142857142857, 4.230769230769231, 5.565217391304348, 4.823529411764706, 4.512195121951219, 4.176470588235294, 4.7272727272727275, 4.619047619047619, 5.0, 4.368421052631579, 4.4375, 4.228070175438597, 4.375, 4.2105263157894735, 4.909090909090909, 4.64, 4.461538461538462, 5.076923076923077, 4.054054054054054, 4.25, 4.636363636363637, 4.625, 5.3125, 4.25, 4.875, 4.791666666666667, 4.53125, 4.538461538461538, 4.333333333333333, 4.793103448275862, 4.430769230769231, 4.454545454545454, 4.471698113207547, 4.888888888888889, 4.531645569620253, 4.680851063829787, 4.571428571428571, 4.4411764705882355, 4.478260869565218, 4.136363636363637, 4.595238095238095, 4.328125, 4.933333333333334, 4.6992481203007515, 4.384615384615385, 4.6, 4.045454545454546, 4.92, 4.681818181818182, 4.481481481481482, 4.647058823529412, 5.666666666666667, 4.131147540983607, 5.0, 4.133333333333334, 4.7, 5.266666666666667, 4.545454545454546, 4.423728813559322, 4.136363636363637, 4.904761904761905, 4.82051282051282, 4.638888888888889, 4.7272727272727275, 4.524590163934426, 4.75, 5.208333333333333, 4.875, 4.473684210526316, 5.055555555555555, 4.464285714285714, 4.2105263157894735, 4.545454545454546, 4.380952380952381, 4.708333333333333, 5.0, 4.388888888888889, 4.526315789473684, 4.2105263157894735, 4.642857142857143, 4.157894736842105, 4.7, 5.5, 4.589743589743589, 4.476190476190476, 4.52, 3.975, 4.111111111111111, 4.666666666666667, 4.194444444444445, 4.513513513513513, 4.55, 4.769230769230769, 4.466666666666667, 4.682926829268292, 4.4, 4.604651162790698, 4.378378378378378, 4.813953488372093, 4.653846153846154, 5.043478260869565, 4.523809523809524, 4.538461538461538, 4.909090909090909, 4.361111111111111, 5.235294117647059, 4.076923076923077, 4.591836734693878, 4.315789473684211, 4.565217391304348, 4.575757575757576, 4.931506849315069, 4.705882352941177, 4.923076923076923, 4.8, 4.448275862068965, 4.358208955223881, 4.555555555555555, 4.411764705882353, 4.404040404040404, 5.363636363636363, 4.913043478260869, 4.052631578947368, 4.134020618556701, 4.617021276595745, 4.833333333333333, 4.217391304347826, 4.48, 4.65, 4.111111111111111, 4.528301886792453, 3.8333333333333335, 4.391304347826087, 4.27536231884058, 4.615384615384615, 4.504854368932039, 4.666666666666667, 4.620689655172414, 4.409090909090909, 4.733333333333333, 4.275862068965517, 4.666666666666667, 4.583333333333333, 4.666666666666667, 4.894736842105263, 4.392857142857143, 4.571428571428571, 4.642857142857143, 4.689655172413793, 4.217391304347826, 4.894736842105263, 4.708333333333333, 4.75, 4.409090909090909, 4.64, 4.535714285714286, 4.25, 4.821428571428571, 4.8, 5.107142857142857, 4.2, 4.6, 4.592592592592593, 4.46875, 4.8, 4.583333333333333, 4.709677419354839, 4.4, 4.6923076923076925, 4.555555555555555, 5.0, 4.742857142857143, 4.6875, 4.40625, 4.545454545454546, 4.583333333333333, 4.555555555555555, 4.904761904761905, 4.535714285714286, 4.4, 4.65, 5.0, 4.823529411764706, 4.1923076923076925, 5.166666666666667, 4.605263157894737, 4.466666666666667, 4.741935483870968, 4.319148936170213, 4.4, 4.605633802816901, 4.24, 4.28125, 5.285714285714286, 4.709677419354839, 4.6875, 4.267857142857143, 4.391304347826087, 5.071428571428571, 4.714285714285714, 4.696428571428571, 4.541666666666667, 4.625, 4.75, 4.56, 4.2444444444444445, 4.7727272727272725, 4.714285714285714, 4.416666666666667, 5.25, 4.470588235294118, 5.461538461538462, 4.666666666666667, 4.666666666666667, 4.529411764705882, 4.37037037037037, 5.0, 4.625, 5.083333333333333, 5.153846153846154, 5.166666666666667, 4.875, 4.923076923076923, 4.758620689655173, 5.136363636363637, 5.375, 4.702127659574468, 4.444444444444445, 4.779816513761468, 5.0, 4.2105263157894735, 5.0, 4.655172413793103, 4.173913043478261, 4.7894736842105265, 4.821428571428571, 4.666666666666667, 4.666666666666667, 4.235294117647059, 4.234375, 4.8, 4.931034482758621, 4.30625, 5.076923076923077, 4.653846153846154, 5.0, 4.7407407407407405, 4.5, 4.642857142857143, 4.894736842105263, 4.725, 4.521739130434782, 4.357142857142857, 4.2745098039215685, 4.517241379310345, 4.587301587301587, 4.416666666666667, 5.225, 4.571428571428571, 4.65, 4.545454545454546, 4.777777777777778, 4.8125, 4.625, 4.45, 5.071428571428571, 4.9, 4.291666666666667, 4.653061224489796, 4.36, 4.696969696969697, 5.409090909090909, 4.714285714285714, 4.636363636363637, 4.411764705882353, 5.190476190476191, 4.605962933118453, 4.181818181818182, 4.588235294117647, 4.6, 5.064516129032258, 4.352941176470588, 4.45, 4.615384615384615, 4.625, 4.542857142857143, 4.7727272727272725, 4.62, 4.780487804878049, 4.454545454545454, 5.222222222222222, 4.584229390681004, 4.95, 4.170212765957447, 4.892857142857143, 4.333333333333333, 4.5625, 4.625, 4.387096774193548, 4.384615384615385, 4.5, 4.5, 4.681818181818182, 4.4, 4.411764705882353, 4.65, 4.2, 4.782608695652174, 4.7, 4.884615384615385, 4.65625, 4.541666666666667, 4.357142857142857, 4.30379746835443, 4.823529411764706, 5.2, 4.705882352941177, 4.428571428571429, 4.708333333333333, 4.8076923076923075, 4.565217391304348, 4.352941176470588, 4.6938775510204085, 4.521739130434782, 4.333333333333333, 4.333333333333333, 4.723404255319149, 4.806451612903226, 4.571428571428571, 5.090909090909091, 4.793103448275862, 4.85, 4.476190476190476, 4.181818181818182, 4.571428571428571, 4.666666666666667, 4.882352941176471, 4.482758620689655, 5.083333333333333, 4.377777777777778, 5.4, 4.5227272727272725, 4.5, 4.35, 5.1, 5.090909090909091, 5.066666666666666, 4.6, 4.3125, 4.769230769230769, 4.582089552238806, 4.166666666666667, 4.473684210526316, 4.473684210526316, 4.396551724137931, 4.7727272727272725, 4.708333333333333, 4.333333333333333, 4.333333333333333, 4.651685393258427, 4.142857142857143, 4.125, 4.407407407407407, 4.578947368421052, 4.925925925925926, 4.6521739130434785, 4.533333333333333, 5.090909090909091, 4.136363636363637, 4.380952380952381, 4.634615384615385, 4.777777777777778, 4.642857142857143, 4.388888888888889, 4.4, 4.285714285714286, 5.0, 4.5, 4.6875, 4.417910447761194, 5.0, 4.96969696969697, 4.333333333333333, 4.590909090909091, 4.166666666666667, 4.125, 5.117647058823529, 4.260869565217392, 4.379310344827586, 4.884615384615385, 4.5, 4.305555555555555, 4.473684210526316, 4.7727272727272725, 5.086956521739131, 4.85, 5.0576923076923075, 4.543478260869565, 4.583333333333333, 5.0, 4.631578947368421, 4.769230769230769, 4.465116279069767, 5.233333333333333, 4.666666666666667, 4.193548387096774, 4.5, 3.9523809523809526, 4.6909090909090905, 4.809523809523809, 4.166666666666667, 4.833333333333333, 4.833333333333333, 4.75, 4.866666666666666, 4.65, 4.214285714285714, 4.526315789473684, 4.411764705882353, 4.770833333333333, 4.36, 4.809523809523809, 4.724137931034483, 4.083333333333333, 4.4, 4.206896551724138, 4.631578947368421, 4.6, 4.333333333333333, 4.636363636363637, 4.397435897435898, 4.473684210526316, 3.85, 4.7727272727272725, 4.884615384615385, 5.0588235294117645, 4.083333333333333, 4.588235294117647, 4.785714285714286, 4.607142857142857, 4.909090909090909, 5.1875, 4.454545454545454, 4.620689655172414, 4.818181818181818, 4.363636363636363, 4.357142857142857, 3.9615384615384617, 4.5, 4.947368421052632, 4.416666666666667, 4.87378640776699, 4.769230769230769, 4.933333333333334, 5.222222222222222, 4.5625, 4.8076923076923075, 4.791044776119403, 4.735294117647059, 4.857142857142857, 4.523809523809524, 4.962962962962963, 4.642857142857143, 4.757575757575758, 4.636363636363637, 4.764705882352941, 4.413793103448276, 4.463829787234043, 5.217391304347826, 4.166666666666667, 4.5, 4.0, 5.275862068965517, 4.5, 4.5, 4.428571428571429, 4.3076923076923075, 4.324675324675325, 4.421052631578948, 4.3076923076923075, 5.153846153846154, 5.292307692307692, 4.4, 4.894736842105263, 4.090909090909091, 5.125, 4.54054054054054, 4.714285714285714, 4.854166666666667, 4.346153846153846, 4.909090909090909, 4.657894736842105, 5.214285714285714, 4.425, 4.853658536585366, 5.0, 4.666666666666667, 4.461538461538462, 4.125, 4.153846153846154, 5.444444444444445, 4.391304347826087, 4.857142857142857, 5.0, 4.818181818181818, 4.913043478260869, 5.0, 4.166666666666667, 4.823529411764706, 4.2, 4.375, 4.9, 5.25, 4.424242424242424, 4.578947368421052, 4.686274509803922, 4.578947368421052, 4.28125, 5.045454545454546, 4.421052631578948, 4.7894736842105265, 4.5, 4.388888888888889, 4.636363636363637, 4.423076923076923, 4.514285714285714, 5.285714285714286, 4.666666666666667, 4.8, 4.545454545454546, 4.642857142857143, 5.071428571428571, 5.076923076923077, 4.634146341463414, 4.619047619047619, 4.391304347826087, 5.175, 4.051724137931035, 4.75, 4.333333333333333, 4.684210526315789, 4.811594202898551, 4.266666666666667, 4.642857142857143, 4.294117647058823, 4.6521739130434785, 4.3478260869565215, 4.153846153846154, 4.846153846153846, 4.333333333333333, 4.763888888888889, 4.8, 4.708333333333333, 4.1875, 4.592592592592593, 4.897435897435898, 4.771428571428571, 5.111111111111111, 4.7317073170731705, 4.541666666666667, 4.769230769230769, 4.5, 4.688888888888889, 5.5, 4.623853211009174, 5.0, 4.8, 4.758620689655173, 4.571428571428571, 5.0, 4.666666666666667, 4.655172413793103, 4.866666666666666, 4.8, 4.244897959183674, 5.083333333333333, 4.3076923076923075, 4.642857142857143, 4.548387096774194, 5.333333333333333, 4.846153846153846, 4.590909090909091, 4.916666666666667, 4.2444444444444445, 4.294117647058823, 4.217391304347826, 4.473684210526316, 4.6, 4.7727272727272725, 4.8076923076923075, 4.676470588235294, 4.592592592592593, 4.222222222222222, 5.08, 5.285714285714286, 4.611111111111111, 4.647058823529412, 5.131578947368421, 4.5, 4.5, 3.888888888888889, 5.125, 4.153846153846154, 4.842105263157895, 4.5636363636363635, 4.818181818181818, 4.966101694915254, 5.071428571428571, 5.235294117647059, 4.384615384615385, 4.073170731707317, 4.6231884057971016, 4.882352941176471, 4.6, 5.066666666666666, 4.648351648351649, 4.545454545454546, 4.388888888888889, 5.862068965517241, 4.702702702702703, 4.5625, 4.777777777777778, 4.40625, 4.413793103448276, 5.295238095238095, 5.1923076923076925, 4.857142857142857, 5.625, 4.7894736842105265, 4.3125, 4.6, 4.7894736842105265, 4.068181818181818, 4.8, 4.923076923076923, 4.066666666666666, 4.56, 4.153846153846154, 4.7272727272727275, 4.611111111111111, 4.423076923076923, 4.404761904761905, 4.8, 5.0, 4.9411764705882355, 4.296296296296297, 4.5, 4.842105263157895, 5.285714285714286, 4.183206106870229, 4.842105263157895, 4.666666666666667, 4.333333333333333, 4.666666666666667, 4.15625, 4.786885245901639, 4.56, 4.633333333333334, 4.3076923076923075, 4.90625, 4.666666666666667, 4.653846153846154, 4.5, 4.333333333333333, 4.125, 5.055555555555555, 4.405405405405405, 4.571428571428571, 4.324324324324325, 4.409090909090909, 4.193548387096774, 4.5, 4.914285714285715, 5.125, 4.842105263157895, 4.125, 4.0, 4.75, 4.809523809523809, 4.9523809523809526, 4.571428571428571, 4.6, 4.444444444444445, 4.741935483870968, 4.764705882352941, 4.741935483870968, 5.0576923076923075, 3.9523809523809526, 4.655172413793103, 4.875, 4.681818181818182, 4.5, 4.390625, 3.923076923076923, 4.636363636363637, 4.618181818181818, 4.628571428571429, 4.818181818181818, 4.8076923076923075, 5.090909090909091, 4.783783783783784, 4.4, 4.521739130434782, 4.294117647058823, 4.85, 5.625, 4.3, 4.142857142857143, 4.105263157894737, 4.296875, 5.064516129032258, 4.041666666666667, 4.324324324324325, 4.388888888888889, 4.176470588235294, 4.541666666666667, 4.84, 4.5, 3.980769230769231, 4.761904761904762, 4.4, 5.324324324324325, 4.45945945945946, 4.75, 4.5, 5.0, 4.6, 4.023255813953488, 4.11864406779661, 4.361111111111111, 4.372549019607843, 4.142857142857143, 5.083333333333333, 4.352941176470588, 4.368421052631579, 4.6521739130434785, 4.888888888888889, 4.428571428571429, 3.9166666666666665, 5.214285714285714, 4.346153846153846, 4.28421052631579, 4.666666666666667, 4.333333333333333, 4.85, 5.019607843137255, 4.5, 4.473684210526316, 4.411764705882353, 4.703703703703703, 4.6, 5.4, 5.142857142857143, 4.64, 4.7272727272727275, 4.545454545454546, 4.75, 4.542857142857143, 4.428571428571429, 4.580645161290323, 4.36, 5.166666666666667, 4.153846153846154, 4.575, 3.9375, 4.818181818181818, 4.1, 5.069444444444445, 4.604166666666667, 4.384615384615385, 5.928571428571429, 5.230769230769231, 4.125, 4.291666666666667, 4.266666666666667, 4.478260869565218, 4.647058823529412, 3.918918918918919, 4.277777777777778, 4.6, 4.214285714285714, 5.0, 4.618421052631579, 5.5, 5.0, 4.0, 4.333333333333333, 4.888888888888889, 3.857142857142857, 3.9545454545454546, 4.728, 4.5, 4.641025641025641, 4.333333333333333, 5.125, 3.730769230769231, 5.125, 4.863636363636363, 4.416666666666667, 4.0, 4.142857142857143, 4.285714285714286, 4.888888888888889, 4.8, 4.313432835820896, 4.4, 4.84, 4.648648648648648, 4.592592592592593, 4.4, 5.015625, 4.694444444444445, 4.851851851851852, 4.724137931034483, 4.95, 4.2105263157894735, 5.0, 4.833333333333333, 4.5636363636363635, 4.6, 4.88, 4.487179487179487, 4.56, 4.133333333333334, 4.475, 4.476190476190476, 4.808219178082192, 4.85, 5.076923076923077, 4.645161290322581, 4.193548387096774, 4.287330316742081, 4.441558441558442, 4.777777777777778, 4.267441860465116, 4.586956521739131, 4.95, 4.846153846153846, 3.9375, 4.304347826086956, 4.25, 4.64, 5.0, 4.473684210526316, 5.045454545454546, 4.6, 4.857142857142857, 4.653846153846154, 4.666666666666667, 4.428571428571429, 4.642857142857143, 4.7407407407407405, 4.458333333333333, 4.653465346534653, 4.35, 4.4, 4.3700787401574805, 5.1, 5.023529411764706, 4.636363636363637, 4.708333333333333, 4.3, 4.535714285714286, 5.097560975609756, 4.384615384615385, 4.5, 4.2, 4.526315789473684, 4.7, 4.384615384615385, 5.222222222222222, 4.344827586206897, 5.146341463414634, 4.704918032786885, 4.583333333333333, 4.476190476190476, 4.5, 4.391304347826087, 4.5, 4.786407766990291, 4.6, 4.285714285714286, 4.375, 5.0588235294117645, 4.277777777777778, 5.222222222222222, 4.5, 4.769230769230769, 4.523809523809524, 4.795454545454546, 4.28, 4.326530612244898, 4.217391304347826, 4.701492537313433, 4.739130434782608, 4.538461538461538, 4.65, 4.352941176470588, 4.894736842105263, 4.6, 4.285714285714286, 4.6923076923076925, 4.090909090909091, 4.217391304347826, 4.428571428571429, 4.838709677419355, 4.388888888888889, 4.703703703703703, 3.857142857142857, 4.813953488372093, 4.36, 4.583333333333333, 4.7727272727272725, 4.787878787878788, 4.961538461538462, 4.385542168674699, 4.926470588235294, 4.867924528301887, 4.6, 4.9, 4.6521739130434785, 4.5, 4.388888888888889, 5.0, 4.625, 4.32258064516129, 4.390243902439025, 4.257142857142857, 4.2592592592592595, 4.296296296296297, 4.529411764705882, 4.291666666666667, 4.7, 4.576923076923077, 4.55, 4.5, 4.162162162162162, 4.285714285714286, 4.65, 4.75, 4.48, 5.230769230769231, 4.571428571428571, 4.583333333333333, 4.722222222222222, 4.7368421052631575, 4.647058823529412, 4.642857142857143, 4.333333333333333, 4.9921259842519685, 5.079545454545454, 4.7727272727272725, 4.416666666666667, 4.581881533101045, 4.242857142857143, 4.4375, 4.409836065573771, 4.411764705882353, 4.724137931034483, 4.4375, 4.761904761904762, 4.666666666666667, 5.142857142857143, 4.6, 4.870967741935484, 4.809523809523809, 4.633333333333334, 4.7368421052631575, 4.481481481481482, 4.661538461538462, 4.909090909090909, 4.533333333333333, 4.111111111111111, 4.875, 4.472222222222222, 4.25, 4.830508474576271, 4.576923076923077, 4.361111111111111, 4.416666666666667, 4.555555555555555, 4.666666666666667, 4.59375, 4.21875, 4.523809523809524, 3.9523809523809526, 4.424657534246576, 4.5, 4.916666666666667, 4.373737373737374, 4.2, 5.15, 4.75, 4.785714285714286, 4.555555555555555, 4.785714285714286, 4.085106382978723, 4.5, 4.5625, 4.916666666666667, 4.7368421052631575, 4.538461538461538, 4.36734693877551, 4.878787878787879, 4.933333333333334, 4.055555555555555, 5.0, 4.4, 4.604651162790698, 4.611111111111111, 4.111111111111111, 4.3, 4.363636363636363, 4.612676056338028, 4.458333333333333, 5.538461538461538, 4.945652173913044, 4.724137931034483, 4.043478260869565, 4.526315789473684, 5.611111111111111, 4.777777777777778, 4.875, 4.111111111111111, 3.8181818181818183, 4.5, 4.5, 4.8, 4.520833333333333, 4.45, 4.44, 4.777777777777778, 4.391304347826087, 3.8, 4.87037037037037, 4.842105263157895, 4.434782608695652, 4.3125, 4.222222222222222, 4.461538461538462, 5.125, 4.769230769230769, 4.769230769230769, 4.444444444444445, 4.731343283582089, 4.7272727272727275, 5.076923076923077, 5.0, 4.484848484848484, 4.388888888888889, 4.436507936507937, 5.1, 4.5, 4.5, 4.9743589743589745, 4.2105263157894735, 4.619047619047619, 4.523809523809524, 4.62962962962963, 4.987179487179487, 5.1, 3.64, 4.451612903225806, 4.615384615384615, 4.679245283018868, 4.605263157894737, 4.818181818181818, 4.75796178343949, 4.703703703703703, 4.604651162790698, 4.473684210526316, 4.738095238095238, 4.590909090909091, 4.476190476190476, 4.288135593220339, 4.836734693877551, 4.428571428571429, 4.5, 4.380952380952381, 4.086956521739131, 4.161290322580645, 4.545454545454546, 4.416666666666667, 4.619047619047619, 4.357142857142857, 4.545454545454546, 4.333333333333333, 4.491228070175438, 4.571428571428571, 4.571428571428571, 4.285714285714286, 4.434782608695652, 4.857142857142857, 4.769230769230769, 4.556962025316456, 5.0, 4.974683544303797, 4.684210526315789, 4.25, 4.576923076923077, 4.72, 4.846153846153846, 4.8125, 4.724137931034483, 4.703703703703703, 4.24, 4.647058823529412, 5.416666666666667, 4.3125, 4.473684210526316, 4.863636363636363, 4.176470588235294, 4.594594594594595, 4.117647058823529, 4.6521739130434785, 4.409090909090909, 4.576923076923077, 4.4, 4.846153846153846, 4.157894736842105, 4.4, 4.8, 4.25, 4.634920634920635, 5.0, 4.2272727272727275, 4.3, 4.857142857142857, 4.62962962962963, 4.515151515151516, 4.791666666666667, 4.454545454545454, 4.266666666666667, 4.73, 4.933333333333334, 4.153846153846154, 5.208333333333333, 4.786666666666667, 4.909090909090909, 4.45, 4.458333333333333, 4.257142857142857, 4.619047619047619, 5.5, 5.142857142857143, 5.125, 4.9393939393939394, 4.62962962962963, 4.711538461538462, 5.5, 4.84, 4.416666666666667, 5.16, 4.723404255319149, 4.857142857142857, 4.476190476190476, 4.3, 4.466666666666667, 4.76, 4.375, 3.9696969696969697, 4.8, 4.791666666666667, 4.4, 4.46875, 4.815789473684211, 4.166666666666667, 4.761904761904762, 4.625, 5.636363636363637, 4.3, 4.2, 4.222222222222222, 4.541666666666667, 4.764705882352941, 4.142857142857143, 4.454545454545454, 4.676470588235294, 5.6, 4.352941176470588, 4.916666666666667, 4.454545454545454, 4.404255319148936, 5.0, 4.514285714285714, 4.729166666666667, 5.076923076923077, 4.4, 4.6, 4.480392156862745, 3.8823529411764706, 4.793103448275862, 3.948051948051948, 4.933333333333334, 4.323529411764706, 4.5625, 4.166666666666667, 4.666666666666667, 4.555555555555555, 4.433333333333334, 4.760869565217392, 4.62962962962963, 4.383597883597884, 4.782608695652174, 5.230769230769231, 4.071428571428571, 4.722222222222222, 4.2727272727272725, 4.59375, 4.428571428571429, 4.7272727272727275, 4.888888888888889, 4.583333333333333, 4.834782608695652, 5.142857142857143, 5.2, 4.303030303030303, 4.4375, 4.428571428571429, 4.821428571428571, 5.0625, 4.228571428571429, 4.6, 4.633333333333334, 4.333333333333333, 4.45, 4.565217391304348, 5.055555555555555, 4.538461538461538, 4.117647058823529, 4.777777777777778, 4.285714285714286, 4.833333333333333, 4.461538461538462, 4.7894736842105265, 4.666666666666667, 4.65, 4.368421052631579, 4.589743589743589, 4.961538461538462, 4.457142857142857, 4.416666666666667, 4.2272727272727275, 4.833333333333333, 4.647058823529412, 4.7272727272727275, 4.964285714285714, 4.785714285714286, 4.5625, 4.228571428571429, 4.956521739130435, 4.285714285714286, 5.021739130434782, 4.923076923076923, 4.066666666666666, 4.4, 4.1, 4.538461538461538, 4.394736842105263, 4.375, 4.619047619047619, 5.0, 3.8636363636363638, 4.833333333333333, 4.379310344827586, 4.684210526315789, 4.911764705882353, 3.6153846153846154, 5.142857142857143, 5.538461538461538, 4.5777777777777775, 4.842105263157895, 4.529411764705882, 4.458333333333333, 4.628571428571429, 4.857142857142857, 4.619047619047619, 4.346153846153846, 5.318181818181818, 4.733333333333333, 4.986486486486487, 4.130434782608695, 4.2727272727272725, 4.8, 4.434782608695652, 4.6716417910447765, 4.366666666666666, 4.5476190476190474, 4.795918367346939, 5.0, 4.851851851851852, 4.857142857142857, 5.111111111111111, 4.555555555555555, 4.972222222222222, 4.552631578947368, 5.0, 5.36, 3.977777777777778, 5.388888888888889, 4.903225806451613, 4.6, 4.368421052631579, 4.6875, 4.75, 4.791666666666667, 5.0344827586206895, 4.5, 5.625, 4.533333333333333, 4.68, 4.379310344827586, 4.607142857142857, 4.095238095238095, 4.875, 4.9, 4.270833333333333, 4.730769230769231, 4.523809523809524, 4.55, 4.588235294117647, 4.368421052631579, 4.88135593220339, 4.846153846153846, 4.9411764705882355, 4.730769230769231, 4.576923076923077, 4.739130434782608, 4.5, 4.333333333333333, 4.65, 4.466666666666667, 4.930232558139535, 4.348837209302325, 4.25, 4.551724137931035, 4.863636363636363, 4.642857142857143, 4.266666666666667, 4.201754385964913, 4.48936170212766, 4.216216216216216, 4.636363636363637, 3.9642857142857144, 4.551724137931035, 5.571428571428571, 4.344827586206897, 4.7727272727272725, 4.675, 4.05, 4.7272727272727275, 4.608695652173913, 4.571428571428571, 4.333333333333333, 4.954022988505747, 4.761904761904762, 4.204545454545454, 5.3125, 4.546052631578948, 4.4, 4.346153846153846, 4.666666666666667, 4.5, 5.038461538461538, 4.681818181818182, 4.818181818181818, 4.107692307692307, 5.055555555555555, 4.575342465753424, 5.125, 4.384615384615385, 4.739130434782608, 4.3023255813953485, 4.9, 4.083333333333333, 4.473684210526316, 4.7272727272727275, 4.785714285714286, 4.611111111111111, 4.533333333333333, 4.625, 3.769230769230769, 5.172413793103448, 4.806451612903226, 4.884615384615385, 4.0, 5.666666666666667, 4.354166666666667, 5.03125, 6.333333333333333, 4.571428571428571, 4.5, 4.28, 4.153846153846154, 4.416666666666667, 4.173913043478261, 4.619047619047619, 4.823529411764706, 4.615384615384615, 4.148936170212766, 4.785714285714286, 4.641025641025641, 4.76, 4.521739130434782, 5.260869565217392, 4.5, 4.617021276595745, 4.631578947368421, 5.083333333333333, 4.821428571428571, 4.444444444444445, 4.7, 4.745454545454545, 4.65625, 4.758620689655173, 4.739130434782608, 4.9523809523809526, 4.857142857142857, 4.55, 4.888888888888889, 4.055555555555555, 5.375, 4.8125, 3.909090909090909, 5.125, 4.391304347826087, 5.117647058823529, 4.133333333333334, 4.416666666666667, 4.52, 4.838709677419355, 4.2631578947368425, 4.352941176470588, 4.4, 5.2105263157894735, 4.470588235294118, 4.88, 4.714285714285714, 4.625, 4.2272727272727275, 4.25, 5.190476190476191, 4.818181818181818, 4.75, 4.0, 5.214285714285714, 4.764705882352941, 4.833333333333333, 4.533333333333333, 4.25, 4.88, 4.357142857142857, 4.857142857142857, 5.0, 4.833333333333333, 4.857142857142857, 4.1, 4.735849056603773, 4.176470588235294, 5.148148148148148, 4.285714285714286, 4.355263157894737, 4.230769230769231, 4.607142857142857, 4.681818181818182, 4.615384615384615, 4.7, 5.055555555555555, 5.2075471698113205, 4.395833333333333, 4.615384615384615, 4.409090909090909, 4.3478260869565215, 4.384615384615385, 5.173913043478261, 4.56, 4.5625, 4.666666666666667, 4.916666666666667, 5.4, 4.933333333333334, 4.894736842105263, 4.419354838709677, 4.666666666666667, 4.176470588235294, 4.583333333333333, 4.311594202898551, 4.777777777777778, 4.661290322580645, 4.963636363636364, 4.636363636363637, 4.902597402597403, 4.285714285714286, 4.833333333333333, 4.909090909090909, 4.682926829268292, 4.254545454545455, 4.2631578947368425, 4.333333333333333, 4.5, 4.373134328358209, 4.266666666666667, 4.977777777777778, 4.526315789473684, 4.75, 4.739130434782608, 4.583333333333333, 4.382352941176471, 4.892857142857143, 4.780487804878049, 4.4, 4.6521739130434785, 4.473684210526316, 5.142857142857143, 5.088888888888889, 4.555555555555555, 4.365384615384615, 4.48, 4.3076923076923075, 4.5675675675675675, 4.470588235294118, 4.714285714285714, 4.666666666666667, 4.217391304347826, 4.25, 4.666666666666667, 4.884615384615385, 4.447368421052632, 4.337662337662338, 4.7, 4.82051282051282, 4.75, 4.384615384615385, 4.574468085106383, 4.55, 4.153846153846154, 4.275862068965517, 4.208333333333333, 4.285714285714286, 4.213333333333333, 4.59375, 5.08, 4.76, 4.25, 4.734375, 4.368421052631579, 5.117647058823529, 4.541666666666667, 4.2105263157894735, 5.2105263157894735, 4.25, 4.25, 4.8125, 4.125, 4.64, 4.0, 4.925925925925926, 4.333333333333333, 4.391304347826087, 3.5, 4.380952380952381, 4.627272727272727, 4.9411764705882355, 4.345679012345679, 4.454545454545454, 4.384615384615385, 4.504854368932039, 4.607142857142857, 4.529411764705882, 4.9, 5.2592592592592595, 4.703703703703703, 4.681818181818182, 4.684210526315789, 4.357142857142857, 4.521739130434782, 4.857142857142857, 4.733333333333333, 4.205882352941177, 4.675675675675675, 5.583333333333333, 4.6923076923076925, 4.925925925925926, 4.3, 4.9411764705882355, 4.631578947368421, 4.391304347826087, 4.117647058823529, 4.318181818181818, 3.8181818181818183, 4.666666666666667, 4.75, 4.818181818181818, 4.474074074074074, 4.428571428571429, 4.428571428571429, 4.814285714285714, 4.347058823529411, 4.769230769230769, 4.352941176470588, 4.346153846153846, 4.269230769230769, 4.25, 4.454545454545454, 4.055555555555555, 4.266666666666667, 4.24, 4.28125, 4.7368421052631575, 4.809523809523809, 4.970588235294118, 5.75, 4.675675675675675, 4.2631578947368425, 4.9, 4.565217391304348, 4.8125, 5.3, 4.791666666666667, 5.5, 4.625, 4.730769230769231, 4.5, 4.269230769230769, 4.614583333333333, 4.583333333333333, 4.526315789473684, 4.7, 4.6, 4.8125, 4.916666666666667, 5.2407407407407405, 5.142857142857143, 5.75, 4.441860465116279, 4.529411764705882, 4.705882352941177, 5.1521739130434785, 4.777777777777778, 4.142857142857143, 4.8, 4.285714285714286, 4.5476190476190474, 4.851851851851852, 4.655172413793103, 3.9166666666666665, 4.446428571428571, 4.185185185185185, 4.285714285714286, 4.620689655172414, 4.785714285714286, 4.821428571428571, 5.375, 4.72, 4.454545454545454, 5.090909090909091, 4.666666666666667, 4.647058823529412, 4.533333333333333, 4.9, 4.561643835616438, 4.204081632653061, 4.565217391304348, 4.714285714285714, 4.693693693693693, 4.205882352941177, 4.304347826086956, 4.5, 4.7272727272727275, 4.190476190476191, 4.535714285714286, 4.678571428571429, 5.125, 4.361111111111111, 4.833333333333333, 4.636363636363637, 4.8, 4.298507462686567, 4.666666666666667, 4.607142857142857, 4.076923076923077, 5.277777777777778, 4.96, 4.466666666666667, 4.588235294117647, 4.297872340425532, 4.368421052631579, 4.586206896551724, 4.896551724137931, 4.2, 4.387755102040816, 3.5675675675675675, 4.606060606060606, 4.647058823529412, 4.2631578947368425, 4.636363636363637, 4.903061224489796, 4.327272727272727, 4.833333333333333, 3.7333333333333334, 4.3125, 4.769230769230769, 4.483333333333333, 5.105263157894737, 4.7, 4.84375, 4.645161290322581, 4.514285714285714, 5.25, 4.428571428571429, 4.833333333333333, 5.166666666666667, 4.346153846153846, 4.578947368421052, 4.466666666666667, 4.518518518518518, 4.2, 4.75, 4.892857142857143, 4.4, 4.333333333333333, 4.533333333333333, 4.046511627906977, 4.235294117647059, 4.51063829787234, 4.217391304347826, 4.745454545454545, 4.516129032258065, 4.35, 4.4, 5.071428571428571, 4.433962264150943, 4.947368421052632, 4.421052631578948, 5.0, 4.636363636363637, 4.0256410256410255, 4.627450980392157, 4.857142857142857, 4.173913043478261, 5.214285714285714, 4.8, 4.689655172413793, 4.4363636363636365, 4.541666666666667, 4.5813953488372094, 4.714285714285714, 5.08, 4.158415841584159, 4.4, 4.462365591397849, 4.632911392405063, 4.275, 4.517241379310345, 5.396825396825397, 4.583333333333333, 4.571428571428571, 4.407407407407407, 4.714285714285714, 4.456140350877193, 4.777777777777778, 4.413793103448276, 4.037037037037037, 4.545454545454546, 4.375, 3.9166666666666665, 4.404371584699454, 4.747663551401869, 4.96, 5.4, 4.40625, 4.688888888888889, 4.777777777777778, 3.861111111111111, 4.64, 5.3125, 4.0, 4.3076923076923075, 5.212121212121212, 4.478260869565218, 4.184615384615385, 4.888888888888889, 4.390243902439025, 4.238095238095238, 4.724137931034483, 4.428571428571429, 4.916666666666667, 4.55, 4.588235294117647, 4.428571428571429, 4.88, 4.6875, 3.9166666666666665, 4.4, 4.333333333333333, 4.583333333333333, 4.3125, 4.565217391304348, 4.217391304347826, 4.375, 4.15, 4.442528735632184, 4.529411764705882, 4.538461538461538, 4.712328767123288, 4.9411764705882355, 4.7, 4.15, 4.458333333333333, 4.599431818181818, 4.5625, 5.6, 4.428571428571429, 4.5625, 4.5, 4.3, 4.28, 4.433333333333334, 4.527777777777778, 5.2727272727272725, 4.326923076923077, 5.0, 4.825, 4.783783783783784, 4.666666666666667, 4.8125, 4.333333333333333, 4.68, 4.815384615384615, 4.444444444444445, 4.416666666666667, 4.416666666666667, 4.194444444444445, 4.680851063829787, 4.888888888888889, 4.411764705882353, 4.3, 4.7, 5.304347826086956, 4.666666666666667, 4.625, 4.962962962962963, 5.125, 4.714285714285714, 4.872340425531915, 4.277777777777778, 4.224137931034483, 4.521739130434782, 4.526315789473684, 4.285714285714286, 4.866666666666666, 4.296296296296297, 4.888888888888889, 4.645161290322581, 4.722222222222222, 4.578947368421052, 4.818181818181818, 4.685714285714286, 4.2926829268292686, 4.233333333333333, 4.366666666666666, 4.857142857142857, 4.857142857142857, 4.8, 4.8, 5.142857142857143, 4.6923076923076925, 5.0, 4.909090909090909, 4.8125, 4.333333333333333, 4.283018867924528, 4.466666666666667, 4.5, 4.4, 4.956521739130435, 5.0, 4.636363636363637, 4.709677419354839, 4.75, 4.421052631578948, 4.569620253164557, 4.461538461538462, 4.3478260869565215, 3.857142857142857, 4.484848484848484, 4.161616161616162, 4.782608695652174, 4.695652173913044, 4.722222222222222, 4.348837209302325, 4.846153846153846, 4.666666666666667, 4.241379310344827, 4.823529411764706, 4.5, 4.545454545454546, 4.741379310344827, 4.896551724137931, 4.933333333333334, 4.44, 4.24, 5.146341463414634, 4.3090909090909095, 4.473333333333334, 5.0, 5.266666666666667, 4.631578947368421, 4.5, 4.555555555555555, 4.655172413793103, 4.826086956521739, 4.277777777777778, 4.541666666666667, 5.181818181818182, 4.7894736842105265, 4.62962962962963, 4.4772727272727275, 4.68, 4.8125, 4.925925925925926, 5.0, 4.5, 4.631578947368421, 4.642857142857143, 5.0, 4.538461538461538, 4.909090909090909, 4.609375, 4.571428571428571, 4.454545454545454, 4.310344827586207, 3.8055555555555554, 4.725, 4.588235294117647, 4.408163265306122, 4.882352941176471, 5.54054054054054, 3.5, 4.55, 4.62962962962963, 4.0625, 4.466666666666667, 4.523809523809524, 4.035714285714286, 4.517006802721088, 4.3125, 4.651785714285714, 4.6571428571428575, 4.181818181818182, 4.857142857142857, 4.363636363636363, 4.6875, 4.5, 4.7272727272727275, 4.410256410256411, 4.954545454545454, 4.9, 4.870967741935484, 4.636363636363637, 5.333333333333333, 4.8933333333333335, 4.819444444444445, 4.7272727272727275, 4.416666666666667, 4.473684210526316, 4.341772151898734, 4.684210526315789, 4.411764705882353, 4.444444444444445, 4.555555555555555, 4.125, 5.015625, 5.0, 4.535714285714286, 4.384615384615385, 4.590909090909091, 5.5, 4.3545454545454545, 4.423076923076923, 4.4523809523809526, 4.571428571428571, 4.846153846153846, 4.214285714285714, 4.7, 4.722222222222222, 4.214285714285714, 4.444444444444445, 4.431372549019608, 4.217391304347826, 4.176470588235294, 4.685185185185185, 4.190476190476191, 4.809523809523809, 4.222222222222222, 4.682926829268292, 4.352941176470588, 4.961538461538462, 4.208333333333333, 4.878787878787879, 4.666666666666667, 4.549019607843137, 4.875, 4.5, 4.954545454545454, 4.88, 4.966666666666667, 4.7272727272727275, 4.166666666666667, 4.7, 4.7368421052631575, 4.230769230769231, 5.166666666666667, 4.809523809523809, 4.26530612244898, 4.4, 4.325581395348837, 4.894736842105263, 4.489795918367347, 5.046296296296297, 4.357142857142857, 4.0, 4.212121212121212, 4.157894736842105, 4.633333333333334, 4.904761904761905, 4.3, 4.8, 4.027777777777778, 5.0, 4.684210526315789, 4.666666666666667, 4.375, 4.407079646017699, 4.7727272727272725, 4.875, 4.588235294117647, 4.592592592592593, 5.25, 4.88, 4.461538461538462, 4.2727272727272725, 4.433333333333334, 4.870967741935484, 4.037037037037037, 4.407407407407407, 4.45, 4.153846153846154, 5.090909090909091, 4.838709677419355, 4.5, 4.533333333333333, 4.531914893617022, 4.2, 4.666666666666667, 4.533333333333333, 4.21875, 4.6231884057971016, 4.65, 4.476190476190476, 4.410256410256411, 4.733333333333333, 4.421052631578948, 5.153846153846154, 4.544554455445544, 4.476190476190476, 4.587301587301587, 4.565217391304348, 4.666666666666667, 4.814814814814815, 4.161290322580645, 4.275862068965517, 4.433333333333334, 4.153846153846154, 4.4423076923076925, 4.970588235294118, 4.208955223880597, 4.617021276595745, 4.153846153846154, 4.666666666666667, 4.3125, 4.545454545454546, 4.363636363636363, 4.75, 4.620253164556962, 4.96969696969697, 4.809523809523809, 4.6, 4.103448275862069, 4.953488372093023, 4.565217391304348, 5.083333333333333, 4.111111111111111, 4.538461538461538, 5.0, 4.3125, 5.084033613445378, 4.470588235294118, 4.55, 4.444444444444445, 5.0476190476190474, 5.033333333333333, 5.032258064516129, 4.3076923076923075, 4.555555555555555, 5.619047619047619, 4.65, 4.836538461538462, 4.555555555555555, 4.473684210526316, 4.826086956521739, 4.619047619047619, 4.166666666666667, 5.375, 5.045454545454546, 5.071428571428571, 4.5, 4.842105263157895, 4.133333333333334, 4.342465753424658, 4.884615384615385, 4.762711864406779, 4.528301886792453, 4.709677419354839, 4.181818181818182, 5.222222222222222, 4.666666666666667, 4.54, 4.038461538461538, 4.578947368421052, 4.785714285714286, 4.533333333333333, 4.566666666666666, 4.833333333333333, 4.6, 4.428571428571429, 4.466666666666667, 4.7894736842105265, 4.702127659574468, 4.3578947368421055, 4.818181818181818, 4.777777777777778, 4.4, 4.526315789473684, 4.9, 4.260869565217392, 5.190476190476191, 4.604166666666667, 5.3076923076923075, 4.6, 4.888888888888889, 4.666666666666667, 4.7727272727272725, 4.1875, 4.041666666666667, 4.75, 5.0476190476190474, 4.467741935483871, 4.3125, 4.489795918367347, 4.166666666666667, 4.4, 4.555555555555555, 4.611111111111111, 4.923076923076923, 4.301369863013699, 4.705882352941177, 4.923076923076923, 4.2727272727272725, 5.095238095238095, 5.2727272727272725, 4.322033898305085, 4.166666666666667, 4.636363636363637, 4.909090909090909, 4.583333333333333, 4.571428571428571, 5.285714285714286, 4.611111111111111, 4.785714285714286, 5.15625, 4.769230769230769, 5.416666666666667, 4.181818181818182, 4.294117647058823, 4.666666666666667, 4.725, 4.230769230769231, 5.230769230769231, 4.315789473684211, 4.387096774193548, 4.88, 4.444444444444445, 4.470588235294118, 4.920634920634921, 4.64, 4.285714285714286, 4.45945945945946, 4.72, 4.714285714285714, 4.269230769230769, 4.842105263157895, 4.866666666666666, 4.758620689655173, 4.142857142857143, 4.846153846153846, 4.2727272727272725, 5.25, 4.911111111111111, 4.888888888888889, 4.411764705882353, 5.578947368421052, 4.2105263157894735, 4.35, 4.7272727272727275, 4.833333333333333, 4.571428571428571, 4.512396694214876, 4.75, 5.3076923076923075, 5.076923076923077, 4.533333333333333, 4.291666666666667, 4.5, 5.5, 4.25, 5.0, 4.285714285714286, 4.441860465116279, 4.788461538461538, 5.0, 5.0, 4.346534653465347, 4.72, 4.333333333333333, 4.64935064935065, 4.596153846153846, 4.690476190476191, 4.133333333333334, 4.666666666666667, 5.0, 4.486486486486487, 4.2, 4.818181818181818, 4.166666666666667, 4.214285714285714, 4.235294117647059, 5.285714285714286, 4.875, 4.948936170212766, 4.644444444444445, 4.566666666666666, 4.769230769230769, 4.333333333333333, 4.538461538461538, 4.071428571428571, 4.714285714285714, 4.545454545454546, 4.904761904761905, 4.685714285714286, 4.538461538461538, 4.7631578947368425, 5.0, 4.521739130434782, 4.466666666666667, 5.0476190476190474, 4.47887323943662, 5.08, 4.9523809523809526, 4.35, 5.4, 4.617647058823529, 4.793103448275862, 4.902439024390244, 4.571428571428571, 5.115384615384615, 4.684210526315789, 4.555555555555555, 4.125, 4.625, 4.419354838709677, 4.5, 5.166666666666667, 4.379310344827586, 5.181818181818182, 5.142857142857143, 4.842105263157895, 4.35064935064935, 4.625, 4.516129032258065, 5.125, 4.770833333333333, 5.113207547169812, 4.9, 4.609195402298851, 4.541666666666667, 5.857142857142857, 4.357142857142857, 4.375, 4.753968253968254, 5.0, 4.235294117647059, 4.769230769230769, 4.190476190476191, 5.090909090909091, 5.833333333333333, 4.428571428571429, 4.608695652173913, 4.5588235294117645, 4.161290322580645, 4.819277108433735, 4.904761904761905, 3.909090909090909, 4.247058823529412, 4.315789473684211, 4.636363636363637, 4.384615384615385, 4.495238095238095, 4.782608695652174, 4.7407407407407405, 5.181818181818182, 4.75, 4.571428571428571, 4.631578947368421, 4.666666666666667, 4.529411764705882, 4.478260869565218, 4.333333333333333, 4.212765957446808, 4.363636363636363, 4.545454545454546, 4.2, 5.5, 4.34375, 4.448275862068965, 4.863636363636363, 4.65, 4.5, 4.4, 4.043478260869565, 4.25, 4.375, 4.236363636363636, 4.099009900990099, 4.282051282051282, 4.5, 4.451612903225806, 4.75, 4.304347826086956, 4.4, 4.681818181818182, 4.516129032258065, 4.473684210526316, 4.388059701492537, 5.088235294117647, 5.035714285714286, 4.384615384615385, 4.421052631578948, 4.714285714285714, 4.613636363636363, 4.1, 4.17741935483871, 4.358974358974359, 5.277777777777778, 4.763513513513513, 5.0, 5.185185185185185, 4.0, 4.666666666666667, 4.2727272727272725, 5.0, 4.6, 4.214285714285714, 4.217391304347826, 5.3, 4.342105263157895, 4.078431372549019, 5.0, 5.4, 5.052631578947368, 4.555555555555555, 5.172413793103448, 4.2272727272727275, 4.4375, 4.555555555555555, 3.9444444444444446, 3.972972972972973, 4.956521739130435, 4.65, 4.04, 4.535714285714286, 4.689655172413793, 4.294117647058823, 5.090909090909091, 4.428571428571429, 4.6, 4.0344827586206895, 4.441860465116279, 4.653846153846154, 4.529411764705882, 5.0, 3.76, 4.475, 4.666666666666667, 4.558139534883721, 4.568181818181818, 4.948717948717949, 5.0588235294117645, 4.545454545454546, 4.5, 4.344827586206897, 4.428571428571429, 4.703703703703703, 4.153846153846154, 4.545454545454546, 4.461538461538462, 4.6, 5.2727272727272725, 4.4, 4.388888888888889, 4.809523809523809, 4.729166666666667, 5.111111111111111, 4.7, 5.866666666666666, 4.583333333333333, 4.109090909090909, 4.413793103448276, 4.71, 4.491803278688525, 4.642857142857143, 4.695652173913044, 4.277777777777778, 4.3125, 4.4363636363636365, 4.787234042553192, 4.903225806451613, 4.176470588235294, 4.837837837837838, 4.636363636363637, 5.166666666666667, 5.0, 4.857142857142857, 5.076923076923077, 4.5625, 4.173913043478261, 4.75, 4.317073170731708, 4.25, 4.666666666666667, 4.470588235294118, 4.185185185185185, 4.535714285714286, 4.5, 5.121212121212121, 4.568627450980392, 5.0, 4.142857142857143, 4.615384615384615, 4.695652173913044, 4.136363636363637, 4.65, 4.769230769230769, 5.0, 5.2368421052631575, 4.666666666666667, 4.545454545454546, 4.583333333333333, 5.0, 4.173913043478261, 4.703703703703703, 4.7, 4.833333333333333, 5.217391304347826, 4.428571428571429, 4.172413793103448, 4.176470588235294, 5.0, 4.28, 5.127272727272727, 4.5, 4.666666666666667, 4.625, 4.588235294117647, 4.5, 4.566666666666666, 4.953125, 4.88, 4.863636363636363, 4.888888888888889, 4.2, 4.190476190476191, 4.051282051282051, 4.7727272727272725, 4.3076923076923075, 4.2, 4.730769230769231, 4.875, 4.8125, 4.857142857142857, 4.55, 4.525, 4.72, 4.821428571428571, 4.875, 4.617021276595745, 4.673913043478261, 5.148148148148148, 4.454545454545454, 4.469387755102041, 5.130434782608695, 3.7, 5.0, 4.823529411764706, 5.190476190476191, 3.9444444444444446, 4.733333333333333, 4.5, 4.9375, 4.857142857142857, 5.0, 4.833333333333333, 3.9545454545454546, 4.5625, 4.666666666666667, 4.058139534883721, 4.653846153846154, 4.338983050847458, 4.32258064516129, 4.857142857142857, 4.684210526315789, 4.764705882352941, 4.7317073170731705, 5.1891891891891895, 5.08, 4.625, 4.841269841269841, 4.362637362637362, 4.6, 5.277777777777778, 4.7272727272727275, 4.714285714285714, 4.352941176470588, 4.2, 4.611111111111111, 4.444444444444445, 5.090909090909091, 4.515837104072398, 4.694444444444445, 4.5, 4.8, 4.582089552238806, 4.8, 4.157894736842105, 4.529411764705882, 4.217391304347826, 5.15, 4.225806451612903, 4.545454545454546, 4.36, 4.121951219512195, 4.818181818181818, 4.875, 4.87719298245614, 4.363636363636363, 3.9285714285714284, 4.714285714285714, 4.235294117647059, 4.27027027027027, 4.333333333333333, 4.35, 4.684210526315789, 4.111111111111111, 4.5625, 4.923076923076923, 5.023809523809524, 5.151898734177215, 5.142857142857143, 4.375, 4.379310344827586, 4.555555555555555, 4.9, 4.2592592592592595, 4.821428571428571, 4.741935483870968, 4.8125, 4.523809523809524, 4.4423076923076925, 5.0, 4.538461538461538, 4.48, 4.78125, 3.6923076923076925, 4.407407407407407, 4.933333333333334, 4.838383838383838, 4.961538461538462, 4.111111111111111, 4.5, 4.111111111111111, 4.9, 4.666666666666667, 4.46875, 4.6923076923076925, 4.6923076923076925, 4.127659574468085, 4.640776699029126, 5.238095238095238, 4.454545454545454, 4.65, 4.384615384615385, 4.615384615384615, 5.0, 4.642857142857143, 4.571428571428571, 4.285714285714286, 4.888888888888889, 4.634146341463414, 4.419354838709677, 4.695652173913044, 4.658536585365853, 4.173913043478261, 4.722222222222222, 5.4324324324324325, 4.6923076923076925, 4.611111111111111, 4.1, 4.56, 4.612903225806452, 4.470588235294118, 4.2727272727272725, 4.205128205128205, 4.666666666666667, 4.261904761904762, 4.684210526315789, 4.321428571428571, 5.230769230769231, 5.125, 4.555555555555555, 4.6, 4.16, 4.777777777777778, 4.52, 4.695652173913044, 4.823529411764706, 4.818181818181818, 4.454545454545454, 4.6923076923076925, 5.125, 5.0, 3.9705882352941178, 4.225, 4.478260869565218, 4.831521739130435, 4.0, 4.24, 4.125, 4.0, 4.7272727272727275, 4.545454545454546, 3.914285714285714, 5.16, 4.185185185185185, 4.411764705882353, 4.608695652173913, 4.414634146341464, 4.96, 4.71875, 4.388888888888889, 4.461538461538462, 4.714285714285714, 4.56, 4.7, 5.125, 4.7272727272727275, 4.7272727272727275, 4.888888888888889, 4.904761904761905, 4.8125, 4.391304347826087, 4.2444444444444445, 3.8260869565217392, 4.571428571428571, 4.5, 4.538461538461538, 4.277777777777778, 5.1, 4.8, 5.157894736842105, 4.466666666666667, 4.923076923076923, 4.434782608695652, 4.6923076923076925, 5.0, 4.466666666666667, 5.055555555555555, 4.5, 4.685714285714286, 4.864864864864865, 4.294117647058823, 4.533333333333333, 4.661764705882353, 5.439024390243903, 4.7368421052631575, 4.754098360655738, 4.5, 4.2105263157894735, 4.68, 3.8055555555555554, 4.59375, 4.722222222222222, 5.0625, 5.015384615384615, 4.333333333333333, 4.904761904761905, 5.1, 3.923076923076923, 4.4692737430167595, 4.55, 4.625, 4.5, 4.96875, 4.5, 4.509090909090909, 4.685990338164252, 4.5, 5.125, 4.433333333333334, 4.6125, 4.45, 4.853658536585366, 5.333333333333333, 4.75, 4.666666666666667, 4.375, 4.5, 4.142857142857143, 5.214285714285714, 4.285714285714286, 4.153846153846154, 4.2, 4.6923076923076925, 4.75, 4.297297297297297, 4.666666666666667, 4.357142857142857, 5.1, 4.3125, 4.153846153846154, 3.9, 4.08, 5.5, 4.090909090909091, 4.642857142857143, 5.285714285714286, 4.230769230769231, 4.25, 4.333333333333333, 3.806451612903226, 4.806451612903226, 4.3125, 4.733333333333333, 4.304347826086956, 4.285714285714286, 5.0, 4.681818181818182, 4.133333333333334, 4.3076923076923075, 4.222222222222222, 4.714285714285714, 4.837837837837838, 4.928571428571429, 4.5, 4.96, 4.333333333333333, 4.75, 4.535714285714286, 4.366666666666666, 5.148936170212766, 5.0, 5.0, 5.142857142857143, 4.777777777777778, 4.533333333333333, 4.333333333333333, 4.3125, 4.944444444444445, 5.2, 4.388888888888889, 4.346153846153846, 5.421052631578948, 4.2592592592592595, 4.183673469387755, 4.673913043478261, 4.836734693877551, 4.421052631578948, 4.076923076923077, 4.529411764705882, 4.65, 4.518518518518518, 4.818181818181818, 4.151515151515151, 4.645161290322581, 4.791666666666667, 4.285714285714286, 4.674418604651163, 4.478260869565218, 4.222222222222222, 4.072727272727272, 4.476190476190476, 4.5, 4.2105263157894735, 4.3076923076923075, 4.535714285714286, 5.333333333333333, 5.042857142857143, 4.303030303030303, 4.4035087719298245, 4.67741935483871, 4.75, 4.578947368421052, 4.607142857142857, 3.9642857142857144, 4.764705882352941, 4.166666666666667, 4.666666666666667, 4.75, 4.314285714285714, 4.4, 4.846153846153846, 4.523809523809524, 4.625, 4.75, 4.666666666666667, 4.714285714285714, 4.709677419354839, 4.857142857142857, 4.961538461538462, 4.535714285714286, 4.970588235294118, 4.8, 4.636363636363637, 4.827586206896552, 4.1923076923076925, 4.590909090909091, 4.6, 4.378378378378378, 5.0, 4.333333333333333, 4.9393939393939394, 4.6938775510204085, 4.484848484848484, 4.823529411764706, 4.4, 4.545454545454546, 4.444444444444445, 5.580645161290323, 4.448275862068965, 5.6, 4.617647058823529, 4.090909090909091, 4.472477064220183, 4.222222222222222, 5.0625, 4.447368421052632, 4.282051282051282, 5.111111111111111, 4.551724137931035, 4.666666666666667, 4.285714285714286, 4.777777777777778, 4.4, 4.068965517241379, 4.526315789473684, 3.9375, 4.25, 5.125, 4.391304347826087, 4.833333333333333, 4.583333333333333, 4.333333333333333, 4.380434782608695, 4.611111111111111, 4.823529411764706, 3.875, 4.8, 5.041666666666667, 4.214285714285714, 4.3125, 4.538461538461538, 5.0, 4.638888888888889, 4.877551020408164, 4.696969696969697, 5.0588235294117645, 4.555555555555555, 4.04, 4.830985915492958, 4.627118644067797, 4.411764705882353, 4.888888888888889, 5.025, 4.611111111111111, 4.951871657754011, 4.385714285714286, 4.933333333333334, 4.6521739130434785, 4.827586206896552, 4.434782608695652, 4.918032786885246, 4.514285714285714, 4.363636363636363, 5.0285714285714285, 4.454545454545454, 5.294117647058823, 4.161290322580645, 4.4, 4.773584905660377, 5.054054054054054, 3.6666666666666665, 4.625, 4.813559322033898, 4.615384615384615, 4.871794871794871, 4.7368421052631575, 4.25, 4.523809523809524, 4.266666666666667, 4.761904761904762, 4.75, 4.769230769230769, 5.0, 4.425, 4.769230769230769, 4.181818181818182, 4.208333333333333, 4.354838709677419, 4.44, 4.9, 4.462264150943396, 4.88, 4.36, 4.380952380952381, 4.142857142857143, 4.373333333333333, 4.85, 4.809523809523809, 4.190476190476191, 5.217391304347826, 5.461538461538462, 4.45, 4.321428571428571, 3.782608695652174, 4.75, 4.617021276595745, 5.714285714285714, 4.8, 4.684210526315789, 4.739130434782608, 5.176470588235294, 4.323943661971831, 4.290322580645161, 4.5, 4.363636363636363, 5.25, 5.043478260869565, 4.333333333333333, 4.761904761904762, 4.614285714285714, 4.5, 4.570652173913044, 4.67741935483871, 4.829457364341085, 4.136363636363637, 4.5, 4.76, 4.454545454545454, 4.28125, 4.904761904761905, 3.8620689655172415, 4.916666666666667, 4.696969696969697, 4.547826086956522, 4.235294117647059, 4.75, 4.521739130434782, 4.7, 5.038461538461538, 4.375, 4.833333333333333, 4.416666666666667, 4.681818181818182, 5.0, 5.111111111111111, 5.25, 4.229166666666667, 4.472727272727273, 4.515151515151516, 4.696969696969697, 4.857142857142857, 4.651315789473684, 4.836734693877551, 4.75, 4.2, 4.625, 4.344827586206897, 4.333333333333333, 4.608695652173913, 4.3076923076923075, 4.208333333333333, 4.722222222222222, 4.3, 4.391304347826087, 4.833333333333333, 4.416666666666667, 4.545454545454546, 4.6440677966101696, 4.454545454545454, 4.5, 4.85, 4.6, 4.466666666666667, 5.0, 4.485714285714286, 4.428571428571429, 4.739726027397261, 5.454545454545454, 4.2894736842105265, 4.162790697674419, 4.818181818181818, 4.695652173913044, 5.030487804878049, 4.314285714285714, 4.5, 5.0, 4.954545454545454, 5.0, 4.5, 4.5, 3.857142857142857, 4.5, 4.604651162790698, 4.731343283582089, 4.321428571428571, 4.882352941176471, 4.909090909090909, 4.6923076923076925, 4.809523809523809, 4.40625, 4.515151515151516, 4.463276836158192, 4.636363636363637, 4.285714285714286, 4.877551020408164, 4.592592592592593, 4.442622950819672, 4.178571428571429, 4.62962962962963, 4.833333333333333, 4.708333333333333, 4.805555555555555, 4.55, 4.714285714285714, 4.6, 4.728571428571429, 4.634146341463414, 4.371428571428571, 4.333333333333333, 4.15625, 4.573770491803279, 4.611111111111111, 4.1923076923076925, 4.413043478260869, 4.153846153846154, 3.903225806451613, 4.241379310344827, 4.530612244897959, 4.628571428571429, 4.376146788990826, 4.833333333333333, 4.375, 4.857142857142857, 4.409090909090909, 4.627906976744186, 5.171052631578948, 4.0, 4.509433962264151, 4.708333333333333, 4.666666666666667, 4.217391304347826, 5.235294117647059, 4.369565217391305, 4.461538461538462, 4.2727272727272725, 4.695652173913044, 4.421052631578948, 4.684210526315789, 4.7368421052631575, 4.1, 4.936708860759493, 4.424242424242424, 4.6, 4.636363636363637, 4.842105263157895, 4.4, 3.857142857142857, 4.875, 4.523809523809524, 4.6, 4.571428571428571, 4.24, 4.5, 4.6, 4.296296296296297, 4.958333333333333, 4.545454545454546, 5.166666666666667, 4.777777777777778, 4.379310344827586, 4.105263157894737, 4.1, 4.571428571428571, 4.137931034482759, 4.85, 4.521739130434782, 4.2727272727272725, 4.454545454545454, 4.551724137931035, 4.846153846153846, 4.285714285714286, 4.0, 4.6, 4.642857142857143, 4.6521739130434785, 4.983050847457627, 4.315789473684211, 4.764705882352941, 4.806451612903226, 4.5, 4.651162790697675, 4.166666666666667, 5.363636363636363, 4.6923076923076925, 4.630434782608695, 4.4222222222222225, 5.0625, 4.447368421052632, 4.521739130434782, 4.764705882352941, 3.7916666666666665, 4.181818181818182, 4.486486486486487, 4.308943089430894, 4.631578947368421, 4.396825396825397, 4.483870967741935, 4.933333333333334, 4.5, 4.666666666666667, 4.218181818181818, 5.041666666666667, 4.769230769230769, 5.2105263157894735, 4.571428571428571, 5.090909090909091, 4.7272727272727275, 4.7272727272727275, 4.340909090909091, 4.555555555555555, 4.1875, 4.133333333333334, 4.7407407407407405, 4.321428571428571, 5.071428571428571, 4.0, 4.709677419354839, 4.809523809523809, 4.558282208588957, 4.5344827586206895, 5.055555555555555, 4.260869565217392, 4.636363636363637, 4.585365853658536, 4.537313432835821, 4.846153846153846, 4.5, 4.3, 4.869565217391305, 4.181818181818182, 4.962962962962963, 4.172413793103448, 4.153846153846154, 4.4375, 4.848101265822785, 4.657657657657658, 4.476190476190476, 4.65, 4.45945945945946, 4.56, 4.96969696969697, 4.62962962962963, 4.431818181818182, 4.714285714285714, 4.571428571428571, 3.9696969696969697, 4.076923076923077, 4.191489361702128, 5.05, 4.2727272727272725, 4.458333333333333, 4.75, 4.877551020408164, 4.580645161290323, 4.6521739130434785, 4.133333333333334, 4.614678899082569, 4.430622009569378, 4.666666666666667, 4.777777777777778, 4.333333333333333, 4.204545454545454, 4.463414634146342, 5.2, 5.0, 5.085714285714285, 4.304347826086956, 4.022222222222222, 4.757575757575758, 5.0, 4.782894736842105, 4.59375, 4.933333333333334, 4.333333333333333, 4.4, 4.380952380952381, 4.477611940298507, 4.4, 4.958333333333333, 4.743589743589744, 4.294117647058823, 4.461538461538462, 4.413043478260869, 5.016949152542373, 4.523809523809524, 4.594594594594595, 4.277777777777778, 4.631147540983607, 4.0, 4.375, 4.869565217391305, 4.142857142857143, 4.888888888888889, 4.636363636363637, 4.193548387096774, 4.3478260869565215, 4.797101449275362, 4.739130434782608, 4.642857142857143, 4.0, 4.4817073170731705, 4.733333333333333, 4.555555555555555, 4.909090909090909, 4.333333333333333, 4.25, 4.1875, 4.5, 4.271028037383178, 4.808219178082192, 4.7, 4.75, 4.722222222222222, 4.111111111111111, 4.235294117647059, 4.633333333333334, 4.655172413793103, 5.0, 5.043478260869565, 4.607142857142857, 5.2105263157894735, 4.882352941176471, 4.6, 5.181818181818182, 4.53932584269663, 4.6020408163265305, 5.019354838709678, 4.589285714285714, 4.71875, 4.6875, 4.897435897435898, 4.222222222222222, 4.914285714285715, 4.688888888888889, 5.333333333333333, 4.397435897435898, 4.315789473684211, 4.85, 4.380952380952381, 4.96, 4.757575757575758, 4.4, 4.714285714285714, 4.730769230769231, 4.611111111111111, 4.8076923076923075, 4.5476190476190474, 4.333333333333333, 4.5, 4.136363636363637, 3.933333333333333, 4.075, 4.823529411764706, 4.777777777777778, 4.6875, 4.391304347826087, 4.466666666666667, 4.157894736842105, 4.515151515151516, 4.304347826086956, 4.756756756756757, 5.2592592592592595, 4.615384615384615, 4.696969696969697, 4.45, 4.7727272727272725, 4.454545454545454, 5.405405405405405, 4.466666666666667, 4.833333333333333, 4.625, 4.737704918032787, 4.023255813953488, 4.318181818181818, 5.111111111111111, 4.157894736842105, 4.3, 4.75, 4.678571428571429, 4.533333333333333, 4.923076923076923, 4.566666666666666, 4.545454545454546, 4.327272727272727, 4.340909090909091, 4.452173913043478, 4.583333333333333, 4.230769230769231, 4.3, 4.96, 4.285714285714286, 4.615384615384615, 4.3125, 5.0, 4.961538461538462, 4.555555555555555, 4.409090909090909, 4.75, 4.175, 4.6521739130434785, 4.714285714285714, 4.5625, 4.538461538461538, 4.37037037037037, 4.571428571428571, 4.5, 5.071428571428571, 3.9615384615384617, 4.68, 4.741935483870968, 5.333333333333333, 4.75, 4.75, 4.45, 4.225806451612903, 4.3076923076923075, 4.962121212121212, 4.625, 4.5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn1PiZ3tHRSP"
      },
      "source": [
        "old_traindata=get_data(location=basepath+'old/train_samples.txt')\n",
        "old_train_textlist=[old_traindata[key] for key in old_traindata.keys()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_z094o1d_fh"
      },
      "source": [
        "Parametrii word2vec: size=1000 e dimensiunea outputului, rezonabil avand in vedere ca avem sute de mii de cuvinte. Cuvintele care apar de mai putine ori decat `min_count` sunt ignorate, dar tocmai acelea ne intereseaza."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmP1jiWpkOX5"
      },
      "source": [
        "vw_size=5000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8_jjx3rBAIo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "6eeb1924-415a-4821-945c-13eae36044fe"
      },
      "source": [
        "print(train_textlist[0])\n",
        "print(train_textlist[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[';%fE', 'mr#&', 'crmx', 'temjc@m', \"%'wb:\", '}hHAm@@m', 'ykm=aa', 'Eje@', 'Ejh=', 'EcrZk', 's}lZ$', 'rhfh', '}h@kofe@mk', 'RgWE<', '>mfor@m', '@#@', 'm=hkaa', 'TFr>o*', 'h}Ah', 'EHfm}e@mHk', 'e#hj@', 'j&}k', 'gAmaH', 'mgmkafe', 'cmT:', 'k>.h', 'XH(q!', '}FW', '@*oDgB', '#Sx.W', 'hZ', 'jh=', 'chrZ', '}k#h', 'svcNt', 'ejmc@m', 'gYAmZ', 'efke@m', 'h}Ah', 'g@@m', '>m&', '}%k', 'tr(:', ';wxq', 'Ere', 'E*}ga', 'hgZ', 'h$mhr@m', 'tkafe@m', 't@A', '%#sE', '=hkaa@m', 'm*gH', 'E@he=@m', 'wk}hX', 'Ejhr=@m', 'Ejhr=@m', 'h@mg:@']\n",
            "['sAFW', 'K#xk}t', 'fH@ae', 'm&Xd', '>h&', '@#', 'l@Rd}a', '@Hc', 'liT', 'ehAr@m', 'Xgmz', '!}a', '}eAr@m', 'Be', 'g@@m', 'efH', 'RB(D', 'Ehk&']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvCW6dpQdmja"
      },
      "source": [
        "w2v = Word2Vec(sentences=train_textlist, size=vw_size, min_count=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJtl4BjT5GrL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "f8416335-b535-4eec-e863-a392e6b645c0"
      },
      "source": [
        "w2v.save(basepath+\"word2vec.model\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv7F_rErf9aM"
      },
      "source": [
        "Functie care da dataset **pentru Word2Vec**. Inputul e chiar dictionarul de texte si word2vec-ul care le va converti. Ca feature al unui text vom lua suma tuturor vectorilor pentru a avea dimensiune constanta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fvx2o8NphPbf"
      },
      "source": [
        "def get_word2vec_features(w2vmodel, vw_size: int, data: dict):\n",
        "    \"\"\"\n",
        "    data e un dictionar cu int-uri id-uri de texte si texte, impartite in cuvinte\n",
        "    labels e un dictionar cu int-uri id-uri de texte si int-uri 0 sau 1 (romaneasca sau moldoveneasca)\n",
        "    \"\"\"\n",
        "    #!ASSUMES DATA IS NOT YET SPLIT\n",
        "    big_list=[data[key].split(\" \") for key in data.keys()]\n",
        "    features=torch.zeros(len(big_list), vw_size)\n",
        "    print(f'features shape: {features.shape}')\n",
        "    for txt_idx in range(len(big_list)):\n",
        "      for word in big_list[txt_idx]:\n",
        "        if word in w2vmodel.wv: #daca cuvantul e in vocabularul creeat deja\n",
        "          word_enc=torch.tensor(w2vmodel[word])\n",
        "          # print(f'{word} encoded as {word_enc}')\n",
        "          features[txt_idx]+=word_enc\n",
        "      features[txt_idx]=(1/len(big_list[txt_idx]))*features[txt_idx]\n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKMb7QxPfLX_"
      },
      "source": [
        "#a test that labels aren't messed up; WILL NOT WORK IF dict_train_labels\n",
        "read_keys=[key for key in train_data.keys()]\n",
        "label_keys=[key for key in dict_train_labels.keys()]\n",
        "for i in range(len(read_keys)):\n",
        "  print(f'{read_keys[i]}  {label_keys[i]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-Z-bCTqlahD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "1e7cc0ed-8d28-4e2b-fad9-f6d4698081a3"
      },
      "source": [
        "train_features=get_word2vec_features(w2v, vw_size, train_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "features shape: torch.Size([7757, 5000])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mmCESd5ws5F"
      },
      "source": [
        "torch.save(train_features, basepath+'w2v_train_features.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oBSJrsxxxxS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "62f3b491-0c1d-4475-cbe8-a770df7cf8d1"
      },
      "source": [
        "validation_features = get_word2vec_features(w2v, vw_size, validation_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "features shape: torch.Size([2656, 5000])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zPoRjFjyYux"
      },
      "source": [
        "torch.save(validation_features, basepath+'w2v_validation_features.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHD-olKATp7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "outputId": "142852d9-95b6-4e0c-da9f-238918895d5a"
      },
      "source": [
        "print(train_features)\n",
        "print(validation_features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.0374,  0.0446, -0.0241,  ..., -0.0049, -0.0145, -0.0163],\n",
            "        [-0.0450,  0.0536, -0.0290,  ..., -0.0059, -0.0174, -0.0196],\n",
            "        [-0.0528,  0.0633, -0.0342,  ..., -0.0070, -0.0207, -0.0231],\n",
            "        ...,\n",
            "        [-0.0533,  0.0641, -0.0345,  ..., -0.0071, -0.0209, -0.0233],\n",
            "        [-0.0222,  0.0266, -0.0144,  ..., -0.0029, -0.0087, -0.0097],\n",
            "        [-0.0445,  0.0533, -0.0288,  ..., -0.0058, -0.0174, -0.0194]])\n",
            "tensor([[-0.0703,  0.0844, -0.0455,  ..., -0.0094, -0.0275, -0.0307],\n",
            "        [-0.0534,  0.0640, -0.0346,  ..., -0.0071, -0.0208, -0.0233],\n",
            "        [-0.0609,  0.0731, -0.0394,  ..., -0.0081, -0.0238, -0.0265],\n",
            "        ...,\n",
            "        [-0.0323,  0.0386, -0.0209,  ..., -0.0042, -0.0126, -0.0141],\n",
            "        [-0.0665,  0.0797, -0.0430,  ..., -0.0088, -0.0260, -0.0290],\n",
            "        [-0.0548,  0.0657, -0.0355,  ..., -0.0073, -0.0215, -0.0239]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J60UlPFvdSJz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "d535384e-95b5-442c-fd04-c222b3f302a2"
      },
      "source": [
        "# NO LONGER NEEDED\n",
        "# validation_target_features=get_word2vec_features(w2v, vw_size, validation_target_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "features shape: torch.Size([215, 1000])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jElR8shdWLh"
      },
      "source": [
        "# NO LONGER NEEDED\n",
        "# torch.save(validation_target_features, basepath+'w2v_validation_target_features.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YkMFfLFzKKz"
      },
      "source": [
        "Incarca featururile **daca au fost salvate deja** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qMOnOhk1a1E"
      },
      "source": [
        "train_features=torch.load(basepath+'w2v_train_features.pt')\n",
        "validation_features=torch.load(basepath+'w2v_validation_features.pt')\n",
        "# validation_target_features=torch.load(basepath+'w2v_validation_target_features.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAdn5eR_AAsn"
      },
      "source": [
        "print(validation_features[:30, 1:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tP_2k7F4sBq"
      },
      "source": [
        "Creeaza dataseturi si data loadere"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RaYBSky4xG4"
      },
      "source": [
        "def get_dataloader(data, labels, batch_size, shuffle=False):\n",
        "  from torch.utils.data import TensorDataset\n",
        "  from torch.utils.data import DataLoader\n",
        "  dataset = TensorDataset(data, labels)\n",
        "  data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=False)\n",
        "  return data_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnUV6W4z4whQ"
      },
      "source": [
        "batch_size=800"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hq1rRBgd5QUH"
      },
      "source": [
        "train_dataloader=get_dataloader(train_features, train_labels, batch_size, False)\n",
        "validation_dataloader=get_dataloader(validation_features, validation_labels, batch_size, False)\n",
        "\n",
        "#NOT NEEDED\n",
        "# validation_target_dataloader=get_dataloader(validation_target_features, validation_target_labels, batch_size, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL4Y8vkw2Imu"
      },
      "source": [
        "#Partea II: Configurarea retelelor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Q7RXggWWP2q"
      },
      "source": [
        "## **Clase de retele neuronalle folositle pentru rezolvarea task-ului**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJt2tTVQWPQq"
      },
      "source": [
        "class ClassifierNN(nn.Module):\n",
        "  def __init__(self, in_size: int, out_size: int, interm_size: int, no_interm_layers: int):\n",
        "    super().__init__()\n",
        "    #apeleaza constructorul clasei mostenite\n",
        "    self._no_interm_layers=no_interm_layers\n",
        "    self._layers = [nn.Linear(in_size, interm_size)]\n",
        "    for i in range(no_interm_layers - 1) :\n",
        "      self._layers.append(nn.Linear(interm_size, interm_size))\n",
        "    self._layers.append(nn.Linear(interm_size, out_size))\n",
        "    self._layers=nn.ModuleList(self._layers)\n",
        "  def __call__(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    out = torch.relu(self._layers[0](x))\n",
        "    for i in range(1, self._no_interm_layers - 1) :\n",
        "      out = torch.relu(self._layers[i](out))\n",
        "    out =self._layers[-1](out)\n",
        "    out=F.log_softmax(out, dim=1)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDXC6a59Rn6F"
      },
      "source": [
        "class DescendingClassifier(nn.Module):\n",
        "  def __init__(self, in_size):\n",
        "    super().__init__()\n",
        "    self.f1=nn.Linear(in_size, 1000)\n",
        "    self.f2=nn.Linear(1000, 100)\n",
        "    self.f3=nn.Linear(100, 2)\n",
        "  def __call__(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    out=torch.relu(self.f1(x))\n",
        "    out=torch.relu(self.f2(out))\n",
        "    out=self.f3(out)\n",
        "    out=torch.log_softmax(out, dim=1)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-57_qGQWShi"
      },
      "source": [
        "##**Functii de training de model:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lxtw_bARQJy"
      },
      "source": [
        "### **TfIdf trainer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od3PUOqib3YP"
      },
      "source": [
        "#NOT USED\n",
        "def lossfun(output, labels):\n",
        "  #NOT IN USE\n",
        "  loss=torch.mean(torch.abs(torch.argmax(output, dim=1)-labels).type(torch.FloatTensor))\n",
        "  return(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r82mUOc6WWMw"
      },
      "source": [
        "def train_model_tfidf(model, optim, features, true_labels, batch_size, no_epochs, treshold=0.3):\n",
        "    for e in range(no_epochs):\n",
        "        for i in range(features.shape[0] // batch_size) :\n",
        "            optim.zero_grad()\n",
        "            batch, batch_labels=get_batch_tfidf(features, true_labels, batch_size, i)\n",
        "            predicted=model(batch)\n",
        "            loss=torch.nn.functional.nll_loss(predicted, batch_labels)\n",
        "            loss.backward()\n",
        "            with torch.no_grad():\n",
        "                optim.step()\n",
        "                print(f'epoch {e} has train loss: {loss} on batch {i} (batch_size is {batch_size}')\n",
        "                # print(f'At epoch {e} model predicts:{predicted[:5]}')\n",
        "                # print(f'Real lables of batch are: {batch_labels[:5]}')\n",
        "                from sklearn.metrics import confusion_matrix\n",
        "                print(f'At epoch {e} on batch {i} confusion matrix is {confusion_matrix(batch_labels, torch.argmax(predicted, dim=1))}')\n",
        "            if (loss<treshold):\n",
        "              break\n",
        "    return(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-fzuIaZiPBi"
      },
      "source": [
        "### **Word2Vec trainer**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8ywf-kodte8"
      },
      "source": [
        "def train_model_word2vec(model, optim, train_loader, batch_size, no_epochs, treshold=0.05):\n",
        "    for e in range(no_epochs):\n",
        "        for batch_idx, (batch, batch_labels) in enumerate(train_loader):\n",
        "            optim.zero_grad()\n",
        "            predicted=model(batch)\n",
        "            loss=torch.nn.functional.cross_entropy(predicted, batch_labels)\n",
        "            loss.backward()\n",
        "            with torch.no_grad():\n",
        "                optim.step()\n",
        "                # print(f'batch is {batch[:10]}')\n",
        "                # print(f'batch labels are {batch_labels[:10]}')\n",
        "                print(f'epoch {e} has train loss: {loss} on batch {batch_idx} (batch_size is {batch_size})')\n",
        "                print(f'At epoch {e} model predicts:{predicted[:5]}')\n",
        "                print(f'Real lables of batch are: {batch_labels[:5]}')\n",
        "                print('\\n\\n\\n')\n",
        "            if (loss<treshold):\n",
        "              break\n",
        "    return(model)\n",
        "\n",
        "def test_model_word2vec(model, test_loader, batch_size):\n",
        "  total_loss=0\n",
        "  num_batches=0\n",
        "  for batch_idx, (batch, batch_labels) in enumerate(test_loader):\n",
        "    with torch.no_grad():\n",
        "      predicted=model(batch)\n",
        "      loss=torch.nn.functional.nll_loss(predicted, batch_labels)\n",
        "      num_batches+=1\n",
        "      total_loss+=loss\n",
        "      print(f'We have test loss: {loss} on batch {batch_idx} (batch_size is {batch_size})')\n",
        "  total_loss/=num_batches\n",
        "  return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt3Q6aBK2h3F"
      },
      "source": [
        "##**Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hl-hqeGWXSf"
      },
      "source": [
        "Initializare trainig:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1qEEuKNWZNS"
      },
      "source": [
        "# model=ClassifierNN(word_count_train.shape[1], 2, 15, 2).to(device) #(i, o, interm_size, number of interm layers)\n",
        "# model=DescendingClassifier(in_size=word_count_train.shape[1]).to(device)\n",
        "# batch_size=600\n",
        "\n",
        "model=ClassifierNN(in_size=word_count_train.shape[1], out_size=2, interm_size=10000, no_interm_layers=2).to(device)\n",
        "# model=DescendingClassifier(in_size=word_count_train.shape[1]).to(device)\n",
        "# model=ClassifierNN(in_size=train_features.shape[1], out_size=2, interm_size=1000, no_interm_layers=2).to(device)\n",
        "# model=DescendingClassifier(in_size=train_features.shape[1]).to(device)\n",
        "optim=torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "model=model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vu3FAlpBhpti",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "79a0b965-d6f3-4b94-e42c-617e90d68f12"
      },
      "source": [
        "# print(train_features.shape[1])\n",
        "print(word_count_train.shape[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32874\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YPvxdnCmQiB"
      },
      "source": [
        "no_epochs=1\n",
        "treshold=0.2\n",
        "batch_size=800"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r49D4w2DWlAL"
      },
      "source": [
        "Training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXt-2HrbWokF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "32d35b86-507d-46d1-8511-ecba42c9a2a4"
      },
      "source": [
        "# model=train_model_word2vec(model, optim, train_dataloader, batch_size, no_epochs, treshold)\n",
        "no_epochs=1\n",
        "treshold=0.2\n",
        "batch_size=800\n",
        "model=train_model_tfidf(model, optim, word_count_train, train_labels, batch_size, no_epochs, treshold)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0 has train loss: 0.0038216998800635338 on batch 0 (batch_size is 800\n",
            "At epoch 0 on batch 0 confusion matrix is [[417   0]\n",
            " [  0 383]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxUjd1MuTvcI"
      },
      "source": [
        "for param in model.parameters():\n",
        "  print(param)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ajt6ddcRUR-3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        },
        "outputId": "0f0a3540-5879-42db-8acd-90beec71614e"
      },
      "source": [
        "batch, _=get_batch_tfidf(word_count_train, train_labels, 50, 0)\n",
        "print(model(batch))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.8359, -0.1737],\n",
            "        [-1.4613, -0.2639],\n",
            "        [-1.8600, -0.1692],\n",
            "        [-1.9844, -0.1479],\n",
            "        [-0.0878, -2.4766],\n",
            "        [-1.7329, -0.1945],\n",
            "        [-0.0926, -2.4253],\n",
            "        [-0.0580, -2.8767],\n",
            "        [-0.2125, -1.6531],\n",
            "        [-0.2117, -1.6565],\n",
            "        [-1.5653, -0.2345],\n",
            "        [-0.1016, -2.3373],\n",
            "        [-1.5445, -0.2400],\n",
            "        [-1.9191, -0.1587],\n",
            "        [-0.0364, -3.3308],\n",
            "        [-1.3442, -0.3021],\n",
            "        [-2.3012, -0.1055],\n",
            "        [-1.5691, -0.2335],\n",
            "        [-1.4266, -0.2746],\n",
            "        [-2.0704, -0.1348],\n",
            "        [-1.4314, -0.2731],\n",
            "        [-0.0993, -2.3590],\n",
            "        [-0.1492, -1.9764],\n",
            "        [-0.2864, -1.3900],\n",
            "        [-1.8057, -0.1795],\n",
            "        [-0.0367, -3.3230],\n",
            "        [-0.2428, -1.5345],\n",
            "        [-0.1803, -1.8021],\n",
            "        [-0.1676, -1.8689],\n",
            "        [-0.1175, -2.1998],\n",
            "        [-0.0445, -3.1344],\n",
            "        [-1.3895, -0.2866],\n",
            "        [-0.1783, -1.8122],\n",
            "        [-1.9966, -0.1459],\n",
            "        [-0.1460, -1.9963],\n",
            "        [-0.0714, -2.6744],\n",
            "        [-1.6445, -0.2146],\n",
            "        [-1.4366, -0.2715],\n",
            "        [-1.5738, -0.2323],\n",
            "        [-0.1974, -1.7196],\n",
            "        [-0.1394, -2.0396],\n",
            "        [-0.1772, -1.8175],\n",
            "        [-0.0883, -2.4710],\n",
            "        [-0.0686, -2.7139],\n",
            "        [-0.2312, -1.5778],\n",
            "        [-1.6757, -0.2072],\n",
            "        [-0.1371, -2.0545],\n",
            "        [-0.0551, -2.9260],\n",
            "        [-3.0255, -0.0498],\n",
            "        [-0.0959, -2.3918]], grad_fn=<LogSoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHQBHBs23pwa"
      },
      "source": [
        "Salvare model. **NOT RUN IF TRAIN HASN'T RUN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_GTgwiO3vK_"
      },
      "source": [
        "torch.save(model.state_dict(), basepath+'tfidf_8-12ngrams_classnn_bsize800')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPUNOMYs3vkq"
      },
      "source": [
        "Importare model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGbg3dmm3w7N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e517c062-961c-493f-81a1-661769c49688"
      },
      "source": [
        "model.load_state_dict(torch.load(basepath+'tfidf_8-12ngrams_classnn_bsize800'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vJymtPTSH4Q"
      },
      "source": [
        "### **Test TfIdf words**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vva5TRc6WpWN"
      },
      "source": [
        "Testare pe datele de validare - **TfIdf**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IELSfmoije4p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e33b5d4c-3ad9-4fa5-bec3-25280ce7c417"
      },
      "source": [
        "# converteste matricea la una densa\n",
        "word_count_validation_dense=torch.Tensor(word_count_validation.todense())\n",
        "output_validation=model(torch.Tensor(word_count_validation_dense))\n",
        "with torch.no_grad():\n",
        "  loss_validation=torch.abs(torch.argmax(output_validation, dim=1)-validation_labels)\n",
        "  loss_validation=loss_validation.type(torch.FloatTensor)\n",
        "print(f'Validation loss calculated with argmax is {torch.mean(loss_validation)}')\n",
        "from sklearn.metrics import f1_score\n",
        "print(f'Validation F1 score is {f1_score(validation_labels, torch.argmax(output_validation, dim=1))}')\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(f'Validation confusion matrix is {confusion_matrix(validation_labels, torch.argmax(output_validation, dim=1))}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation loss calculated with argmax is 0.4100150465965271\n",
            "Validation F1 score is 0.6289608177172061\n",
            "Validation confusion matrix is [[644 657]\n",
            " [432 923]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6HB2fLhR9Aj"
      },
      "source": [
        "### **Test TfIdf n-grams**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBT9-yJBRfpN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "f949ba83-9d1f-4623-9ecf-0cb0b45cd56c"
      },
      "source": [
        "# converteste matricea la una densa\n",
        "word_count_validation_ngrams_dense=torch.Tensor(word_count_validation_ngrams.todense())\n",
        "output_validation=model(torch.Tensor(word_count_validation_ngrams_dense))\n",
        "with torch.no_grad():\n",
        "  print(output_validation)\n",
        "  loss_validation=torch.abs(torch.argmax(output_validation, dim=1)-validation_labels)\n",
        "  # loss_validation=F.nll_loss(output_validation, validation_labels)\n",
        "  loss_validation=loss_validation.type(torch.FloatTensor)\n",
        "print(f'Validation loss is {torch.mean(loss_validation)}')\n",
        "# print(f'Validation loss is {loss_validation}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.5236, -0.8974],\n",
            "        [-0.4261, -1.0586],\n",
            "        [-2.2695, -0.1091],\n",
            "        ...,\n",
            "        [-0.6792, -0.7073],\n",
            "        [-1.7652, -0.1877],\n",
            "        [-1.5446, -0.2400]], grad_fn=<LogSoftmaxBackward>)\n",
            "Validation loss is 0.36031627655029297\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKsYz7GOVLRh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "8ca364de-5ac6-44d7-9408-fd256df5fa75"
      },
      "source": [
        "#train test\n",
        "# converteste matricea la una densa\n",
        "word_count_train_ngrams_dense=torch.Tensor(word_count_train_ngrams.todense())\n",
        "output_validation=model(torch.Tensor(word_count_train_ngrams_dense))\n",
        "with torch.no_grad():\n",
        "  print(output_validation)\n",
        "  loss_validation=torch.abs(torch.argmax(output_validation, dim=1)-train_labels)\n",
        "  # loss_validation=F.nll_loss(output_validation, validation_labels)\n",
        "  loss_validation=loss_validation.type(torch.FloatTensor)\n",
        "print(f'Validation loss is {torch.mean(loss_validation)}')\n",
        "# print(f'Validation loss is {loss_validation}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-3.0492e+00, -4.8557e-02],\n",
            "        [-5.6231e+00, -3.6201e-03],\n",
            "        [-5.0480e+00, -6.4426e-03],\n",
            "        ...,\n",
            "        [-2.4009e+00, -9.5008e-02],\n",
            "        [-1.9962e+00, -1.4601e-01],\n",
            "        [-2.6289e+00, -7.4890e-02]], grad_fn=<LogSoftmaxBackward>)\n",
            "Validation loss is 0.3654763400554657\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVYPNQGPjcJz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "59b3a182-4515-4ec3-ce4d-afe77ce3386c"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(validation_labels, torch.argmax(output_validation, dim=1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[864, 437],\n",
              "       [706, 649]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vagDjDyEhUp-"
      },
      "source": [
        "### **Test word2vec**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGww0q3lhXPN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "1d63e5cc-15cb-473d-c58c-48cc2d8ac55a"
      },
      "source": [
        "total_loss=test_model_word2vec(model, validation_dataloader, batch_size)\n",
        "print(f'total test loss is {total_loss}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have test loss: 0.6807268261909485 on batch 0 (batch_size is 800)\n",
            "We have test loss: 0.6872389912605286 on batch 1 (batch_size is 800)\n",
            "We have test loss: 0.686125636100769 on batch 2 (batch_size is 800)\n",
            "We have test loss: 0.6883438229560852 on batch 3 (batch_size is 800)\n",
            "total test loss is 0.6856088042259216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AXUclj3bPon"
      },
      "source": [
        "### Predicting on test and saving to csv:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfVZ8iuCkkrx"
      },
      "source": [
        "test_data=get_data(basepath+'test_samples.txt')\n",
        "test_ids=[key for key in test_data.keys()]\n",
        "write_to_file(test_data, basepath+\"dict_test_data.json\")\n",
        "word_count_test=torch.Tensor(cv.transform([test_data[key] for key in test_data.keys()]).todense())\n",
        "pred_test=model(word_count_test)\n",
        "test_dict={}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9pWCIf3PxVq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "7a23de16-d8fc-48f2-9a8b-5d3fbf198ce1"
      },
      "source": [
        "print(pred_test)\n",
        "pred_test_args=torch.argmax(pred_test, axis=1)\n",
        "print(pred_test_args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.4314, -0.2731],\n",
            "        [-0.0135, -4.3101],\n",
            "        [-1.4291, -0.2738],\n",
            "        ...,\n",
            "        [-1.3479, -0.3008],\n",
            "        [-2.0166, -0.1428],\n",
            "        [-0.8530, -0.5554]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([1, 0, 1,  ..., 1, 1, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5XbFaHYPo84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "13c06582-aabe-4fa3-e16d-c73e00536724"
      },
      "source": [
        "for i in range(len(test_data.keys())):\n",
        "  test_dict[test_ids[i]]=int(torch.argmax(pred_test[i]))\n",
        "print(test_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'110499': 1, '101319': 0, '108883': 1, '100925': 0, '110852': 0, '109538': 1, '108874': 0, '100082': 1, '110470': 0, '109725': 0, '103874': 1, '108214': 1, '102266': 1, '112940': 1, '102318': 1, '102162': 1, '106268': 1, '101888': 0, '108397': 1, '100901': 1, '106980': 1, '106283': 0, '110891': 0, '103818': 0, '111938': 1, '106200': 0, '109116': 0, '107311': 1, '102213': 1, '112378': 1, '110819': 1, '112456': 1, '110846': 1, '104540': 0, '110136': 1, '110718': 0, '103530': 1, '101797': 0, '110143': 0, '102987': 1, '108970': 0, '102506': 1, '109392': 1, '100695': 0, '103336': 1, '110388': 1, '101995': 0, '109812': 0, '107717': 0, '106342': 0, '112201': 0, '104207': 1, '105057': 0, '100110': 1, '103594': 1, '110747': 0, '101456': 1, '108880': 0, '103008': 1, '112871': 1, '101314': 1, '110408': 1, '110125': 1, '106336': 0, '104844': 1, '101023': 0, '110072': 0, '107032': 1, '105883': 0, '101843': 0, '108219': 0, '110919': 1, '104064': 1, '103193': 0, '100828': 0, '103003': 1, '101164': 1, '100528': 1, '103058': 1, '105311': 0, '103760': 0, '106328': 1, '102560': 0, '112713': 1, '110644': 1, '100809': 0, '110002': 1, '100998': 1, '107593': 1, '110869': 0, '109575': 1, '104607': 0, '100909': 0, '106267': 1, '112285': 0, '109520': 0, '100407': 0, '101775': 1, '100014': 1, '103061': 1, '112266': 0, '112412': 0, '109710': 0, '110987': 1, '109718': 1, '104144': 1, '103589': 0, '102550': 0, '106958': 0, '103499': 1, '111890': 0, '109223': 1, '106925': 0, '108481': 0, '112029': 0, '108155': 0, '105843': 1, '101345': 0, '107741': 1, '102728': 1, '100850': 0, '100504': 1, '105346': 0, '109888': 1, '108400': 1, '104394': 1, '101142': 1, '102330': 0, '108077': 1, '101814': 0, '102177': 0, '109238': 1, '100574': 1, '109178': 1, '106081': 1, '106866': 1, '109104': 1, '105642': 0, '100929': 1, '101772': 1, '103053': 1, '102481': 1, '107776': 0, '105950': 1, '103950': 0, '103100': 0, '109152': 1, '107769': 1, '103549': 1, '104228': 1, '107568': 0, '110815': 1, '111073': 1, '103370': 0, '109466': 1, '111769': 0, '112206': 0, '107069': 1, '105031': 1, '111235': 1, '103743': 1, '104359': 1, '111945': 1, '108791': 0, '109643': 0, '101520': 0, '112325': 1, '101469': 0, '108466': 0, '103537': 1, '108822': 1, '102741': 1, '101743': 0, '104771': 0, '100372': 0, '102904': 1, '106915': 0, '102556': 0, '101691': 1, '108082': 1, '110304': 1, '108477': 0, '104413': 0, '107838': 1, '105236': 0, '104888': 0, '108660': 0, '101097': 1, '100898': 1, '109257': 1, '111778': 0, '110487': 0, '109544': 0, '103107': 0, '107387': 1, '110344': 0, '111476': 0, '105187': 0, '107940': 1, '110249': 0, '104546': 0, '104241': 1, '109098': 1, '112880': 1, '103990': 1, '104564': 1, '109692': 0, '100304': 0, '101494': 0, '101688': 1, '111661': 1, '101406': 1, '112311': 1, '103768': 1, '108906': 0, '106001': 1, '107261': 1, '110949': 1, '111273': 0, '100640': 1, '107352': 1, '110660': 1, '106307': 1, '103154': 1, '111774': 0, '110081': 1, '112079': 1, '104736': 1, '104177': 1, '105138': 1, '103292': 1, '100016': 1, '111430': 0, '104809': 0, '103027': 1, '107704': 0, '110441': 1, '107177': 0, '109366': 0, '111095': 0, '100937': 1, '110243': 1, '104071': 0, '102230': 1, '100006': 0, '106089': 1, '100475': 1, '103025': 0, '105558': 1, '102373': 0, '102413': 0, '110502': 1, '100718': 0, '111564': 1, '104578': 0, '102524': 1, '105505': 1, '105822': 1, '107768': 0, '104746': 1, '103572': 0, '111349': 0, '105990': 0, '101898': 1, '111599': 0, '105625': 0, '110983': 1, '101698': 0, '102444': 1, '106942': 0, '112167': 1, '110343': 0, '102487': 1, '102324': 1, '101909': 0, '109097': 1, '112284': 1, '106728': 1, '100520': 0, '106520': 1, '111748': 0, '109923': 1, '112841': 1, '100904': 0, '102151': 0, '109478': 1, '106786': 1, '111885': 1, '106019': 1, '101001': 1, '104347': 0, '107252': 1, '101006': 0, '107273': 1, '106234': 0, '105976': 0, '110358': 1, '104906': 1, '112316': 1, '108597': 0, '100961': 0, '107412': 1, '102753': 1, '102348': 1, '111274': 1, '106296': 0, '110746': 0, '112058': 1, '103325': 1, '109233': 1, '103252': 0, '102264': 1, '103083': 1, '102659': 0, '100786': 0, '109234': 1, '105808': 0, '111093': 1, '108148': 0, '101394': 1, '110584': 0, '109972': 0, '110836': 1, '100721': 1, '110938': 1, '104584': 0, '105383': 0, '106788': 1, '106155': 1, '112908': 0, '102150': 1, '105203': 1, '101715': 1, '112455': 1, '108842': 1, '110082': 0, '112942': 1, '100705': 0, '112726': 1, '106994': 1, '101067': 1, '107180': 0, '106284': 0, '103534': 1, '102845': 1, '111677': 1, '112109': 0, '102912': 1, '105550': 1, '106888': 0, '108368': 0, '105278': 0, '107129': 0, '109092': 1, '108075': 0, '112502': 1, '108458': 1, '104122': 1, '100248': 1, '107418': 1, '104573': 0, '110435': 0, '104076': 0, '100580': 0, '103249': 0, '107921': 0, '111163': 0, '104534': 0, '111762': 1, '101005': 0, '112380': 0, '104189': 0, '111382': 1, '111360': 1, '111491': 1, '104634': 1, '100019': 0, '108508': 1, '101208': 0, '100406': 0, '109405': 0, '110400': 1, '110944': 1, '109839': 1, '109048': 1, '108306': 0, '106725': 1, '104821': 0, '108233': 0, '108501': 1, '112805': 1, '109357': 0, '101950': 1, '109081': 0, '104895': 0, '107504': 0, '105095': 1, '103218': 0, '109781': 0, '104440': 0, '102486': 1, '105292': 1, '109455': 0, '107573': 1, '108675': 1, '107912': 1, '101256': 1, '108732': 0, '105884': 1, '105742': 1, '110252': 0, '101160': 0, '102337': 1, '104059': 0, '104964': 0, '109072': 0, '107310': 0, '101555': 1, '112589': 0, '107524': 0, '105359': 1, '101799': 0, '106124': 1, '109952': 1, '103740': 1, '111591': 0, '106796': 1, '108702': 1, '102925': 1, '109750': 1, '101117': 0, '110173': 1, '104216': 0, '108034': 0, '111464': 1, '101690': 1, '102502': 0, '104017': 1, '104969': 0, '106563': 1, '102708': 0, '104315': 1, '107949': 1, '101783': 1, '112633': 1, '109523': 0, '101952': 1, '104913': 1, '106127': 0, '109918': 1, '103597': 1, '100626': 1, '110532': 1, '111904': 0, '104702': 0, '112522': 0, '103890': 1, '111598': 0, '101615': 0, '104742': 1, '102436': 1, '109227': 1, '106660': 1, '104631': 1, '101201': 0, '101200': 1, '100783': 1, '105728': 0, '104586': 1, '112391': 1, '100594': 1, '106204': 1, '109619': 0, '111640': 1, '110300': 1, '100675': 0, '105688': 1, '103735': 1, '110467': 0, '110687': 1, '107239': 1, '106822': 1, '100222': 0, '100932': 1, '112107': 1, '107176': 1, '110279': 0, '107633': 1, '100726': 1, '100441': 0, '107750': 1, '109585': 1, '109177': 1, '111735': 1, '110367': 1, '111448': 0, '105273': 1, '100521': 1, '107166': 1, '101999': 1, '107806': 1, '107204': 0, '111201': 1, '106867': 1, '102427': 0, '101807': 1, '101717': 1, '101798': 0, '111477': 0, '102644': 1, '102501': 0, '108035': 0, '111521': 1, '101537': 0, '108356': 1, '100286': 0, '107242': 0, '111714': 1, '105167': 0, '110132': 0, '111089': 1, '103455': 1, '108129': 1, '108659': 1, '105818': 0, '112405': 1, '107955': 1, '102699': 1, '109322': 0, '106530': 1, '103531': 0, '106449': 0, '108177': 1, '112243': 1, '111503': 1, '112610': 0, '109879': 0, '101320': 1, '105521': 1, '106854': 1, '103015': 0, '102806': 1, '111549': 1, '109175': 1, '106923': 0, '110101': 0, '105515': 1, '106881': 1, '108681': 1, '103213': 1, '112889': 1, '101859': 1, '109701': 1, '107681': 0, '106195': 0, '109674': 0, '103005': 0, '111013': 1, '106148': 1, '101946': 1, '107392': 0, '106944': 1, '111628': 0, '112535': 1, '102976': 0, '111976': 0, '108753': 1, '109078': 0, '111849': 0, '111126': 1, '103276': 0, '110392': 1, '112099': 1, '101135': 1, '106852': 0, '102954': 0, '104156': 1, '107929': 1, '106258': 1, '103923': 0, '102588': 1, '111871': 1, '107857': 0, '112687': 0, '103316': 0, '107245': 1, '109059': 1, '101446': 1, '110753': 0, '103126': 1, '110941': 0, '106300': 1, '104317': 0, '108102': 1, '100351': 0, '103974': 0, '112652': 0, '109992': 0, '104857': 1, '101420': 0, '108473': 1, '109149': 1, '106158': 1, '103237': 1, '110699': 0, '110134': 1, '106553': 0, '112863': 1, '103436': 0, '100403': 1, '101302': 0, '112354': 1, '105699': 1, '111914': 0, '110784': 0, '112817': 1, '101530': 0, '112654': 0, '105635': 1, '103037': 0, '109827': 1, '106476': 0, '110698': 1, '104222': 0, '103411': 0, '107340': 1, '106772': 0, '101879': 0, '107376': 0, '111034': 1, '103658': 1, '107832': 0, '101730': 0, '100984': 0, '109734': 1, '105251': 0, '107827': 0, '102762': 0, '104530': 1, '111892': 0, '112292': 1, '102390': 1, '102251': 1, '109602': 1, '104277': 0, '102924': 1, '111128': 1, '111889': 1, '109052': 0, '106299': 1, '101091': 1, '101192': 1, '106777': 0, '103679': 0, '103859': 1, '105864': 1, '103980': 1, '105640': 1, '100220': 1, '109535': 1, '101607': 0, '108843': 1, '105212': 1, '102066': 0, '110385': 1, '106535': 0, '106798': 1, '108764': 1, '111788': 1, '100193': 0, '106374': 1, '105917': 1, '101794': 0, '100871': 1, '103926': 1, '101460': 1, '101428': 0, '108092': 0, '109752': 1, '109468': 1, '110297': 0, '112894': 0, '107471': 1, '101848': 1, '103868': 0, '111250': 0, '109819': 0, '103278': 0, '110381': 1, '108983': 0, '105751': 1, '102225': 0, '104590': 0, '103987': 1, '102651': 0, '109613': 0, '102198': 1, '101362': 1, '101982': 0, '108752': 0, '101961': 1, '107444': 1, '101311': 1, '112046': 1, '107837': 1, '112964': 1, '102211': 1, '104558': 1, '111527': 1, '100542': 0, '105063': 0, '100775': 1, '100583': 0, '109617': 1, '109889': 1, '107020': 1, '111086': 0, '111779': 1, '105958': 0, '106072': 0, '106345': 1, '108396': 0, '104705': 1, '101547': 1, '103823': 1, '112585': 0, '104380': 1, '111148': 0, '104773': 1, '108589': 1, '101811': 0, '103091': 0, '104477': 1, '112525': 1, '105663': 1, '103515': 1, '111490': 1, '104174': 0, '102450': 0, '109444': 1, '100315': 1, '112664': 1, '110022': 1, '100062': 1, '101198': 1, '103676': 0, '108226': 0, '100250': 1, '100007': 1, '112684': 1, '105673': 1, '106954': 0, '103181': 0, '103404': 0, '112769': 1, '100075': 1, '110087': 1, '102463': 1, '103295': 0, '106797': 0, '111897': 1, '102790': 0, '112815': 1, '112012': 1, '101599': 1, '107398': 1, '104330': 0, '100781': 1, '105999': 0, '103207': 1, '108010': 1, '107464': 0, '102354': 1, '101182': 0, '111907': 1, '108339': 1, '104949': 0, '101336': 1, '105113': 0, '104422': 1, '109487': 0, '109290': 1, '105651': 1, '103060': 1, '108779': 1, '110096': 0, '109690': 0, '110062': 1, '108847': 1, '107521': 1, '104271': 1, '110103': 1, '108920': 0, '112640': 1, '105181': 0, '112857': 1, '101810': 0, '106719': 0, '103434': 1, '110608': 0, '104361': 0, '102482': 0, '102079': 1, '111291': 1, '108717': 1, '112553': 0, '110969': 0, '103225': 1, '111107': 1, '110610': 1, '110989': 0, '101317': 1, '103427': 0, '106909': 0, '108570': 1, '112521': 0, '110469': 0, '112786': 0, '107960': 1, '102618': 0, '112865': 0, '102331': 0, '112148': 0, '106467': 1, '102360': 0, '111390': 1, '109765': 1, '112116': 1, '111596': 1, '112145': 0, '111555': 1, '112542': 1, '110076': 0, '103236': 1, '113027': 0, '103227': 0, '102553': 1, '102082': 1, '105067': 1, '111804': 0, '107298': 1, '109753': 1, '101666': 1, '109427': 1, '106721': 1, '110681': 0, '103719': 1, '107553': 1, '105174': 0, '112551': 1, '105776': 1, '100535': 1, '100770': 0, '109681': 0, '104678': 0, '102408': 1, '105741': 1, '111035': 1, '111830': 0, '100959': 0, '110429': 0, '112475': 1, '109387': 1, '108480': 0, '106169': 0, '100265': 0, '101363': 1, '101270': 0, '109173': 0, '105088': 0, '111486': 1, '110327': 1, '106275': 1, '106837': 1, '105935': 0, '101828': 0, '109683': 0, '102752': 0, '103152': 0, '101727': 0, '109213': 1, '106201': 0, '112324': 0, '104595': 0, '105306': 0, '101601': 0, '103656': 1, '111551': 1, '107862': 0, '103745': 1, '111403': 0, '105017': 1, '100292': 1, '109002': 1, '112787': 1, '103541': 1, '100009': 1, '102033': 1, '100397': 1, '109845': 1, '109386': 1, '103716': 1, '100232': 1, '110121': 0, '100023': 0, '105260': 1, '105614': 1, '108604': 0, '108095': 1, '106188': 0, '106297': 1, '110465': 1, '101597': 1, '106278': 0, '112421': 1, '109079': 1, '111384': 0, '108797': 1, '108868': 0, '111879': 1, '107191': 0, '104181': 1, '106729': 0, '111181': 0, '106179': 1, '109449': 0, '112328': 1, '104651': 0, '112197': 0, '104959': 1, '100756': 0, '108038': 1, '101224': 1, '100348': 1, '102663': 0, '110706': 0, '111948': 0, '103899': 1, '100918': 0, '107683': 0, '104598': 1, '107841': 0, '112885': 1, '101103': 0, '104684': 1, '100213': 1, '106752': 0, '100891': 0, '110999': 0, '108922': 0, '108146': 1, '112074': 1, '102242': 0, '107842': 1, '110334': 1, '110727': 1, '107367': 1, '102064': 1, '101852': 1, '105570': 1, '112458': 0, '102647': 1, '104474': 1, '112953': 1, '106916': 0, '104802': 0, '108014': 1, '102946': 1, '103047': 0, '100845': 0, '104162': 1, '110050': 0, '111881': 0, '105792': 1, '112179': 0, '112212': 1, '102577': 0, '103071': 0, '104640': 0, '109001': 1, '104266': 0, '107371': 0, '111463': 0, '111029': 1, '104160': 1, '111660': 0, '112839': 0, '109653': 0, '108173': 1, '107530': 1, '103839': 0, '104208': 0, '101619': 0, '110854': 0, '112777': 0, '101654': 0, '110303': 0, '107414': 1, '104351': 1, '112115': 1, '107221': 1, '101833': 0, '108492': 0, '105197': 0, '101481': 0, '108854': 0, '107000': 1, '109482': 0, '107017': 1, '108891': 0, '112288': 1, '105527': 0, '106506': 0, '104599': 1, '107374': 1, '103834': 1, '112911': 0, '101849': 1, '103400': 0, '109537': 1, '104092': 1, '108941': 1, '112590': 1, '110122': 1, '108059': 1, '109039': 1, '103506': 1, '104956': 1, '102601': 0, '101755': 0, '108811': 1, '109655': 1, '104805': 1, '105010': 1, '101325': 1, '107810': 1, '107189': 1, '107698': 0, '102928': 1, '107372': 0, '102407': 1, '112305': 1, '103377': 0, '101529': 0, '109090': 1, '112947': 0, '109964': 1, '112679': 1, '110925': 1, '101015': 1, '112265': 1, '110317': 1, '112709': 0, '100608': 1, '111197': 0, '107320': 0, '110246': 1, '109251': 0, '103452': 0, '108441': 0, '110080': 0, '112870': 0, '104716': 1, '111441': 1, '110688': 1, '110495': 0, '100096': 0, '100989': 1, '103633': 0, '106357': 0, '107657': 1, '100725': 0, '107210': 0, '104521': 0, '111791': 1, '102376': 1, '108827': 0, '106084': 1, '107796': 0, '106640': 0, '102951': 1, '104682': 0, '107386': 0, '108213': 1, '101443': 1, '112268': 1, '104639': 1, '100584': 1, '107037': 1, '109426': 0, '104409': 0, '107566': 0, '101732': 1, '101413': 1, '109176': 0, '102352': 0, '107481': 1, '109239': 0, '100123': 0, '110375': 0, '104721': 0, '106483': 1, '110535': 1, '109272': 0, '102947': 0, '105432': 1, '102336': 1, '110029': 1, '111675': 0, '104215': 1, '100511': 1, '100729': 1, '106561': 0, '102419': 0, '103111': 1, '105586': 0, '107171': 1, '101756': 1, '105820': 1, '108105': 1, '101831': 1, '106830': 0, '105901': 1, '108063': 0, '108086': 1, '109931': 0, '101744': 0, '112516': 1, '112959': 0, '103124': 1, '109296': 1, '101012': 1, '108865': 1, '108411': 1, '112832': 0, '105110': 1, '110475': 0, '105207': 1, '110542': 0, '107224': 1, '108446': 1, '103001': 1, '109024': 1, '110166': 0, '100517': 1, '108635': 1, '110756': 1, '109775': 1, '110837': 0, '110649': 0, '108559': 0, '104519': 1, '106303': 0, '109123': 1, '100632': 1, '107250': 1, '102062': 0, '106427': 1, '103979': 0, '100141': 1, '100956': 0, '107557': 1, '112136': 1, '104414': 1, '109398': 0, '105313': 1, '107732': 1, '100518': 1, '109664': 1, '107348': 0, '110872': 1, '110193': 1, '103062': 1, '104095': 1, '100839': 1, '100679': 0, '103808': 1, '104689': 1, '102205': 0, '105716': 0, '109008': 1, '100912': 1, '102233': 0, '111171': 0, '104952': 1, '101036': 1, '103840': 1, '104688': 1, '102600': 1, '102998': 0, '100837': 0, '105423': 0, '110731': 1, '110032': 1, '104738': 1, '100648': 1, '106583': 1, '103628': 0, '108799': 0, '109243': 0, '106111': 1, '112632': 1, '105602': 1, '101969': 0, '102089': 0, '110461': 0, '100432': 1, '101802': 0, '110000': 0, '100610': 0, '109549': 1, '108252': 1, '110505': 1, '101316': 0, '112287': 1, '101720': 0, '102073': 0, '111337': 1, '103824': 0, '104993': 1, '107356': 1, '105052': 1, '112466': 0, '106298': 0, '112188': 1, '112286': 1, '112878': 1, '104822': 1, '100667': 0, '101283': 1, '109004': 1, '103185': 0, '105290': 1, '111799': 0, '109023': 0, '110664': 0, '103453': 1, '108946': 1, '108515': 0, '104425': 0, '100428': 1, '111615': 0, '104515': 1, '110060': 1, '107337': 1, '103421': 0, '101150': 1, '109985': 1, '106214': 0, '107213': 1, '103648': 1, '107335': 0, '107285': 0, '102311': 1, '103511': 0, '103315': 1, '101252': 1, '101110': 1, '106090': 0, '110466': 0, '110306': 0, '102019': 1, '106053': 1, '108093': 0, '100864': 0, '107956': 1, '109979': 1, '107637': 1, '110887': 1, '109621': 1, '105127': 0, '111417': 1, '108725': 1, '100748': 0, '100037': 1, '112432': 1, '108765': 0, '103330': 1, '108413': 0, '111854': 1, '109714': 0, '102356': 0, '106631': 0, '112716': 0, '108484': 0, '103220': 0, '102286': 1, '111589': 1, '105280': 0, '109998': 0, '109689': 0, '110621': 1, '111558': 1, '104341': 1, '110486': 1, '103723': 0, '101119': 1, '100314': 0, '107697': 1, '107064': 0, '105736': 0, '108111': 0, '111811': 1, '107642': 1, '103492': 0, '108359': 0, '108652': 1, '104547': 1, '108981': 0, '106720': 1, '102515': 0, '106045': 1, '101647': 1, '101450': 0, '100223': 1, '102815': 1, '110091': 1, '109981': 1, '103444': 1, '109676': 1, '107754': 1, '108616': 0, '104654': 1, '103827': 1, '111388': 1, '106548': 1, '106645': 1, '109542': 0, '108669': 0, '105156': 0, '101975': 1, '108747': 0, '104373': 1, '108999': 1, '105475': 1, '100761': 1, '102629': 0, '110098': 1, '110150': 1, '109634': 0, '103755': 1, '104793': 0, '100106': 1, '100657': 0, '101883': 1, '102038': 1, '110973': 1, '101919': 1, '110369': 0, '101068': 1, '100552': 1, '101390': 1, '110370': 1, '107055': 1, '112973': 0, '112622': 1, '102447': 1, '104618': 1, '107685': 1, '111212': 0, '109777': 1, '104018': 1, '110978': 1, '103208': 1, '106829': 1, '110207': 1, '102317': 1, '110738': 0, '100101': 1, '104484': 1, '110139': 0, '103920': 1, '112568': 0, '102902': 1, '105824': 0, '107572': 0, '102504': 1, '111724': 1, '108524': 1, '107815': 1, '110915': 0, '111395': 0, '105811': 1, '109922': 1, '102668': 1, '104016': 1, '102039': 0, '100576': 0, '111233': 0, '101571': 1, '111351': 0, '108862': 1, '105859': 0, '107302': 1, '106508': 1, '110950': 1, '110661': 1, '102429': 1, '111383': 0, '104896': 0, '103024': 1, '110135': 1, '106902': 1, '104707': 0, '102639': 0, '113002': 1, '102107': 0, '110271': 1, '107505': 1, '105448': 1, '103667': 1, '112661': 1, '105148': 1, '102851': 1, '110278': 1, '112462': 0, '108701': 1, '108859': 0, '112348': 0, '104389': 1, '112598': 1, '112105': 1, '108457': 0, '112468': 0, '104535': 1, '110928': 1, '105402': 1, '109498': 0, '112529': 0, '101347': 1, '106526': 1, '102657': 1, '106245': 1, '101559': 0, '109014': 0, '101333': 1, '104946': 1, '104433': 1, '107220': 1, '105916': 1, '112967': 1, '101063': 0, '103415': 1, '108282': 1, '105245': 1, '106029': 0, '108025': 1, '106266': 0, '104428': 0, '105269': 0, '111658': 1, '110254': 1, '107355': 0, '102773': 1, '112171': 0, '108415': 1, '101220': 1, '109237': 1, '102161': 0, '110141': 1, '107692': 1, '111929': 0, '106847': 0, '100654': 1, '109198': 0, '105945': 1, '100803': 0, '105227': 0, '107610': 1, '102259': 1, '101885': 1, '104772': 1, '110311': 1, '105329': 0, '105133': 1, '109464': 0, '109157': 0, '102896': 1, '102394': 0, '102328': 0, '104323': 1, '100203': 1, '101787': 1, '109624': 1, '102385': 0, '106557': 0, '100842': 1, '108593': 1, '105548': 0, '104362': 1, '111965': 0, '100673': 0, '111285': 0, '101028': 1, '108449': 1, '105169': 1, '111225': 1, '102736': 1, '108294': 1, '101580': 1, '112337': 1, '101306': 0, '102130': 1, '108210': 0, '107483': 0, '110421': 1, '101382': 0, '112368': 1, '101191': 0, '102197': 1, '112100': 1, '103435': 0, '106459': 0, '100424': 1, '111801': 0, '105922': 1, '103600': 1, '102615': 0, '103272': 0, '102069': 1, '107469': 1, '110219': 1, '109314': 1, '107799': 1, '109547': 0, '105386': 0, '101516': 0, '103793': 1, '112851': 1, '103791': 1, '109513': 1, '106904': 1, '105476': 1, '111466': 1, '101061': 0, '111444': 0, '109638': 0, '104791': 1, '108462': 0, '100242': 1, '100762': 0, '100024': 0, '100149': 0, '100435': 0, '101895': 1, '109920': 1, '111566': 0, '111173': 0, '104031': 1, '108130': 1, '108198': 0, '102490': 1, '102531': 1, '108114': 0, '108910': 0, '109107': 1, '100175': 1, '108704': 1, '105563': 1, '106969': 0, '103245': 1, '103854': 0, '104862': 1, '110102': 0, '107742': 1, '100494': 0, '100026': 1, '104741': 0, '107461': 0, '106485': 0, '112770': 0, '105315': 1, '106256': 0, '106987': 0, '107050': 0, '108137': 0, '104102': 1, '104432': 1, '109894': 1, '105338': 0, '110690': 0, '106659': 1, '109207': 1, '106511': 0, '103634': 0, '106499': 1, '106771': 1, '104481': 1, '100745': 1, '106108': 0, '109225': 1, '111346': 1, '102649': 1, '105385': 0, '106136': 0, '104831': 1, '101285': 0, '101742': 0, '105770': 1, '100071': 1, '111105': 0, '109505': 1, '104622': 0, '107434': 0, '105721': 0, '100053': 0, '109991': 1, '107886': 0, '107552': 0, '107707': 1, '109897': 0, '101492': 1, '110414': 0, '101071': 0, '106696': 0, '110991': 0, '109046': 1, '105363': 1, '109733': 1, '103994': 0, '107775': 1, '103112': 0, '104864': 1, '112106': 0, '100621': 1, '110001': 0, '108655': 1, '104871': 0, '106600': 0, '107063': 0, '106028': 0, '100505': 1, '109308': 0, '103344': 1, '111777': 1, '112239': 1, '112774': 0, '111459': 1, '102196': 0, '112549': 0, '111117': 1, '102232': 1, '109462': 0, '101693': 1, '109829': 0, '110772': 1, '112920': 1, '107991': 1, '103727': 1, '100312': 0, '106047': 0, '105578': 1, '111755': 1, '111494': 1, '105969': 1, '103135': 0, '102802': 1, '100226': 1, '100879': 0, '106606': 0, '102156': 0, '107327': 0, '103147': 1, '101082': 0, '111835': 0, '108081': 1, '105115': 1, '104816': 1, '111296': 1, '106955': 0, '101076': 1, '105889': 0, '100502': 1, '111043': 0, '102838': 1, '100644': 1, '107005': 1, '112600': 1, '112102': 0, '109453': 1, '107170': 1, '101976': 0, '100207': 1, '101014': 1, '104567': 1, '102368': 1, '102008': 1, '110877': 0, '110537': 1, '106091': 0, '103449': 1, '109789': 0, '111175': 1, '110801': 1, '101184': 0, '100310': 1, '106399': 1, '108329': 0, '105493': 1, '112700': 1, '101244': 0, '101673': 1, '107734': 1, '107655': 1, '107136': 0, '104840': 1, '103176': 1, '110809': 1, '107936': 0, '104951': 0, '101470': 1, '105211': 1, '110689': 1, '105504': 1, '100486': 1, '112612': 1, '109818': 0, '101209': 1, '103182': 0, '106382': 1, '101189': 1, '111110': 0, '107539': 0, '110536': 0, '103068': 1, '111930': 1, '107979': 0, '111934': 0, '107844': 1, '106636': 1, '111646': 1, '112467': 1, '106398': 0, '105229': 1, '102811': 1, '111740': 0, '102475': 1, '102435': 1, '100266': 0, '108893': 1, '103620': 1, '100306': 1, '109771': 1, '107149': 0, '101429': 1, '102822': 0, '101570': 0, '112513': 0, '112822': 1, '101265': 1, '101583': 1, '107816': 0, '109626': 0, '108275': 1, '112090': 0, '110931': 0, '100221': 1, '102666': 1, '101178': 0, '109593': 0, '107787': 0, '106329': 1, '101253': 0, '111728': 0, '111593': 1, '101070': 0, '102216': 1, '100797': 0, '108966': 1, '106543': 1, '100682': 1, '100084': 0, '112657': 1, '110800': 1, '102605': 0, '104977': 0, '105020': 1, '103844': 1, '100816': 0, '100571': 1, '109263': 0, '105980': 0, '106174': 1, '107911': 0, '111559': 0, '110820': 0, '104006': 0, '100367': 1, '100346': 0, '101606': 1, '101501': 0, '111873': 1, '102307': 1, '101458': 1, '104552': 0, '102200': 1, '112867': 1, '100032': 1, '103306': 1, '110613': 1, '110402': 1, '104916': 0, '105918': 1, '103778': 1, '112473': 0, '109962': 0, '108064': 0, '105788': 1, '112162': 0, '111418': 1, '111577': 0, '108163': 0, '109700': 0, '110575': 0, '101586': 0, '111183': 0, '101826': 1, '108367': 0, '103361': 0, '104212': 1, '103902': 0, '105495': 0, '100138': 0, '102745': 0, '100474': 1, '104057': 0, '104325': 0, '103592': 1, '105328': 1, '100740': 0, '105139': 1, '107599': 0, '107590': 0, '105000': 0, '106716': 0, '108744': 0, '105874': 1, '105547': 0, '103787': 0, '101328': 0, '111864': 1, '108638': 0, '108992': 0, '108203': 0, '102127': 0, '108373': 1, '110406': 1, '112080': 0, '107452': 1, '105446': 1, '103935': 0, '107357': 0, '105685': 1, '110363': 1, '102203': 0, '103955': 1, '105790': 1, '100093': 0, '101980': 1, '100404': 0, '102878': 0, '103804': 1, '106792': 1, '100844': 1, '111152': 1, '110622': 0, '103984': 1, '109309': 0, '107029': 1, '103872': 0, '107567': 1, '107425': 1, '103049': 1, '105048': 1, '102832': 1, '103683': 1, '104930': 1, '106597': 1, '100489': 1, '108613': 1, '102346': 1, '105911': 0, '107382': 1, '105123': 1, '109499': 1, '112327': 1, '111164': 0, '111629': 1, '108721': 1, '110685': 0, '105844': 1, '111815': 0, '107077': 0, '105924': 0, '103726': 1, '107152': 1, '107091': 0, '110245': 1, '110446': 1, '104675': 1, '106594': 1, '110697': 0, '112611': 0, '104153': 1, '101078': 1, '107494': 0, '107044': 1, '103438': 1, '102000': 1, '103177': 1, '103210': 0, '101773': 0, '102854': 1, '109451': 0, '104728': 1, '104462': 1, '107128': 0, '110804': 0, '103812': 1, '109305': 0, '104957': 1, '110522': 1, '102941': 1, '110582': 1, '106472': 0, '110040': 1, '107430': 1, '111695': 0, '102972': 1, '109953': 1, '103605': 1, '111994': 1, '100274': 0, '104360': 1, '107920': 1, '106949': 0, '110570': 1, '102132': 1, '105523': 1, '100660': 1, '100669': 1, '108113': 1, '110647': 1, '110342': 1, '112465': 1, '107957': 0, '107740': 1, '100513': 0, '108349': 1, '105984': 1, '104041': 0, '101840': 1, '111633': 0, '108309': 0, '110920': 1, '110541': 0, '109652': 1, '104029': 0, '108423': 1, '100941': 0, '100128': 0, '108530': 0, '100562': 0, '109848': 1, '102500': 1, '100080': 1, '103080': 0, '111240': 1, '103770': 0, '109659': 1, '103801': 0, '111865': 0, '104213': 0, '112800': 1, '104915': 0, '102186': 0, '101108': 1, '104649': 1, '104023': 0, '104198': 0, '107164': 1, '104983': 1, '102454': 1, '108212': 1, '103621': 1, '111501': 0, '109415': 0, '105510': 1, '100392': 1, '109670': 0, '110768': 0, '109085': 0, '102986': 0, '111267': 1, '103414': 1, '100641': 0, '112068': 0, '109163': 1, '103228': 0, '103342': 0, '100515': 0, '104482': 0, '111318': 1, '100662': 1, '103674': 0, '112557': 0, '113032': 1, '107151': 0, '102199': 1, '104089': 1, '107122': 1, '110670': 0, '111758': 1, '107139': 0, '108521': 1, '112364': 1, '106683': 1, '101391': 1, '108769': 1, '100219': 1, '102182': 0, '108310': 0, '101804': 1, '104606': 0, '107664': 0, '106992': 1, '107441': 0, '102638': 1, '104850': 0, '111330': 1, '101366': 0, '111478': 0, '111266': 0, '107569': 0, '109310': 1, '101155': 0, '112196': 1, '108412': 0, '110930': 0, '112139': 1, '106735': 1, '112078': 1, '100279': 0, '105623': 1, '106241': 0, '111780': 1, '112971': 1, '109928': 1, '112358': 0, '104892': 1, '105391': 1, '101346': 1, '110384': 0, '107793': 1, '109637': 1, '103540': 0, '108543': 1, '106943': 0, '110201': 1, '112933': 0, '112804': 1, '104201': 0, '102548': 1, '110157': 1, '107581': 1, '107016': 0, '102133': 0, '101069': 0, '106811': 1, '105431': 1, '101990': 1, '110186': 1, '107183': 0, '111776': 0, '110116': 0, '110838': 0, '109369': 0, '110668': 0, '104446': 1, '111404': 0, '112744': 1, '102952': 1, '107427': 1, '108098': 1, '103200': 1, '100158': 1, '101245': 0, '105576': 0, '103784': 1, '111018': 1, '112319': 1, '110391': 0, '112625': 1, '112126': 1, '104104': 1, '110715': 1, '107182': 0, '106013': 0, '103907': 0, '100743': 1, '100327': 1, '108048': 1, '111060': 1, '101104': 0, '110994': 1, '104623': 1, '100539': 0, '104338': 1, '103758': 1, '108431': 1, '104155': 1, '106092': 0, '108301': 1, '108045': 0, '108249': 1, '112675': 0, '109995': 1, '112299': 1, '101777': 1, '107417': 0, '100276': 0, '111607': 1, '109500': 1, '103297': 1, '104078': 1, '111807': 0, '107548': 1, '100168': 0, '103783': 1, '102301': 1, '112195': 0, '104379': 1, '100381': 0, '110142': 1, '100446': 1, '108324': 1, '104511': 1, '103313': 1, '102410': 0, '108099': 0, '109485': 1, '110099': 0, '101427': 1, '107849': 0, '112323': 0, '102357': 1, '102431': 1, '101259': 1, '100150': 1, '104121': 1, '102581': 0, '111996': 1, '102295': 0, '103166': 0, '103271': 1, '101393': 0, '105307': 1, '103441': 1, '101484': 1, '110015': 0, '103426': 0, '109029': 0, '105587': 1, '107223': 0, '112052': 1, '102192': 1, '105301': 0, '111771': 1, '106615': 1, '108740': 1, '107288': 1, '100950': 0, '107826': 0, '111874': 0, '108377': 1, '110378': 1, '107474': 1, '111415': 0, '107115': 0, '110856': 1, '103136': 1, '106655': 1, '103397': 1, '110958': 1, '100613': 0, '100713': 1, '108828': 1, '103134': 1, '105120': 1, '105789': 0, '102814': 1, '100921': 1, '101109': 0, '103855': 0, '105268': 1, '102013': 1, '105886': 0, '102858': 1, '101422': 1, '112404': 1, '108493': 1, '103487': 1, '104234': 0, '107485': 1, '100533': 0, '101651': 1, '108403': 1, '106055': 1, '105361': 1, '100605': 1, '105926': 1, '111053': 1, '101262': 0, '110709': 0, '104776': 1, '105064': 0, '109779': 1, '105477': 0, '100249': 1, '105858': 0, '104232': 1, '104642': 0, '102650': 0, '100855': 1, '105661': 1, '108602': 1, '100878': 1, '111635': 1, '104934': 0, '100899': 1, '104453': 0, '105761': 0, '105186': 1, '111332': 1, '107385': 1, '107105': 0, '106504': 1, '108159': 0, '111493': 1, '107864': 0, '111880': 1, '109076': 1, '100672': 0, '105217': 0, '104708': 0, '104412': 0, '102658': 0, '107919': 1, '108135': 1, '108566': 0, '103360': 0, '112365': 1, '112511': 1, '110472': 0, '102380': 1, '106087': 0, '107256': 1, '101046': 1, '110792': 1, '104961': 1, '105906': 1, '104902': 1, '105764': 0, '108948': 1, '106778': 0, '112579': 1, '102006': 0, '107014': 1, '102927': 1, '106574': 1, '109056': 0, '112273': 1, '103905': 0, '112766': 1, '102898': 0, '101891': 1, '103752': 1, '102547': 1, '102292': 1, '107061': 0, '108371': 1, '102585': 1, '109467': 1, '102754': 0, '111872': 1, '105739': 1, '111442': 0, '110129': 0, '101019': 1, '101625': 1, '104580': 1, '109128': 1, '103561': 1, '112234': 1, '109766': 1, '103697': 0, '112721': 0, '112149': 0, '102070': 0, '102636': 1, '112849': 1, '110884': 0, '105040': 1, '111508': 1, '103259': 0, '101892': 1, '110817': 1, '100751': 1, '110624': 0, '105524': 0, '100127': 1, '103574': 1, '103066': 0, '100971': 0, '100708': 1, '109277': 0, '111038': 0, '100370': 0, '109278': 0, '107225': 0, '102685': 1, '102837': 1, '107771': 0, '105633': 0, '101541': 1, '109660': 1, '106908': 0, '107178': 0, '103941': 0, '100822': 1, '101364': 1, '112199': 1, '104030': 0, '107561': 0, '110090': 0, '102313': 1, '101626': 0, '106628': 0, '112030': 1, '101471': 0, '105910': 0, '109254': 1, '107053': 0, '100861': 1, '112430': 1, '107197': 1, '110657': 0, '110788': 0, '112701': 0, '103829': 0, '100691': 1, '109210': 1, '103764': 1, '101297': 0, '111972': 1, '104269': 0, '108733': 0, '111411': 1, '105942': 0, '112489': 1, '103425': 1, '112312': 1, '108605': 1, '109739': 0, '108079': 1, '105832': 0, '104137': 0, '110366': 0, '108199': 0, '105802': 1, '111012': 0, '106232': 1, '110292': 1, '101350': 0, '110745': 0, '111923': 1, '101157': 1, '103119': 1, '104718': 0, '105825': 1, '107040': 1, '109117': 0, '104118': 1, '106365': 1, '105443': 1, '111859': 1, '100588': 1, '109561': 1, '101372': 0, '102565': 1, '110203': 1, '111074': 1, '112345': 0, '111423': 1, '108776': 1, '105225': 1, '105265': 0, '105214': 1, '107448': 0, '105164': 0, '107533': 1, '109788': 1, '111497': 0, '108519': 1, '107751': 1, '105686': 1, '105620': 1, '112308': 1, '105960': 1, '109348': 1, '112020': 0, '110890': 1, '107856': 1, '100914': 1, '100825': 1, '107206': 0, '109924': 0, '112923': 1, '103401': 1, '109810': 1, '112495': 1, '109834': 0, '100981': 0, '102110': 1, '106856': 0, '101963': 0, '103925': 1, '104804': 0, '100343': 1, '106652': 1, '111532': 0, '103978': 1, '107253': 1, '101237': 0, '100856': 0, '106002': 0, '103019': 1, '104743': 1, '103041': 1, '103461': 0, '104911': 0, '109963': 1, '100104': 0, '110645': 1, '105003': 0, '104158': 1, '111209': 1, '102702': 1, '110859': 1, '106917': 1, '101017': 1, '108115': 1, '104069': 0, '106456': 1, '110603': 0, '111094': 1, '109569': 0, '103976': 1, '112083': 1, '102774': 1, '100674': 1, '113017': 1, '110221': 0, '103268': 1, '103713': 1, '103327': 1, '111377': 1, '112218': 0, '100994': 0, '109557': 0, '109209': 0, '108692': 1, '107304': 1, '100522': 0, '111884': 0, '100952': 1, '102144': 1, '105932': 0, '102383': 0, '107625': 0, '107324': 1, '103348': 1, '102401': 0, '101546': 0, '101125': 1, '104286': 0, '109987': 0, '112452': 1, '107871': 0, '100303': 1, '112381': 1, '111447': 1, '107439': 0, '102170': 1, '112915': 0, '111639': 0, '100827': 1, '106839': 1, '102813': 1, '102890': 0, '106759': 0, '111355': 0, '102329': 0, '105293': 0, '101551': 0, '108504': 0, '110241': 0, '105288': 1, '106021': 1, '108379': 1, '110588': 1, '105422': 0, '103277': 0, '109150': 0, '104819': 1, '112636': 1, '110845': 0, '110528': 1, '112692': 1, '102656': 1, '105033': 0, '110981': 0, '100409': 0, '102820': 0, '101901': 0, '102058': 1, '109773': 0, '111236': 0, '102756': 0, '110456': 1, '108798': 0, '109242': 0, '112131': 0, '101419': 1, '102626': 0, '110559': 0, '110614': 0, '104002': 1, '101324': 1, '108141': 1, '105531': 1, '103636': 1, '103996': 1, '106726': 0, '107678': 1, '105553': 1, '110492': 0, '102825': 1, '100054': 0, '103911': 0, '108323': 1, '101569': 1, '109694': 0, '100965': 1, '104297': 0, '103206': 1, '109989': 0, '107549': 1, '102021': 1, '108538': 0, '101396': 0, '112742': 1, '111618': 1, '109635': 1, '107028': 1, '106462': 0, '103981': 1, '106378': 0, '108472': 0, '106956': 1, '105104': 1, '108645': 0, '103951': 1, '105575': 1, '108164': 1, '104020': 1, '110599': 1, '102514': 0, '110065': 0, '106032': 0, '106965': 1, '113012': 1, '106150': 0, '109745': 1, '110309': 0, '110220': 0, '106097': 0, '100109': 1, '110825': 1, '106025': 0, '109503': 1, '102620': 0, '103318': 0, '109289': 0, '108313': 1, '108448': 0, '105180': 1, '102893': 0, '107292': 1, '106968': 0, '104703': 0, '111316': 1, '111986': 1, '107623': 1, '109122': 1, '103917': 1, '105035': 1, '109564': 0, '111895': 0, '108314': 0, '108260': 0, '107603': 0, '107865': 1, '104758': 0, '112724': 1, '109328': 1, '110341': 1, '104049': 1, '104308': 1, '103429': 1, '103098': 1, '107046': 1, '108263': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7psEQSl5t8Tn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "aebec159-74e9-41e2-cc70-a5d7f47477d6"
      },
      "source": [
        "arr_test=np.zeros((len(test_ids),  2), dtype=np.int64)\n",
        "for i in range(len(test_data.keys())):\n",
        "  arr_test[i, 0]=np.array(test_ids[i], dtype=np.int64)\n",
        "  arr_test[i, 1]=np.array(pred_test_args[i], dtype=np.int64)\n",
        "print(arr_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[110499      1]\n",
            " [101319      0]\n",
            " [108883      1]\n",
            " ...\n",
            " [103098      1]\n",
            " [107046      1]\n",
            " [108263      1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeGNskJRvX6n"
      },
      "source": [
        "np.savetxt(\"predictii.csv\", arr_test, delimiter=\",\", header='id,label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Loc8ev4jGRr"
      },
      "source": [
        "# Ahitectura: Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpQrOi3KjOQT"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size: int, embedding_size: int, \n",
        "                 rnn_size: int):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.rnn_size = rnn_size\n",
        "        \n",
        "        self.embedding=nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size)\n",
        "        self.rnn_cell = nn.GRUCell(input_size = embedding_size,\n",
        "                                   hidden_size = rnn_size)\n",
        "        self.logits = nn.Linear(in_features=rnn_size, out_features=vocab_size)\n",
        "        self.softmax = nn.Softmax(dim = 2)\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "    \n",
        "    def get_loss(self, logits: torch.FloatTensor, y: torch.FloatTensor):\n",
        "        return self.loss(logits.view(-1,logits.shape[-1]), y.squeeze())\n",
        "        \n",
        "    \n",
        "    def get_logits(self, hidden_states: torch.FloatTensor):\n",
        "        return self.logits(hidden_states)\n",
        "        \n",
        "    def forward(self, text_vectors_raw: torch.LongTensor, \n",
        "                hidden_start: torch.FloatTensor = None) -> torch.FloatTensor:\n",
        "        #batch_size x trim_len x embedding_dim\n",
        "        text_vectors=self.embedding(text_vectors_raw)\n",
        "        # print(f'embedded shape: {text_vectors.shape}')\n",
        "        #compute hidden states and logits for each time step\n",
        "        hidden_states_list = []\n",
        "        prev_hidden = hidden_start\n",
        "        # print(f'previous hidden shape {prev_hidden.shape}')\n",
        "        for t in range(text_vectors.shape[0]):\n",
        "            hidden_state = self.rnn_cell(text_vectors[t,:], prev_hidden) #the t-th word of processed text i.e. t-th word vector\n",
        "            hidden_states_list.append(hidden_state)\n",
        "            prev_hidden = hidden_state\n",
        "        \n",
        "        #one element of hidden_states_list has shape batch_size x max_len x embedding_dim\n",
        "        hidden_states = torch.stack(hidden_states_list, dim=1)\n",
        "        # print(f'final hidden states shape: {hidden_states.shape} \\n\\n')\n",
        "        return hidden_states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AqmppFCvoDy"
      },
      "source": [
        "rnn_size = 50\n",
        "vocab_size = len(cv.vocabulary_)\n",
        "embedding_size = 100\n",
        "max_grad_norm = 5.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbmCHoHs_G_R"
      },
      "source": [
        "def make_text_features(text, word_count_model):\n",
        "    \"\"\"\n",
        "    functia ia cuvintele textului primit si le trece pe fiecare prin wordcounter\n",
        "    apoi creeaza un tensor de dimensiune [nr_cuvinte_din_text x 1]\n",
        "    label-urile returnate sunt un tensor de dimensiune [nr_cuvinte_din_text x nr_cuvinte_din_vocabular]\n",
        "    tensorul de returnate contine pe fiecare linie un vector de dimensiunea vocabularului cu 1 in pozitia cuvantului si 0 in rest\n",
        "    \"\"\"\n",
        "    text_words_short=torch.LongTensor( len(text), 1)\n",
        "    text_actual_words=torch.Tensor(len(text), len(word_count_model.vocabulary_))\n",
        "    for word_no, word in enumerate(text):\n",
        "      wcount_emb=torch.tensor(word_count_model.transform([word]).todense())\n",
        "      text_words_short[word_no]=torch.argmax(wcount_emb).type(torch.LongTensor)\n",
        "      # text_actual_words[word_no]=torch.tensor(word_count_model.transform([word]).todense())\n",
        "    return text_words_short"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1QeZ-uD8yyG"
      },
      "source": [
        "def train_rnn_epoch(model, optimizer, textlist, word_count_model, max_grad_norm, epoch_num: int) -> float:\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        for text_idx in range(len(textlist)):\n",
        "            hidden_start = torch.zeros(1, model.rnn_size).to(device)\n",
        "            text_words_indexes = make_text_features(textlist[text_idx], word_count_model)\n",
        "            optimizer.zero_grad()\n",
        "            hidden_states = model(text_words_indexes, hidden_start)\n",
        "            #rezultatele pentru fiecare bucata de text din batch\n",
        "            logits = model.get_logits(hidden_states)\n",
        "            text_loss = model.get_loss(logits, text_words_indexes)\n",
        "            text_loss += text_loss.item()\n",
        "                       \n",
        "            text_loss.backward()\n",
        "            \n",
        "            # reducem gradientii la o constanta daca sunt prea mari\n",
        "            torch.nn.utils.clip_grad_norm_(list(model.parameters()), \n",
        "                                           max_grad_norm)\n",
        "            \n",
        "            optimizer.step()\n",
        "            \n",
        "\n",
        "            if text_idx % 30 == 0:\n",
        "                print(f'epoch {epoch_num}, text {text_idx} has batch loss = {text_loss.item()}')\n",
        "        epoch_loss /= text_idx\n",
        "        \n",
        "        return epoch_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAUDpc00OTip",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c21d197f-d0bc-4c93-80f1-08e3c3070bcb"
      },
      "source": [
        "a=make_text_features(train_textlist[0], cv)\n",
        "print(a.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([58, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVTUCQk7vBUO"
      },
      "source": [
        "network = RNN(vocab_size=vocab_size,embedding_size=embedding_size, rnn_size=rnn_size)\n",
        "network = network.to(device)\n",
        "optimizer = torch.optim.Adam(params = network.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKhWGmSIW9OO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8ae29d8f-756d-40df-82ba-33b6410db9fc"
      },
      "source": [
        "no_epochs=30\n",
        "for e in range(1, no_epochs):\n",
        "  train_rnn_epoch(model=network, optimizer=optimizer, textlist=train_textlist, word_count_model=cv, max_grad_norm=max_grad_norm, epoch_num=e)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1, text 0 has batch loss = 20.89650535583496\n",
            "epoch 1, text 100 has batch loss = 20.53720474243164\n",
            "epoch 1, text 200 has batch loss = 20.449190139770508\n",
            "epoch 1, text 300 has batch loss = 17.05722427368164\n",
            "epoch 1, text 400 has batch loss = 20.598848342895508\n",
            "epoch 1, text 500 has batch loss = 17.35858154296875\n",
            "epoch 1, text 600 has batch loss = 17.56346321105957\n",
            "epoch 1, text 700 has batch loss = 16.88450813293457\n",
            "epoch 1, text 800 has batch loss = 18.41565704345703\n",
            "epoch 1, text 900 has batch loss = 16.94827651977539\n",
            "epoch 1, text 1000 has batch loss = 15.07629108428955\n",
            "epoch 1, text 1100 has batch loss = 15.285298347473145\n",
            "epoch 1, text 1200 has batch loss = 14.341145515441895\n",
            "epoch 1, text 1300 has batch loss = 12.853402137756348\n",
            "epoch 1, text 1400 has batch loss = 15.776110649108887\n",
            "epoch 1, text 1500 has batch loss = 14.27728271484375\n",
            "epoch 1, text 1600 has batch loss = 13.286785125732422\n",
            "epoch 1, text 1700 has batch loss = 13.100395202636719\n",
            "epoch 1, text 1800 has batch loss = 14.574934005737305\n",
            "epoch 1, text 1900 has batch loss = 11.814085960388184\n",
            "epoch 1, text 2000 has batch loss = 14.388253211975098\n",
            "epoch 1, text 2100 has batch loss = 10.440366744995117\n",
            "epoch 1, text 2200 has batch loss = 14.607208251953125\n",
            "epoch 1, text 2300 has batch loss = 11.259299278259277\n",
            "epoch 1, text 2400 has batch loss = 14.400842666625977\n",
            "epoch 1, text 2500 has batch loss = 11.502140998840332\n",
            "epoch 1, text 2600 has batch loss = 10.071844100952148\n",
            "epoch 1, text 2700 has batch loss = 9.285207748413086\n",
            "epoch 1, text 2800 has batch loss = 6.973327159881592\n",
            "epoch 1, text 2900 has batch loss = 10.850218772888184\n",
            "epoch 1, text 3000 has batch loss = 10.204875946044922\n",
            "epoch 1, text 3100 has batch loss = 8.265103340148926\n",
            "epoch 1, text 3200 has batch loss = 13.765363693237305\n",
            "epoch 1, text 3300 has batch loss = 5.414170742034912\n",
            "epoch 1, text 3400 has batch loss = 6.925825595855713\n",
            "epoch 1, text 3500 has batch loss = 7.736629962921143\n",
            "epoch 1, text 3600 has batch loss = 9.405031204223633\n",
            "epoch 1, text 3700 has batch loss = 7.414373874664307\n",
            "epoch 1, text 3800 has batch loss = 8.623140335083008\n",
            "epoch 1, text 3900 has batch loss = 8.72674560546875\n",
            "epoch 1, text 4000 has batch loss = 5.3289313316345215\n",
            "epoch 1, text 4100 has batch loss = 5.360785484313965\n",
            "epoch 1, text 4200 has batch loss = 3.431330442428589\n",
            "epoch 1, text 4300 has batch loss = 5.838303565979004\n",
            "epoch 1, text 4400 has batch loss = 2.5990829467773438\n",
            "epoch 1, text 4500 has batch loss = 5.645176887512207\n",
            "epoch 1, text 4600 has batch loss = 5.350670337677002\n",
            "epoch 1, text 4700 has batch loss = 4.12013053894043\n",
            "epoch 1, text 4800 has batch loss = 5.736607551574707\n",
            "epoch 1, text 4900 has batch loss = 11.466652870178223\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-209-0d5a9475dfbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mno_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mtrain_rnn_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtextlist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_textlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-203-710121b4c9c4>\u001b[0m in \u001b[0;36mtrain_rnn_epoch\u001b[0;34m(model, optimizer, textlist, word_count_model, max_grad_norm, epoch_num)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mtext_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtext_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mtext_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# reducem gradientii la o constanta daca sunt prea mari\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPs4wrY-tV5p"
      },
      "source": [
        "torch.save(network.state_dict(), basepath+'rnn_size50_att2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DguTWexbJjbn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fc9fea23-4ad1-4194-f453-757e6b2fbcd9"
      },
      "source": [
        "network.load_state_dict(torch.load(basepath+'rnn_size50_att1'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koC5WCQZj-2J"
      },
      "source": [
        "Getting an additional layer on top of it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_aFOuRRj6MF"
      },
      "source": [
        "class SimpleNN(nn.Module):\n",
        "  def __init__(self,in_size: int, interm_size: int, out_size: int):\n",
        "    super().__init__()\n",
        "    self.lin1=nn.Linear(in_size, interm_size)\n",
        "    self.lin2=nn.Linear(interm_size, interm_size)\n",
        "    self.lin3=nn.Linear(interm_size, out_size)\n",
        "  def forward(self, x):\n",
        "    out=F.relu(self.lin1(x))\n",
        "    out=F.relu(self.lin2(out))\n",
        "    out=self.lin3(out)\n",
        "    return F.log_softmax(out, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt5TZEQXlkAh"
      },
      "source": [
        "\"\"\"functie folosita pana la urma pentru a obtine toate hidden state-urile finale de pe textele de antrenament si validare, pentru ca se poate\n",
        "antrena reteaua fully-connected pusa peste hidden-state-uri mult mai usor asa\"\"\"\n",
        "def get_rnn_feature(rnn, train_textlist, word_count_model, train_labels, batch_size):\n",
        "  no_batches=len(train_textlist)//batch_size\n",
        "  for batch_idx in range(no_batches):\n",
        "    batch_texts=train_textlist[batch_idx*batch_size: min((batch_idx+1)*(batch_size), len(train_textlist))]\n",
        "    output_features=torch.Tensor(len(batch_texts), rnn.rnn_size) #vom lua doar ultimul hidden state al retelei pentru fiecare text\n",
        "    output_labels=train_labels[batch_idx*batch_size: min((batch_idx+1)*batch_size, len(train_labels))]\n",
        "    for text_id, text in enumerate(batch_texts):\n",
        "      text_features=make_text_features(text, word_count_model) #returneaza feature-uri de [lg_text x dim_vocab]\n",
        "      # print(f'text_features after make_text_features {text_features.shape}')\n",
        "      #trec returnatele prin recurenta\n",
        "      hidden_start = torch.zeros(1, rnn.rnn_size).to(device)\n",
        "      recurrent_output=rnn(text_features, hidden_start) #output de dimensiune [lg_text x rnn_size]\n",
        "      #fac detach ca gradientii sa nu se propage si asupra retelei rnn (deja antrenate)\n",
        "      recurrent_output.detach_()\n",
        "      # print(f'recurrent output shape {recurrent_output.shape}')\n",
        "      #ma intereseaza doar ultimul hidden state (asociat ultimului cuvant din text); pe el il pastrez\n",
        "      #fac un reshape just in case\n",
        "      recurrent_output=recurrent_output.view(-1,recurrent_output.shape[-1])\n",
        "      last_hidden_state=recurrent_output[-1, :]\n",
        "      # print(f'last_hidden_state shape:{last_hidden_state.shape}')\n",
        "      output_features[text_id]=last_hidden_state\n",
        "    print(output_features.shape)\n",
        "    print(output_labels.shape)\n",
        "    return output_features, output_labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bktTfQkYk8jY"
      },
      "source": [
        "def train_combo(rnn, simple_nn, optimizer_for_simple_nn, rec_train_features, word_count_model, train_labels, batch_size, epoch):\n",
        "    epoch_loss=0.0\n",
        "    no_batches=len(train_textlist)//batch_size\n",
        "    for batch_idx in range(no_batches):\n",
        "      batch=rec_train_features[batch_idx*batch_size: min((batch_idx+1)*(batch_size), len(train_textlist)), :]\n",
        "      batch_labels=train_labels[batch_idx*batch_size: min((batch_idx+1)*batch_size, len(train_labels))]\n",
        "      optimizer_for_simple_nn.zero_grad()\n",
        "      batch_output=simple_nn(batch)\n",
        "      loss=F.cross_entropy(batch_output, batch_labels)\n",
        "      loss.backward()\n",
        "      optimizer_for_simple_nn.step()\n",
        "      with torch.no_grad():\n",
        "          print(f'loss on batch {batch_idx} in epoch {epoch} for the simple nn is: {loss}')\n",
        "\n",
        "def test_combo(rnn, simple_nn, rec_val_features, word_count_model, validation_labels):\n",
        "    test_loss=0.0\n",
        "    with torch.no_grad():\n",
        "      output=simple_nn(rec_val_features)\n",
        "      # loss=F.cross_entropy(batch_output, batch_labels)\n",
        "      print(torch.argmax(output, dim=1))\n",
        "      print(validation_labels)\n",
        "      loss = torch.abs(torch.argmax(output, dim=1) - validation_labels).type(torch.FloatTensor)\n",
        "      print(f'test loss for the simple nn is: {torch.mean(loss)}')\n",
        "      test_loss+=float(torch.mean(loss))\n",
        "    return test_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejgbPcXKsdmA"
      },
      "source": [
        "no_epochs_simple_nn=400\n",
        "batch_size=64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCPCL9eUZ6nb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1966f5e4-8d70-4b73-dd60-e4cd7ce6bf19"
      },
      "source": [
        "rec_train_features, _=get_rnn_features(network, train_textlist, cv, train_labels, batch_size=len(train_textlist))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([7757, 50])\n",
            "torch.Size([7757])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejpk_Sq8aPIs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "de547d1c-5e70-456e-fab8-222b6003d58f"
      },
      "source": [
        "rec_val_features, _=get_rnn_features(network, validation_textlist, cv, validation_labels, batch_size=len(validation_textlist))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2656, 50])\n",
            "torch.Size([2656])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kUPFUUHeWVp"
      },
      "source": [
        "torch.save(rec_train_features, basepath+'rec_train_features.pt')\n",
        "torch.save(rec_train_features, basepath+'rec_validation_features.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8h2zjXasqZ3"
      },
      "source": [
        "simple_nn=SimpleNN(network.rnn_size, 1000, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjru-n_2YU1h"
      },
      "source": [
        "optimizer_for_simple_nn=torch.optim.Adam(simple_nn.parameters(), lr=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvJ_WrJUshT4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "072181b0-6ed2-486f-8b9e-cd1fa3156716"
      },
      "source": [
        "for e in range(no_epochs_simple_nn):\n",
        "  train_combo(network, simple_nn, optimizer_for_simple_nn, rec_train_features, cv, train_labels, batch_size, e)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "loss on batch 43 in epoch 52 for the simple nn is: 0.5122823119163513\n",
            "loss on batch 44 in epoch 52 for the simple nn is: 0.5795986652374268\n",
            "loss on batch 45 in epoch 52 for the simple nn is: 0.5101519227027893\n",
            "loss on batch 46 in epoch 52 for the simple nn is: 0.4630879759788513\n",
            "loss on batch 47 in epoch 52 for the simple nn is: 0.47553592920303345\n",
            "loss on batch 48 in epoch 52 for the simple nn is: 0.4991553723812103\n",
            "loss on batch 49 in epoch 52 for the simple nn is: 0.5440242886543274\n",
            "loss on batch 50 in epoch 52 for the simple nn is: 0.47559598088264465\n",
            "loss on batch 51 in epoch 52 for the simple nn is: 0.5452596545219421\n",
            "loss on batch 52 in epoch 52 for the simple nn is: 0.40655383467674255\n",
            "loss on batch 53 in epoch 52 for the simple nn is: 0.39584875106811523\n",
            "loss on batch 54 in epoch 52 for the simple nn is: 0.557030439376831\n",
            "loss on batch 55 in epoch 52 for the simple nn is: 0.49535274505615234\n",
            "loss on batch 56 in epoch 52 for the simple nn is: 0.5770833492279053\n",
            "loss on batch 57 in epoch 52 for the simple nn is: 0.7069675326347351\n",
            "loss on batch 58 in epoch 52 for the simple nn is: 0.5222753882408142\n",
            "loss on batch 59 in epoch 52 for the simple nn is: 0.4980446696281433\n",
            "loss on batch 60 in epoch 52 for the simple nn is: 0.49068161845207214\n",
            "loss on batch 61 in epoch 52 for the simple nn is: 0.4483126103878021\n",
            "loss on batch 62 in epoch 52 for the simple nn is: 0.48481157422065735\n",
            "loss on batch 63 in epoch 52 for the simple nn is: 0.501011848449707\n",
            "loss on batch 64 in epoch 52 for the simple nn is: 0.44759660959243774\n",
            "loss on batch 65 in epoch 52 for the simple nn is: 0.4516664743423462\n",
            "loss on batch 66 in epoch 52 for the simple nn is: 0.49730440974235535\n",
            "loss on batch 67 in epoch 52 for the simple nn is: 0.43554890155792236\n",
            "loss on batch 68 in epoch 52 for the simple nn is: 0.46624433994293213\n",
            "loss on batch 69 in epoch 52 for the simple nn is: 0.4888049364089966\n",
            "loss on batch 70 in epoch 52 for the simple nn is: 0.4813343584537506\n",
            "loss on batch 71 in epoch 52 for the simple nn is: 0.5091521739959717\n",
            "loss on batch 72 in epoch 52 for the simple nn is: 0.5016263723373413\n",
            "loss on batch 73 in epoch 52 for the simple nn is: 0.5989726781845093\n",
            "loss on batch 74 in epoch 52 for the simple nn is: 0.5272408723831177\n",
            "loss on batch 75 in epoch 52 for the simple nn is: 0.5217322707176208\n",
            "loss on batch 76 in epoch 52 for the simple nn is: 0.48492497205734253\n",
            "loss on batch 77 in epoch 52 for the simple nn is: 0.4382554888725281\n",
            "loss on batch 78 in epoch 52 for the simple nn is: 0.49635571241378784\n",
            "loss on batch 79 in epoch 52 for the simple nn is: 0.46977055072784424\n",
            "loss on batch 80 in epoch 52 for the simple nn is: 0.5099362134933472\n",
            "loss on batch 81 in epoch 52 for the simple nn is: 0.450280100107193\n",
            "loss on batch 82 in epoch 52 for the simple nn is: 0.7012249231338501\n",
            "loss on batch 83 in epoch 52 for the simple nn is: 0.5384724736213684\n",
            "loss on batch 84 in epoch 52 for the simple nn is: 0.5603920817375183\n",
            "loss on batch 85 in epoch 52 for the simple nn is: 0.5163697004318237\n",
            "loss on batch 86 in epoch 52 for the simple nn is: 0.5169010758399963\n",
            "loss on batch 87 in epoch 52 for the simple nn is: 0.5013272762298584\n",
            "loss on batch 88 in epoch 52 for the simple nn is: 0.3928861916065216\n",
            "loss on batch 89 in epoch 52 for the simple nn is: 0.5197718739509583\n",
            "loss on batch 90 in epoch 52 for the simple nn is: 0.4773578643798828\n",
            "loss on batch 91 in epoch 52 for the simple nn is: 0.5114725232124329\n",
            "loss on batch 92 in epoch 52 for the simple nn is: 0.5644943118095398\n",
            "loss on batch 93 in epoch 52 for the simple nn is: 0.5044478178024292\n",
            "loss on batch 94 in epoch 52 for the simple nn is: 0.42780178785324097\n",
            "loss on batch 95 in epoch 52 for the simple nn is: 0.5225924253463745\n",
            "loss on batch 96 in epoch 52 for the simple nn is: 0.4252766966819763\n",
            "loss on batch 97 in epoch 52 for the simple nn is: 0.5053868889808655\n",
            "loss on batch 98 in epoch 52 for the simple nn is: 0.4645036458969116\n",
            "loss on batch 99 in epoch 52 for the simple nn is: 0.4712454676628113\n",
            "loss on batch 100 in epoch 52 for the simple nn is: 0.5044311285018921\n",
            "loss on batch 101 in epoch 52 for the simple nn is: 0.4219290614128113\n",
            "loss on batch 102 in epoch 52 for the simple nn is: 0.400086373090744\n",
            "loss on batch 103 in epoch 52 for the simple nn is: 0.515160322189331\n",
            "loss on batch 104 in epoch 52 for the simple nn is: 0.46351832151412964\n",
            "loss on batch 105 in epoch 52 for the simple nn is: 0.42929786443710327\n",
            "loss on batch 106 in epoch 52 for the simple nn is: 0.4598820209503174\n",
            "loss on batch 107 in epoch 52 for the simple nn is: 0.4540652632713318\n",
            "loss on batch 108 in epoch 52 for the simple nn is: 0.4573841392993927\n",
            "loss on batch 109 in epoch 52 for the simple nn is: 0.5829800963401794\n",
            "loss on batch 110 in epoch 52 for the simple nn is: 0.3658236563205719\n",
            "loss on batch 111 in epoch 52 for the simple nn is: 0.5131950974464417\n",
            "loss on batch 112 in epoch 52 for the simple nn is: 0.5043994188308716\n",
            "loss on batch 113 in epoch 52 for the simple nn is: 0.5369348526000977\n",
            "loss on batch 114 in epoch 52 for the simple nn is: 0.5203359127044678\n",
            "loss on batch 115 in epoch 52 for the simple nn is: 0.47987422347068787\n",
            "loss on batch 116 in epoch 52 for the simple nn is: 0.5964791774749756\n",
            "loss on batch 117 in epoch 52 for the simple nn is: 0.5605270862579346\n",
            "loss on batch 118 in epoch 52 for the simple nn is: 0.5888200998306274\n",
            "loss on batch 119 in epoch 52 for the simple nn is: 0.548394501209259\n",
            "loss on batch 120 in epoch 52 for the simple nn is: 0.5108376741409302\n",
            "loss on batch 0 in epoch 53 for the simple nn is: 0.48484280705451965\n",
            "loss on batch 1 in epoch 53 for the simple nn is: 0.5946081876754761\n",
            "loss on batch 2 in epoch 53 for the simple nn is: 0.6136602759361267\n",
            "loss on batch 3 in epoch 53 for the simple nn is: 0.5298596620559692\n",
            "loss on batch 4 in epoch 53 for the simple nn is: 0.6692524552345276\n",
            "loss on batch 5 in epoch 53 for the simple nn is: 0.5787999629974365\n",
            "loss on batch 6 in epoch 53 for the simple nn is: 0.5414203405380249\n",
            "loss on batch 7 in epoch 53 for the simple nn is: 0.5118667483329773\n",
            "loss on batch 8 in epoch 53 for the simple nn is: 0.46691861748695374\n",
            "loss on batch 9 in epoch 53 for the simple nn is: 0.4844724237918854\n",
            "loss on batch 10 in epoch 53 for the simple nn is: 0.44344285130500793\n",
            "loss on batch 11 in epoch 53 for the simple nn is: 0.5358934998512268\n",
            "loss on batch 12 in epoch 53 for the simple nn is: 0.5353982448577881\n",
            "loss on batch 13 in epoch 53 for the simple nn is: 0.5463838577270508\n",
            "loss on batch 14 in epoch 53 for the simple nn is: 0.5275806188583374\n",
            "loss on batch 15 in epoch 53 for the simple nn is: 0.4743693768978119\n",
            "loss on batch 16 in epoch 53 for the simple nn is: 0.5383814573287964\n",
            "loss on batch 17 in epoch 53 for the simple nn is: 0.48780497908592224\n",
            "loss on batch 18 in epoch 53 for the simple nn is: 0.48133528232574463\n",
            "loss on batch 19 in epoch 53 for the simple nn is: 0.7224235534667969\n",
            "loss on batch 20 in epoch 53 for the simple nn is: 0.5058391094207764\n",
            "loss on batch 21 in epoch 53 for the simple nn is: 0.5280747413635254\n",
            "loss on batch 22 in epoch 53 for the simple nn is: 0.4752320647239685\n",
            "loss on batch 23 in epoch 53 for the simple nn is: 0.4636240005493164\n",
            "loss on batch 24 in epoch 53 for the simple nn is: 0.4159837067127228\n",
            "loss on batch 25 in epoch 53 for the simple nn is: 0.46643781661987305\n",
            "loss on batch 26 in epoch 53 for the simple nn is: 0.5484455823898315\n",
            "loss on batch 27 in epoch 53 for the simple nn is: 0.49574050307273865\n",
            "loss on batch 28 in epoch 53 for the simple nn is: 0.47097477316856384\n",
            "loss on batch 29 in epoch 53 for the simple nn is: 0.5710031986236572\n",
            "loss on batch 30 in epoch 53 for the simple nn is: 0.4884890913963318\n",
            "loss on batch 31 in epoch 53 for the simple nn is: 0.5622329115867615\n",
            "loss on batch 32 in epoch 53 for the simple nn is: 0.4554314613342285\n",
            "loss on batch 33 in epoch 53 for the simple nn is: 0.48747518658638\n",
            "loss on batch 34 in epoch 53 for the simple nn is: 0.4665496051311493\n",
            "loss on batch 35 in epoch 53 for the simple nn is: 0.4984064996242523\n",
            "loss on batch 36 in epoch 53 for the simple nn is: 0.5084609389305115\n",
            "loss on batch 37 in epoch 53 for the simple nn is: 0.4982535243034363\n",
            "loss on batch 38 in epoch 53 for the simple nn is: 0.5634015202522278\n",
            "loss on batch 39 in epoch 53 for the simple nn is: 0.44635674357414246\n",
            "loss on batch 40 in epoch 53 for the simple nn is: 0.5297943949699402\n",
            "loss on batch 41 in epoch 53 for the simple nn is: 0.43463563919067383\n",
            "loss on batch 42 in epoch 53 for the simple nn is: 0.5095944404602051\n",
            "loss on batch 43 in epoch 53 for the simple nn is: 0.5120824575424194\n",
            "loss on batch 44 in epoch 53 for the simple nn is: 0.5225608348846436\n",
            "loss on batch 45 in epoch 53 for the simple nn is: 0.5047118067741394\n",
            "loss on batch 46 in epoch 53 for the simple nn is: 0.45297130942344666\n",
            "loss on batch 47 in epoch 53 for the simple nn is: 0.5063730478286743\n",
            "loss on batch 48 in epoch 53 for the simple nn is: 0.5059226155281067\n",
            "loss on batch 49 in epoch 53 for the simple nn is: 0.5508702397346497\n",
            "loss on batch 50 in epoch 53 for the simple nn is: 0.4768930971622467\n",
            "loss on batch 51 in epoch 53 for the simple nn is: 0.5377225279808044\n",
            "loss on batch 52 in epoch 53 for the simple nn is: 0.4086416959762573\n",
            "loss on batch 53 in epoch 53 for the simple nn is: 0.45223480463027954\n",
            "loss on batch 54 in epoch 53 for the simple nn is: 0.5608935952186584\n",
            "loss on batch 55 in epoch 53 for the simple nn is: 0.5153812766075134\n",
            "loss on batch 56 in epoch 53 for the simple nn is: 0.5358974933624268\n",
            "loss on batch 57 in epoch 53 for the simple nn is: 0.5279444456100464\n",
            "loss on batch 58 in epoch 53 for the simple nn is: 0.4761689603328705\n",
            "loss on batch 59 in epoch 53 for the simple nn is: 0.5082768201828003\n",
            "loss on batch 60 in epoch 53 for the simple nn is: 0.48968201875686646\n",
            "loss on batch 61 in epoch 53 for the simple nn is: 0.5707815289497375\n",
            "loss on batch 62 in epoch 53 for the simple nn is: 0.4920276999473572\n",
            "loss on batch 63 in epoch 53 for the simple nn is: 0.4818861186504364\n",
            "loss on batch 64 in epoch 53 for the simple nn is: 0.4257020056247711\n",
            "loss on batch 65 in epoch 53 for the simple nn is: 0.44558680057525635\n",
            "loss on batch 66 in epoch 53 for the simple nn is: 0.49741485714912415\n",
            "loss on batch 67 in epoch 53 for the simple nn is: 0.4234190285205841\n",
            "loss on batch 68 in epoch 53 for the simple nn is: 0.45049726963043213\n",
            "loss on batch 69 in epoch 53 for the simple nn is: 0.4739428460597992\n",
            "loss on batch 70 in epoch 53 for the simple nn is: 0.4839230179786682\n",
            "loss on batch 71 in epoch 53 for the simple nn is: 0.49154019355773926\n",
            "loss on batch 72 in epoch 53 for the simple nn is: 0.49053144454956055\n",
            "loss on batch 73 in epoch 53 for the simple nn is: 0.540398359298706\n",
            "loss on batch 74 in epoch 53 for the simple nn is: 0.5157687067985535\n",
            "loss on batch 75 in epoch 53 for the simple nn is: 0.530951738357544\n",
            "loss on batch 76 in epoch 53 for the simple nn is: 0.4809850752353668\n",
            "loss on batch 77 in epoch 53 for the simple nn is: 0.44038209319114685\n",
            "loss on batch 78 in epoch 53 for the simple nn is: 0.486461877822876\n",
            "loss on batch 79 in epoch 53 for the simple nn is: 0.46312415599823\n",
            "loss on batch 80 in epoch 53 for the simple nn is: 0.4977383315563202\n",
            "loss on batch 81 in epoch 53 for the simple nn is: 0.43090227246284485\n",
            "loss on batch 82 in epoch 53 for the simple nn is: 0.5027496218681335\n",
            "loss on batch 83 in epoch 53 for the simple nn is: 0.5293175578117371\n",
            "loss on batch 84 in epoch 53 for the simple nn is: 0.5170698165893555\n",
            "loss on batch 85 in epoch 53 for the simple nn is: 0.5008586645126343\n",
            "loss on batch 86 in epoch 53 for the simple nn is: 0.4222179651260376\n",
            "loss on batch 87 in epoch 53 for the simple nn is: 0.5013731122016907\n",
            "loss on batch 88 in epoch 53 for the simple nn is: 0.39266443252563477\n",
            "loss on batch 89 in epoch 53 for the simple nn is: 0.5172362923622131\n",
            "loss on batch 90 in epoch 53 for the simple nn is: 0.4585110545158386\n",
            "loss on batch 91 in epoch 53 for the simple nn is: 0.4481755793094635\n",
            "loss on batch 92 in epoch 53 for the simple nn is: 0.5465810894966125\n",
            "loss on batch 93 in epoch 53 for the simple nn is: 0.5044923424720764\n",
            "loss on batch 94 in epoch 53 for the simple nn is: 0.41662341356277466\n",
            "loss on batch 95 in epoch 53 for the simple nn is: 0.49997860193252563\n",
            "loss on batch 96 in epoch 53 for the simple nn is: 0.4364630877971649\n",
            "loss on batch 97 in epoch 53 for the simple nn is: 0.48937493562698364\n",
            "loss on batch 98 in epoch 53 for the simple nn is: 0.43768423795700073\n",
            "loss on batch 99 in epoch 53 for the simple nn is: 0.4576282799243927\n",
            "loss on batch 100 in epoch 53 for the simple nn is: 0.5379461646080017\n",
            "loss on batch 101 in epoch 53 for the simple nn is: 0.40594151616096497\n",
            "loss on batch 102 in epoch 53 for the simple nn is: 0.3888765573501587\n",
            "loss on batch 103 in epoch 53 for the simple nn is: 0.5348267555236816\n",
            "loss on batch 104 in epoch 53 for the simple nn is: 0.44955265522003174\n",
            "loss on batch 105 in epoch 53 for the simple nn is: 0.42797642946243286\n",
            "loss on batch 106 in epoch 53 for the simple nn is: 0.46005868911743164\n",
            "loss on batch 107 in epoch 53 for the simple nn is: 0.4599224925041199\n",
            "loss on batch 108 in epoch 53 for the simple nn is: 0.4578145444393158\n",
            "loss on batch 109 in epoch 53 for the simple nn is: 0.5388146638870239\n",
            "loss on batch 110 in epoch 53 for the simple nn is: 0.349386990070343\n",
            "loss on batch 111 in epoch 53 for the simple nn is: 0.4549252986907959\n",
            "loss on batch 112 in epoch 53 for the simple nn is: 0.5012956857681274\n",
            "loss on batch 113 in epoch 53 for the simple nn is: 0.5316170454025269\n",
            "loss on batch 114 in epoch 53 for the simple nn is: 0.5222576856613159\n",
            "loss on batch 115 in epoch 53 for the simple nn is: 0.4724298119544983\n",
            "loss on batch 116 in epoch 53 for the simple nn is: 0.5938371419906616\n",
            "loss on batch 117 in epoch 53 for the simple nn is: 0.5467484593391418\n",
            "loss on batch 118 in epoch 53 for the simple nn is: 0.6079697608947754\n",
            "loss on batch 119 in epoch 53 for the simple nn is: 0.5362739562988281\n",
            "loss on batch 120 in epoch 53 for the simple nn is: 0.49337899684906006\n",
            "loss on batch 0 in epoch 54 for the simple nn is: 0.4767082631587982\n",
            "loss on batch 1 in epoch 54 for the simple nn is: 0.6102390289306641\n",
            "loss on batch 2 in epoch 54 for the simple nn is: 0.5715961456298828\n",
            "loss on batch 3 in epoch 54 for the simple nn is: 0.5569313168525696\n",
            "loss on batch 4 in epoch 54 for the simple nn is: 0.5671342611312866\n",
            "loss on batch 5 in epoch 54 for the simple nn is: 0.6025304198265076\n",
            "loss on batch 6 in epoch 54 for the simple nn is: 0.5414109230041504\n",
            "loss on batch 7 in epoch 54 for the simple nn is: 0.5068891048431396\n",
            "loss on batch 8 in epoch 54 for the simple nn is: 0.45989370346069336\n",
            "loss on batch 9 in epoch 54 for the simple nn is: 0.47638988494873047\n",
            "loss on batch 10 in epoch 54 for the simple nn is: 0.4614056348800659\n",
            "loss on batch 11 in epoch 54 for the simple nn is: 0.5210964679718018\n",
            "loss on batch 12 in epoch 54 for the simple nn is: 0.5413708090782166\n",
            "loss on batch 13 in epoch 54 for the simple nn is: 0.5372236967086792\n",
            "loss on batch 14 in epoch 54 for the simple nn is: 0.500433623790741\n",
            "loss on batch 15 in epoch 54 for the simple nn is: 0.45316001772880554\n",
            "loss on batch 16 in epoch 54 for the simple nn is: 0.5249826908111572\n",
            "loss on batch 17 in epoch 54 for the simple nn is: 0.5563696622848511\n",
            "loss on batch 18 in epoch 54 for the simple nn is: 0.4814552664756775\n",
            "loss on batch 19 in epoch 54 for the simple nn is: 0.4954519271850586\n",
            "loss on batch 20 in epoch 54 for the simple nn is: 0.4589940905570984\n",
            "loss on batch 21 in epoch 54 for the simple nn is: 0.5246374607086182\n",
            "loss on batch 22 in epoch 54 for the simple nn is: 0.5545289516448975\n",
            "loss on batch 23 in epoch 54 for the simple nn is: 0.45928633213043213\n",
            "loss on batch 24 in epoch 54 for the simple nn is: 0.4315420091152191\n",
            "loss on batch 25 in epoch 54 for the simple nn is: 0.4679541289806366\n",
            "loss on batch 26 in epoch 54 for the simple nn is: 0.5354132652282715\n",
            "loss on batch 27 in epoch 54 for the simple nn is: 0.5047111511230469\n",
            "loss on batch 28 in epoch 54 for the simple nn is: 0.4533625543117523\n",
            "loss on batch 29 in epoch 54 for the simple nn is: 0.5609976053237915\n",
            "loss on batch 30 in epoch 54 for the simple nn is: 0.4886278510093689\n",
            "loss on batch 31 in epoch 54 for the simple nn is: 0.5621863603591919\n",
            "loss on batch 32 in epoch 54 for the simple nn is: 0.45237159729003906\n",
            "loss on batch 33 in epoch 54 for the simple nn is: 0.4991406500339508\n",
            "loss on batch 34 in epoch 54 for the simple nn is: 0.4623952805995941\n",
            "loss on batch 35 in epoch 54 for the simple nn is: 0.4890538454055786\n",
            "loss on batch 36 in epoch 54 for the simple nn is: 0.4933794140815735\n",
            "loss on batch 37 in epoch 54 for the simple nn is: 0.4406350553035736\n",
            "loss on batch 38 in epoch 54 for the simple nn is: 0.5393331050872803\n",
            "loss on batch 39 in epoch 54 for the simple nn is: 0.44352421164512634\n",
            "loss on batch 40 in epoch 54 for the simple nn is: 0.5248275995254517\n",
            "loss on batch 41 in epoch 54 for the simple nn is: 0.40851786732673645\n",
            "loss on batch 42 in epoch 54 for the simple nn is: 0.5010008811950684\n",
            "loss on batch 43 in epoch 54 for the simple nn is: 0.5122352838516235\n",
            "loss on batch 44 in epoch 54 for the simple nn is: 0.5152573585510254\n",
            "loss on batch 45 in epoch 54 for the simple nn is: 0.5126925110816956\n",
            "loss on batch 46 in epoch 54 for the simple nn is: 0.4457385241985321\n",
            "loss on batch 47 in epoch 54 for the simple nn is: 0.4770655930042267\n",
            "loss on batch 48 in epoch 54 for the simple nn is: 0.5085204839706421\n",
            "loss on batch 49 in epoch 54 for the simple nn is: 0.537680447101593\n",
            "loss on batch 50 in epoch 54 for the simple nn is: 0.47625860571861267\n",
            "loss on batch 51 in epoch 54 for the simple nn is: 0.5189197659492493\n",
            "loss on batch 52 in epoch 54 for the simple nn is: 0.4248228073120117\n",
            "loss on batch 53 in epoch 54 for the simple nn is: 0.3667074143886566\n",
            "loss on batch 54 in epoch 54 for the simple nn is: 0.5639474391937256\n",
            "loss on batch 55 in epoch 54 for the simple nn is: 0.5134130716323853\n",
            "loss on batch 56 in epoch 54 for the simple nn is: 0.521905779838562\n",
            "loss on batch 57 in epoch 54 for the simple nn is: 0.5129799842834473\n",
            "loss on batch 58 in epoch 54 for the simple nn is: 0.4532179534435272\n",
            "loss on batch 59 in epoch 54 for the simple nn is: 0.5138827562332153\n",
            "loss on batch 60 in epoch 54 for the simple nn is: 0.4816752076148987\n",
            "loss on batch 61 in epoch 54 for the simple nn is: 0.6308102607727051\n",
            "loss on batch 62 in epoch 54 for the simple nn is: 0.47244492173194885\n",
            "loss on batch 63 in epoch 54 for the simple nn is: 0.4978475868701935\n",
            "loss on batch 64 in epoch 54 for the simple nn is: 0.4140062630176544\n",
            "loss on batch 65 in epoch 54 for the simple nn is: 0.4331778883934021\n",
            "loss on batch 66 in epoch 54 for the simple nn is: 0.4957430064678192\n",
            "loss on batch 67 in epoch 54 for the simple nn is: 0.4255504906177521\n",
            "loss on batch 68 in epoch 54 for the simple nn is: 0.4649585485458374\n",
            "loss on batch 69 in epoch 54 for the simple nn is: 0.5514050722122192\n",
            "loss on batch 70 in epoch 54 for the simple nn is: 0.4726548492908478\n",
            "loss on batch 71 in epoch 54 for the simple nn is: 0.6339300274848938\n",
            "loss on batch 72 in epoch 54 for the simple nn is: 0.4813178777694702\n",
            "loss on batch 73 in epoch 54 for the simple nn is: 0.5483423471450806\n",
            "loss on batch 74 in epoch 54 for the simple nn is: 0.5096597075462341\n",
            "loss on batch 75 in epoch 54 for the simple nn is: 0.5075681209564209\n",
            "loss on batch 76 in epoch 54 for the simple nn is: 0.5076502561569214\n",
            "loss on batch 77 in epoch 54 for the simple nn is: 0.4446386992931366\n",
            "loss on batch 78 in epoch 54 for the simple nn is: 0.5408674478530884\n",
            "loss on batch 79 in epoch 54 for the simple nn is: 0.42949333786964417\n",
            "loss on batch 80 in epoch 54 for the simple nn is: 0.48261556029319763\n",
            "loss on batch 81 in epoch 54 for the simple nn is: 0.4251340925693512\n",
            "loss on batch 82 in epoch 54 for the simple nn is: 0.4881089925765991\n",
            "loss on batch 83 in epoch 54 for the simple nn is: 0.5291131138801575\n",
            "loss on batch 84 in epoch 54 for the simple nn is: 0.5399184226989746\n",
            "loss on batch 85 in epoch 54 for the simple nn is: 0.49978458881378174\n",
            "loss on batch 86 in epoch 54 for the simple nn is: 0.42298147082328796\n",
            "loss on batch 87 in epoch 54 for the simple nn is: 0.5837863087654114\n",
            "loss on batch 88 in epoch 54 for the simple nn is: 0.38062551617622375\n",
            "loss on batch 89 in epoch 54 for the simple nn is: 0.5461138486862183\n",
            "loss on batch 90 in epoch 54 for the simple nn is: 0.45135608315467834\n",
            "loss on batch 91 in epoch 54 for the simple nn is: 0.47745129466056824\n",
            "loss on batch 92 in epoch 54 for the simple nn is: 0.5381773114204407\n",
            "loss on batch 93 in epoch 54 for the simple nn is: 0.4700019657611847\n",
            "loss on batch 94 in epoch 54 for the simple nn is: 0.4126207232475281\n",
            "loss on batch 95 in epoch 54 for the simple nn is: 0.6371126174926758\n",
            "loss on batch 96 in epoch 54 for the simple nn is: 0.48928380012512207\n",
            "loss on batch 97 in epoch 54 for the simple nn is: 0.543233335018158\n",
            "loss on batch 98 in epoch 54 for the simple nn is: 0.47950825095176697\n",
            "loss on batch 99 in epoch 54 for the simple nn is: 0.4574013650417328\n",
            "loss on batch 100 in epoch 54 for the simple nn is: 0.48128625750541687\n",
            "loss on batch 101 in epoch 54 for the simple nn is: 0.4009191691875458\n",
            "loss on batch 102 in epoch 54 for the simple nn is: 0.44107943773269653\n",
            "loss on batch 103 in epoch 54 for the simple nn is: 0.528641402721405\n",
            "loss on batch 104 in epoch 54 for the simple nn is: 0.468339204788208\n",
            "loss on batch 105 in epoch 54 for the simple nn is: 0.45631736516952515\n",
            "loss on batch 106 in epoch 54 for the simple nn is: 0.4830000102519989\n",
            "loss on batch 107 in epoch 54 for the simple nn is: 0.4664962887763977\n",
            "loss on batch 108 in epoch 54 for the simple nn is: 0.493619441986084\n",
            "loss on batch 109 in epoch 54 for the simple nn is: 0.5729187726974487\n",
            "loss on batch 110 in epoch 54 for the simple nn is: 0.3591703176498413\n",
            "loss on batch 111 in epoch 54 for the simple nn is: 0.46082988381385803\n",
            "loss on batch 112 in epoch 54 for the simple nn is: 0.5220450162887573\n",
            "loss on batch 113 in epoch 54 for the simple nn is: 0.5677008628845215\n",
            "loss on batch 114 in epoch 54 for the simple nn is: 0.5422919392585754\n",
            "loss on batch 115 in epoch 54 for the simple nn is: 0.4734826385974884\n",
            "loss on batch 116 in epoch 54 for the simple nn is: 0.5893792510032654\n",
            "loss on batch 117 in epoch 54 for the simple nn is: 0.5408215522766113\n",
            "loss on batch 118 in epoch 54 for the simple nn is: 0.6458743810653687\n",
            "loss on batch 119 in epoch 54 for the simple nn is: 0.544286847114563\n",
            "loss on batch 120 in epoch 54 for the simple nn is: 0.5020624399185181\n",
            "loss on batch 0 in epoch 55 for the simple nn is: 0.5339723229408264\n",
            "loss on batch 1 in epoch 55 for the simple nn is: 0.6097733378410339\n",
            "loss on batch 2 in epoch 55 for the simple nn is: 0.6014812588691711\n",
            "loss on batch 3 in epoch 55 for the simple nn is: 0.5523894429206848\n",
            "loss on batch 4 in epoch 55 for the simple nn is: 0.5605147480964661\n",
            "loss on batch 5 in epoch 55 for the simple nn is: 0.5905103087425232\n",
            "loss on batch 6 in epoch 55 for the simple nn is: 0.5439529418945312\n",
            "loss on batch 7 in epoch 55 for the simple nn is: 0.5417423844337463\n",
            "loss on batch 8 in epoch 55 for the simple nn is: 0.49592727422714233\n",
            "loss on batch 9 in epoch 55 for the simple nn is: 0.49228572845458984\n",
            "loss on batch 10 in epoch 55 for the simple nn is: 0.464284211397171\n",
            "loss on batch 11 in epoch 55 for the simple nn is: 0.5274856686592102\n",
            "loss on batch 12 in epoch 55 for the simple nn is: 0.5437460541725159\n",
            "loss on batch 13 in epoch 55 for the simple nn is: 0.5562905073165894\n",
            "loss on batch 14 in epoch 55 for the simple nn is: 0.5198801755905151\n",
            "loss on batch 15 in epoch 55 for the simple nn is: 0.48218676447868347\n",
            "loss on batch 16 in epoch 55 for the simple nn is: 0.5327205657958984\n",
            "loss on batch 17 in epoch 55 for the simple nn is: 0.4834078550338745\n",
            "loss on batch 18 in epoch 55 for the simple nn is: 0.47532349824905396\n",
            "loss on batch 19 in epoch 55 for the simple nn is: 0.48932307958602905\n",
            "loss on batch 20 in epoch 55 for the simple nn is: 0.4898380637168884\n",
            "loss on batch 21 in epoch 55 for the simple nn is: 0.5499939322471619\n",
            "loss on batch 22 in epoch 55 for the simple nn is: 0.481208473443985\n",
            "loss on batch 23 in epoch 55 for the simple nn is: 0.46506011486053467\n",
            "loss on batch 24 in epoch 55 for the simple nn is: 0.41329067945480347\n",
            "loss on batch 25 in epoch 55 for the simple nn is: 0.4694516062736511\n",
            "loss on batch 26 in epoch 55 for the simple nn is: 0.5311596989631653\n",
            "loss on batch 27 in epoch 55 for the simple nn is: 0.49213090538978577\n",
            "loss on batch 28 in epoch 55 for the simple nn is: 0.47157296538352966\n",
            "loss on batch 29 in epoch 55 for the simple nn is: 0.5562596321105957\n",
            "loss on batch 30 in epoch 55 for the simple nn is: 0.5028429627418518\n",
            "loss on batch 31 in epoch 55 for the simple nn is: 0.6299181580543518\n",
            "loss on batch 32 in epoch 55 for the simple nn is: 0.48350927233695984\n",
            "loss on batch 33 in epoch 55 for the simple nn is: 0.490045964717865\n",
            "loss on batch 34 in epoch 55 for the simple nn is: 0.48676159977912903\n",
            "loss on batch 35 in epoch 55 for the simple nn is: 0.5076303482055664\n",
            "loss on batch 36 in epoch 55 for the simple nn is: 0.49089884757995605\n",
            "loss on batch 37 in epoch 55 for the simple nn is: 0.4403707683086395\n",
            "loss on batch 38 in epoch 55 for the simple nn is: 0.5466780066490173\n",
            "loss on batch 39 in epoch 55 for the simple nn is: 0.4593198001384735\n",
            "loss on batch 40 in epoch 55 for the simple nn is: 0.5323749780654907\n",
            "loss on batch 41 in epoch 55 for the simple nn is: 0.4312843680381775\n",
            "loss on batch 42 in epoch 55 for the simple nn is: 0.5466467142105103\n",
            "loss on batch 43 in epoch 55 for the simple nn is: 0.5146765112876892\n",
            "loss on batch 44 in epoch 55 for the simple nn is: 0.5325871109962463\n",
            "loss on batch 45 in epoch 55 for the simple nn is: 0.5222992897033691\n",
            "loss on batch 46 in epoch 55 for the simple nn is: 0.530663013458252\n",
            "loss on batch 47 in epoch 55 for the simple nn is: 0.48800599575042725\n",
            "loss on batch 48 in epoch 55 for the simple nn is: 0.5084323883056641\n",
            "loss on batch 49 in epoch 55 for the simple nn is: 0.5831436514854431\n",
            "loss on batch 50 in epoch 55 for the simple nn is: 0.5025002956390381\n",
            "loss on batch 51 in epoch 55 for the simple nn is: 0.5656742453575134\n",
            "loss on batch 52 in epoch 55 for the simple nn is: 0.44075506925582886\n",
            "loss on batch 53 in epoch 55 for the simple nn is: 0.39024680852890015\n",
            "loss on batch 54 in epoch 55 for the simple nn is: 0.5756612420082092\n",
            "loss on batch 55 in epoch 55 for the simple nn is: 0.49051743745803833\n",
            "loss on batch 56 in epoch 55 for the simple nn is: 0.543104887008667\n",
            "loss on batch 57 in epoch 55 for the simple nn is: 0.5123689770698547\n",
            "loss on batch 58 in epoch 55 for the simple nn is: 0.47542306780815125\n",
            "loss on batch 59 in epoch 55 for the simple nn is: 0.511091411113739\n",
            "loss on batch 60 in epoch 55 for the simple nn is: 0.537109911441803\n",
            "loss on batch 61 in epoch 55 for the simple nn is: 0.4467836916446686\n",
            "loss on batch 62 in epoch 55 for the simple nn is: 0.487525075674057\n",
            "loss on batch 63 in epoch 55 for the simple nn is: 0.5083969235420227\n",
            "loss on batch 64 in epoch 55 for the simple nn is: 0.42495623230934143\n",
            "loss on batch 65 in epoch 55 for the simple nn is: 0.4669561982154846\n",
            "loss on batch 66 in epoch 55 for the simple nn is: 0.49411436915397644\n",
            "loss on batch 67 in epoch 55 for the simple nn is: 0.44996899366378784\n",
            "loss on batch 68 in epoch 55 for the simple nn is: 0.4504142999649048\n",
            "loss on batch 69 in epoch 55 for the simple nn is: 0.4966469407081604\n",
            "loss on batch 70 in epoch 55 for the simple nn is: 0.5042330622673035\n",
            "loss on batch 71 in epoch 55 for the simple nn is: 0.5160950422286987\n",
            "loss on batch 72 in epoch 55 for the simple nn is: 0.5255096554756165\n",
            "loss on batch 73 in epoch 55 for the simple nn is: 0.5515968203544617\n",
            "loss on batch 74 in epoch 55 for the simple nn is: 0.5298346877098083\n",
            "loss on batch 75 in epoch 55 for the simple nn is: 0.5155896544456482\n",
            "loss on batch 76 in epoch 55 for the simple nn is: 0.5266538262367249\n",
            "loss on batch 77 in epoch 55 for the simple nn is: 0.49979808926582336\n",
            "loss on batch 78 in epoch 55 for the simple nn is: 0.5070042610168457\n",
            "loss on batch 79 in epoch 55 for the simple nn is: 0.41974836587905884\n",
            "loss on batch 80 in epoch 55 for the simple nn is: 0.5291371941566467\n",
            "loss on batch 81 in epoch 55 for the simple nn is: 0.43253204226493835\n",
            "loss on batch 82 in epoch 55 for the simple nn is: 0.5969622135162354\n",
            "loss on batch 83 in epoch 55 for the simple nn is: 0.5790911912918091\n",
            "loss on batch 84 in epoch 55 for the simple nn is: 0.4849945306777954\n",
            "loss on batch 85 in epoch 55 for the simple nn is: 0.5446762442588806\n",
            "loss on batch 86 in epoch 55 for the simple nn is: 0.43193894624710083\n",
            "loss on batch 87 in epoch 55 for the simple nn is: 0.5192092061042786\n",
            "loss on batch 88 in epoch 55 for the simple nn is: 0.4103931784629822\n",
            "loss on batch 89 in epoch 55 for the simple nn is: 0.5557015538215637\n",
            "loss on batch 90 in epoch 55 for the simple nn is: 0.5338441729545593\n",
            "loss on batch 91 in epoch 55 for the simple nn is: 0.45906516909599304\n",
            "loss on batch 92 in epoch 55 for the simple nn is: 0.5874029397964478\n",
            "loss on batch 93 in epoch 55 for the simple nn is: 0.6960256695747375\n",
            "loss on batch 94 in epoch 55 for the simple nn is: 0.4115106761455536\n",
            "loss on batch 95 in epoch 55 for the simple nn is: 0.5231220722198486\n",
            "loss on batch 96 in epoch 55 for the simple nn is: 0.4959639608860016\n",
            "loss on batch 97 in epoch 55 for the simple nn is: 0.5214237570762634\n",
            "loss on batch 98 in epoch 55 for the simple nn is: 0.4591321349143982\n",
            "loss on batch 99 in epoch 55 for the simple nn is: 0.4695919156074524\n",
            "loss on batch 100 in epoch 55 for the simple nn is: 0.4952062666416168\n",
            "loss on batch 101 in epoch 55 for the simple nn is: 0.41726019978523254\n",
            "loss on batch 102 in epoch 55 for the simple nn is: 0.5058576464653015\n",
            "loss on batch 103 in epoch 55 for the simple nn is: 0.5485387444496155\n",
            "loss on batch 104 in epoch 55 for the simple nn is: 0.5608038902282715\n",
            "loss on batch 105 in epoch 55 for the simple nn is: 0.42969539761543274\n",
            "loss on batch 106 in epoch 55 for the simple nn is: 0.4625207483768463\n",
            "loss on batch 107 in epoch 55 for the simple nn is: 0.47066110372543335\n",
            "loss on batch 108 in epoch 55 for the simple nn is: 0.4857015609741211\n",
            "loss on batch 109 in epoch 55 for the simple nn is: 0.5462173819541931\n",
            "loss on batch 110 in epoch 55 for the simple nn is: 0.3733922839164734\n",
            "loss on batch 111 in epoch 55 for the simple nn is: 0.4510504901409149\n",
            "loss on batch 112 in epoch 55 for the simple nn is: 0.5123144388198853\n",
            "loss on batch 113 in epoch 55 for the simple nn is: 0.5268781185150146\n",
            "loss on batch 114 in epoch 55 for the simple nn is: 0.5308745503425598\n",
            "loss on batch 115 in epoch 55 for the simple nn is: 0.4665268063545227\n",
            "loss on batch 116 in epoch 55 for the simple nn is: 0.5896402597427368\n",
            "loss on batch 117 in epoch 55 for the simple nn is: 0.5390895009040833\n",
            "loss on batch 118 in epoch 55 for the simple nn is: 0.5635921359062195\n",
            "loss on batch 119 in epoch 55 for the simple nn is: 0.5212451815605164\n",
            "loss on batch 120 in epoch 55 for the simple nn is: 0.5840725302696228\n",
            "loss on batch 0 in epoch 56 for the simple nn is: 0.47553443908691406\n",
            "loss on batch 1 in epoch 56 for the simple nn is: 0.6127290725708008\n",
            "loss on batch 2 in epoch 56 for the simple nn is: 0.6024990677833557\n",
            "loss on batch 3 in epoch 56 for the simple nn is: 0.6446776986122131\n",
            "loss on batch 4 in epoch 56 for the simple nn is: 0.5586606860160828\n",
            "loss on batch 5 in epoch 56 for the simple nn is: 0.7738425731658936\n",
            "loss on batch 6 in epoch 56 for the simple nn is: 0.5892969369888306\n",
            "loss on batch 7 in epoch 56 for the simple nn is: 0.5320805311203003\n",
            "loss on batch 8 in epoch 56 for the simple nn is: 0.5832327008247375\n",
            "loss on batch 9 in epoch 56 for the simple nn is: 0.6112207174301147\n",
            "loss on batch 10 in epoch 56 for the simple nn is: 0.43535518646240234\n",
            "loss on batch 11 in epoch 56 for the simple nn is: 0.5175480842590332\n",
            "loss on batch 12 in epoch 56 for the simple nn is: 0.5352758169174194\n",
            "loss on batch 13 in epoch 56 for the simple nn is: 0.5304043292999268\n",
            "loss on batch 14 in epoch 56 for the simple nn is: 0.5272302031517029\n",
            "loss on batch 15 in epoch 56 for the simple nn is: 0.467465877532959\n",
            "loss on batch 16 in epoch 56 for the simple nn is: 0.5383256077766418\n",
            "loss on batch 17 in epoch 56 for the simple nn is: 0.5073758363723755\n",
            "loss on batch 18 in epoch 56 for the simple nn is: 0.4759798049926758\n",
            "loss on batch 19 in epoch 56 for the simple nn is: 0.5305449962615967\n",
            "loss on batch 20 in epoch 56 for the simple nn is: 0.6341516971588135\n",
            "loss on batch 21 in epoch 56 for the simple nn is: 0.5259341597557068\n",
            "loss on batch 22 in epoch 56 for the simple nn is: 0.4833507537841797\n",
            "loss on batch 23 in epoch 56 for the simple nn is: 0.5323284268379211\n",
            "loss on batch 24 in epoch 56 for the simple nn is: 0.4475851058959961\n",
            "loss on batch 25 in epoch 56 for the simple nn is: 0.45961371064186096\n",
            "loss on batch 26 in epoch 56 for the simple nn is: 0.5279398560523987\n",
            "loss on batch 27 in epoch 56 for the simple nn is: 0.5235323905944824\n",
            "loss on batch 28 in epoch 56 for the simple nn is: 0.513448178768158\n",
            "loss on batch 29 in epoch 56 for the simple nn is: 0.6128823757171631\n",
            "loss on batch 30 in epoch 56 for the simple nn is: 0.48903757333755493\n",
            "loss on batch 31 in epoch 56 for the simple nn is: 0.5566840171813965\n",
            "loss on batch 32 in epoch 56 for the simple nn is: 0.4579218924045563\n",
            "loss on batch 33 in epoch 56 for the simple nn is: 0.5165563821792603\n",
            "loss on batch 34 in epoch 56 for the simple nn is: 0.48969849944114685\n",
            "loss on batch 35 in epoch 56 for the simple nn is: 0.49134233593940735\n",
            "loss on batch 36 in epoch 56 for the simple nn is: 0.5656396746635437\n",
            "loss on batch 37 in epoch 56 for the simple nn is: 0.44010818004608154\n",
            "loss on batch 38 in epoch 56 for the simple nn is: 0.5573666095733643\n",
            "loss on batch 39 in epoch 56 for the simple nn is: 0.4604010581970215\n",
            "loss on batch 40 in epoch 56 for the simple nn is: 0.5600085854530334\n",
            "loss on batch 41 in epoch 56 for the simple nn is: 0.42298397421836853\n",
            "loss on batch 42 in epoch 56 for the simple nn is: 0.548855721950531\n",
            "loss on batch 43 in epoch 56 for the simple nn is: 0.5368509888648987\n",
            "loss on batch 44 in epoch 56 for the simple nn is: 0.5598750710487366\n",
            "loss on batch 45 in epoch 56 for the simple nn is: 0.5182490348815918\n",
            "loss on batch 46 in epoch 56 for the simple nn is: 0.5043356418609619\n",
            "loss on batch 47 in epoch 56 for the simple nn is: 0.4966857135295868\n",
            "loss on batch 48 in epoch 56 for the simple nn is: 0.4903615117073059\n",
            "loss on batch 49 in epoch 56 for the simple nn is: 0.5631189346313477\n",
            "loss on batch 50 in epoch 56 for the simple nn is: 0.5084624886512756\n",
            "loss on batch 51 in epoch 56 for the simple nn is: 0.5222966074943542\n",
            "loss on batch 52 in epoch 56 for the simple nn is: 0.43424880504608154\n",
            "loss on batch 53 in epoch 56 for the simple nn is: 0.3890048861503601\n",
            "loss on batch 54 in epoch 56 for the simple nn is: 0.574006199836731\n",
            "loss on batch 55 in epoch 56 for the simple nn is: 0.5171780586242676\n",
            "loss on batch 56 in epoch 56 for the simple nn is: 0.5619958639144897\n",
            "loss on batch 57 in epoch 56 for the simple nn is: 0.5179861187934875\n",
            "loss on batch 58 in epoch 56 for the simple nn is: 0.47397151589393616\n",
            "loss on batch 59 in epoch 56 for the simple nn is: 0.539165198802948\n",
            "loss on batch 60 in epoch 56 for the simple nn is: 0.48824542760849\n",
            "loss on batch 61 in epoch 56 for the simple nn is: 0.6595711708068848\n",
            "loss on batch 62 in epoch 56 for the simple nn is: 0.49306392669677734\n",
            "loss on batch 63 in epoch 56 for the simple nn is: 0.5499863028526306\n",
            "loss on batch 64 in epoch 56 for the simple nn is: 0.447162926197052\n",
            "loss on batch 65 in epoch 56 for the simple nn is: 0.4337087869644165\n",
            "loss on batch 66 in epoch 56 for the simple nn is: 0.5194936990737915\n",
            "loss on batch 67 in epoch 56 for the simple nn is: 0.5160644054412842\n",
            "loss on batch 68 in epoch 56 for the simple nn is: 0.5106011629104614\n",
            "loss on batch 69 in epoch 56 for the simple nn is: 0.5207207202911377\n",
            "loss on batch 70 in epoch 56 for the simple nn is: 0.5340535044670105\n",
            "loss on batch 71 in epoch 56 for the simple nn is: 0.5378882884979248\n",
            "loss on batch 72 in epoch 56 for the simple nn is: 0.5608139634132385\n",
            "loss on batch 73 in epoch 56 for the simple nn is: 0.5710129141807556\n",
            "loss on batch 74 in epoch 56 for the simple nn is: 0.5264742970466614\n",
            "loss on batch 75 in epoch 56 for the simple nn is: 0.5444934964179993\n",
            "loss on batch 76 in epoch 56 for the simple nn is: 0.5329331159591675\n",
            "loss on batch 77 in epoch 56 for the simple nn is: 0.47402626276016235\n",
            "loss on batch 78 in epoch 56 for the simple nn is: 0.5600507855415344\n",
            "loss on batch 79 in epoch 56 for the simple nn is: 0.455154150724411\n",
            "loss on batch 80 in epoch 56 for the simple nn is: 0.5071187615394592\n",
            "loss on batch 81 in epoch 56 for the simple nn is: 0.44821158051490784\n",
            "loss on batch 82 in epoch 56 for the simple nn is: 0.5103227496147156\n",
            "loss on batch 83 in epoch 56 for the simple nn is: 0.5864137411117554\n",
            "loss on batch 84 in epoch 56 for the simple nn is: 0.49291715025901794\n",
            "loss on batch 85 in epoch 56 for the simple nn is: 0.5415191650390625\n",
            "loss on batch 86 in epoch 56 for the simple nn is: 0.48236083984375\n",
            "loss on batch 87 in epoch 56 for the simple nn is: 0.546967625617981\n",
            "loss on batch 88 in epoch 56 for the simple nn is: 0.43175768852233887\n",
            "loss on batch 89 in epoch 56 for the simple nn is: 0.5630661845207214\n",
            "loss on batch 90 in epoch 56 for the simple nn is: 0.568989098072052\n",
            "loss on batch 91 in epoch 56 for the simple nn is: 0.48288556933403015\n",
            "loss on batch 92 in epoch 56 for the simple nn is: 0.5776664614677429\n",
            "loss on batch 93 in epoch 56 for the simple nn is: 0.5117244124412537\n",
            "loss on batch 94 in epoch 56 for the simple nn is: 0.47326427698135376\n",
            "loss on batch 95 in epoch 56 for the simple nn is: 0.549486517906189\n",
            "loss on batch 96 in epoch 56 for the simple nn is: 0.5060688257217407\n",
            "loss on batch 97 in epoch 56 for the simple nn is: 0.5594912767410278\n",
            "loss on batch 98 in epoch 56 for the simple nn is: 0.4766087234020233\n",
            "loss on batch 99 in epoch 56 for the simple nn is: 0.46617811918258667\n",
            "loss on batch 100 in epoch 56 for the simple nn is: 0.48590192198753357\n",
            "loss on batch 101 in epoch 56 for the simple nn is: 0.6418578028678894\n",
            "loss on batch 102 in epoch 56 for the simple nn is: 0.4194141626358032\n",
            "loss on batch 103 in epoch 56 for the simple nn is: 0.5413886904716492\n",
            "loss on batch 104 in epoch 56 for the simple nn is: 0.5080764293670654\n",
            "loss on batch 105 in epoch 56 for the simple nn is: 0.4486145079135895\n",
            "loss on batch 106 in epoch 56 for the simple nn is: 0.47881990671157837\n",
            "loss on batch 107 in epoch 56 for the simple nn is: 0.48612767457962036\n",
            "loss on batch 108 in epoch 56 for the simple nn is: 0.4805136024951935\n",
            "loss on batch 109 in epoch 56 for the simple nn is: 0.5891388058662415\n",
            "loss on batch 110 in epoch 56 for the simple nn is: 0.5384787917137146\n",
            "loss on batch 111 in epoch 56 for the simple nn is: 0.4761807322502136\n",
            "loss on batch 112 in epoch 56 for the simple nn is: 0.5466751456260681\n",
            "loss on batch 113 in epoch 56 for the simple nn is: 0.7971110343933105\n",
            "loss on batch 114 in epoch 56 for the simple nn is: 0.5560722351074219\n",
            "loss on batch 115 in epoch 56 for the simple nn is: 0.4962685704231262\n",
            "loss on batch 116 in epoch 56 for the simple nn is: 0.5996024012565613\n",
            "loss on batch 117 in epoch 56 for the simple nn is: 0.5813959836959839\n",
            "loss on batch 118 in epoch 56 for the simple nn is: 0.5971280932426453\n",
            "loss on batch 119 in epoch 56 for the simple nn is: 0.6495476365089417\n",
            "loss on batch 120 in epoch 56 for the simple nn is: 0.5097315311431885\n",
            "loss on batch 0 in epoch 57 for the simple nn is: 0.5351706743240356\n",
            "loss on batch 1 in epoch 57 for the simple nn is: 0.6047912836074829\n",
            "loss on batch 2 in epoch 57 for the simple nn is: 0.5790950059890747\n",
            "loss on batch 3 in epoch 57 for the simple nn is: 0.5817607045173645\n",
            "loss on batch 4 in epoch 57 for the simple nn is: 0.8427395224571228\n",
            "loss on batch 5 in epoch 57 for the simple nn is: 0.9142506718635559\n",
            "loss on batch 6 in epoch 57 for the simple nn is: 0.5435823798179626\n",
            "loss on batch 7 in epoch 57 for the simple nn is: 0.5566285848617554\n",
            "loss on batch 8 in epoch 57 for the simple nn is: 0.5041843056678772\n",
            "loss on batch 9 in epoch 57 for the simple nn is: 0.4998060464859009\n",
            "loss on batch 10 in epoch 57 for the simple nn is: 0.4732966721057892\n",
            "loss on batch 11 in epoch 57 for the simple nn is: 0.5269426703453064\n",
            "loss on batch 12 in epoch 57 for the simple nn is: 0.5540820956230164\n",
            "loss on batch 13 in epoch 57 for the simple nn is: 0.5929359197616577\n",
            "loss on batch 14 in epoch 57 for the simple nn is: 0.5313075184822083\n",
            "loss on batch 15 in epoch 57 for the simple nn is: 0.5456410646438599\n",
            "loss on batch 16 in epoch 57 for the simple nn is: 0.7064617276191711\n",
            "loss on batch 17 in epoch 57 for the simple nn is: 0.8432588577270508\n",
            "loss on batch 18 in epoch 57 for the simple nn is: 0.4753352105617523\n",
            "loss on batch 19 in epoch 57 for the simple nn is: 0.5135228037834167\n",
            "loss on batch 20 in epoch 57 for the simple nn is: 0.49621742963790894\n",
            "loss on batch 21 in epoch 57 for the simple nn is: 0.5353450775146484\n",
            "loss on batch 22 in epoch 57 for the simple nn is: 0.5422148108482361\n",
            "loss on batch 23 in epoch 57 for the simple nn is: 0.5568259954452515\n",
            "loss on batch 24 in epoch 57 for the simple nn is: 0.5015735030174255\n",
            "loss on batch 25 in epoch 57 for the simple nn is: 0.48252391815185547\n",
            "loss on batch 26 in epoch 57 for the simple nn is: 0.6420137286186218\n",
            "loss on batch 27 in epoch 57 for the simple nn is: 0.5448270440101624\n",
            "loss on batch 28 in epoch 57 for the simple nn is: 0.5029513835906982\n",
            "loss on batch 29 in epoch 57 for the simple nn is: 0.6431355476379395\n",
            "loss on batch 30 in epoch 57 for the simple nn is: 0.5340783596038818\n",
            "loss on batch 31 in epoch 57 for the simple nn is: 0.5807943940162659\n",
            "loss on batch 32 in epoch 57 for the simple nn is: 0.5023699998855591\n",
            "loss on batch 33 in epoch 57 for the simple nn is: 0.5374283194541931\n",
            "loss on batch 34 in epoch 57 for the simple nn is: 0.5125479102134705\n",
            "loss on batch 35 in epoch 57 for the simple nn is: 0.5222078561782837\n",
            "loss on batch 36 in epoch 57 for the simple nn is: 0.520007848739624\n",
            "loss on batch 37 in epoch 57 for the simple nn is: 0.5593451857566833\n",
            "loss on batch 38 in epoch 57 for the simple nn is: 0.6238687038421631\n",
            "loss on batch 39 in epoch 57 for the simple nn is: 0.48980897665023804\n",
            "loss on batch 40 in epoch 57 for the simple nn is: 0.5562161803245544\n",
            "loss on batch 41 in epoch 57 for the simple nn is: 0.4623398780822754\n",
            "loss on batch 42 in epoch 57 for the simple nn is: 0.5603487491607666\n",
            "loss on batch 43 in epoch 57 for the simple nn is: 0.5379379391670227\n",
            "loss on batch 44 in epoch 57 for the simple nn is: 0.5594631433486938\n",
            "loss on batch 45 in epoch 57 for the simple nn is: 0.551633358001709\n",
            "loss on batch 46 in epoch 57 for the simple nn is: 0.48687419295310974\n",
            "loss on batch 47 in epoch 57 for the simple nn is: 0.5316426753997803\n",
            "loss on batch 48 in epoch 57 for the simple nn is: 0.5281945466995239\n",
            "loss on batch 49 in epoch 57 for the simple nn is: 0.5546658039093018\n",
            "loss on batch 50 in epoch 57 for the simple nn is: 0.5540040731430054\n",
            "loss on batch 51 in epoch 57 for the simple nn is: 0.515860915184021\n",
            "loss on batch 52 in epoch 57 for the simple nn is: 0.4084741175174713\n",
            "loss on batch 53 in epoch 57 for the simple nn is: 0.3910590708255768\n",
            "loss on batch 54 in epoch 57 for the simple nn is: 0.7550034523010254\n",
            "loss on batch 55 in epoch 57 for the simple nn is: 0.5258042216300964\n",
            "loss on batch 56 in epoch 57 for the simple nn is: 0.6082324981689453\n",
            "loss on batch 57 in epoch 57 for the simple nn is: 0.5321334004402161\n",
            "loss on batch 58 in epoch 57 for the simple nn is: 0.48353007435798645\n",
            "loss on batch 59 in epoch 57 for the simple nn is: 0.5369954705238342\n",
            "loss on batch 60 in epoch 57 for the simple nn is: 0.46881410479545593\n",
            "loss on batch 61 in epoch 57 for the simple nn is: 0.739963173866272\n",
            "loss on batch 62 in epoch 57 for the simple nn is: 0.5408992767333984\n",
            "loss on batch 63 in epoch 57 for the simple nn is: 0.5044988393783569\n",
            "loss on batch 64 in epoch 57 for the simple nn is: 0.5367351770401001\n",
            "loss on batch 65 in epoch 57 for the simple nn is: 0.47712957859039307\n",
            "loss on batch 66 in epoch 57 for the simple nn is: 0.5079352855682373\n",
            "loss on batch 67 in epoch 57 for the simple nn is: 0.45913630723953247\n",
            "loss on batch 68 in epoch 57 for the simple nn is: 0.4965258538722992\n",
            "loss on batch 69 in epoch 57 for the simple nn is: 0.5326849818229675\n",
            "loss on batch 70 in epoch 57 for the simple nn is: 0.530678391456604\n",
            "loss on batch 71 in epoch 57 for the simple nn is: 0.530589759349823\n",
            "loss on batch 72 in epoch 57 for the simple nn is: 0.6313353776931763\n",
            "loss on batch 73 in epoch 57 for the simple nn is: 0.5386015772819519\n",
            "loss on batch 74 in epoch 57 for the simple nn is: 0.5237811803817749\n",
            "loss on batch 75 in epoch 57 for the simple nn is: 0.5245968699455261\n",
            "loss on batch 76 in epoch 57 for the simple nn is: 0.49812614917755127\n",
            "loss on batch 77 in epoch 57 for the simple nn is: 0.5104213953018188\n",
            "loss on batch 78 in epoch 57 for the simple nn is: 0.5388056039810181\n",
            "loss on batch 79 in epoch 57 for the simple nn is: 0.4629601538181305\n",
            "loss on batch 80 in epoch 57 for the simple nn is: 0.5033577680587769\n",
            "loss on batch 81 in epoch 57 for the simple nn is: 0.4309554398059845\n",
            "loss on batch 82 in epoch 57 for the simple nn is: 0.5196492671966553\n",
            "loss on batch 83 in epoch 57 for the simple nn is: 0.5839505791664124\n",
            "loss on batch 84 in epoch 57 for the simple nn is: 0.5099997520446777\n",
            "loss on batch 85 in epoch 57 for the simple nn is: 0.5272225141525269\n",
            "loss on batch 86 in epoch 57 for the simple nn is: 0.4322507977485657\n",
            "loss on batch 87 in epoch 57 for the simple nn is: 0.5113488435745239\n",
            "loss on batch 88 in epoch 57 for the simple nn is: 0.4029310941696167\n",
            "loss on batch 89 in epoch 57 for the simple nn is: 0.5385764241218567\n",
            "loss on batch 90 in epoch 57 for the simple nn is: 0.5878444910049438\n",
            "loss on batch 91 in epoch 57 for the simple nn is: 0.46928080916404724\n",
            "loss on batch 92 in epoch 57 for the simple nn is: 0.5581947565078735\n",
            "loss on batch 93 in epoch 57 for the simple nn is: 0.48238328099250793\n",
            "loss on batch 94 in epoch 57 for the simple nn is: 0.41345545649528503\n",
            "loss on batch 95 in epoch 57 for the simple nn is: 0.5122296810150146\n",
            "loss on batch 96 in epoch 57 for the simple nn is: 0.43705517053604126\n",
            "loss on batch 97 in epoch 57 for the simple nn is: 0.5114887952804565\n",
            "loss on batch 98 in epoch 57 for the simple nn is: 0.4551023840904236\n",
            "loss on batch 99 in epoch 57 for the simple nn is: 0.48939070105552673\n",
            "loss on batch 100 in epoch 57 for the simple nn is: 0.4858578145503998\n",
            "loss on batch 101 in epoch 57 for the simple nn is: 0.41609659790992737\n",
            "loss on batch 102 in epoch 57 for the simple nn is: 0.4307965040206909\n",
            "loss on batch 103 in epoch 57 for the simple nn is: 0.5991026163101196\n",
            "loss on batch 104 in epoch 57 for the simple nn is: 0.4552571773529053\n",
            "loss on batch 105 in epoch 57 for the simple nn is: 0.414899617433548\n",
            "loss on batch 106 in epoch 57 for the simple nn is: 0.4588799476623535\n",
            "loss on batch 107 in epoch 57 for the simple nn is: 0.4817337095737457\n",
            "loss on batch 108 in epoch 57 for the simple nn is: 0.4433576464653015\n",
            "loss on batch 109 in epoch 57 for the simple nn is: 0.5490975975990295\n",
            "loss on batch 110 in epoch 57 for the simple nn is: 0.37981826066970825\n",
            "loss on batch 111 in epoch 57 for the simple nn is: 0.48922133445739746\n",
            "loss on batch 112 in epoch 57 for the simple nn is: 0.6020691394805908\n",
            "loss on batch 113 in epoch 57 for the simple nn is: 0.5734929442405701\n",
            "loss on batch 114 in epoch 57 for the simple nn is: 0.5785588026046753\n",
            "loss on batch 115 in epoch 57 for the simple nn is: 0.45124122500419617\n",
            "loss on batch 116 in epoch 57 for the simple nn is: 0.6154231429100037\n",
            "loss on batch 117 in epoch 57 for the simple nn is: 0.5910986065864563\n",
            "loss on batch 118 in epoch 57 for the simple nn is: 0.5765408277511597\n",
            "loss on batch 119 in epoch 57 for the simple nn is: 0.5388302803039551\n",
            "loss on batch 120 in epoch 57 for the simple nn is: 0.5289657115936279\n",
            "loss on batch 0 in epoch 58 for the simple nn is: 0.5034134387969971\n",
            "loss on batch 1 in epoch 58 for the simple nn is: 0.595817506313324\n",
            "loss on batch 2 in epoch 58 for the simple nn is: 0.6013364195823669\n",
            "loss on batch 3 in epoch 58 for the simple nn is: 0.5787432193756104\n",
            "loss on batch 4 in epoch 58 for the simple nn is: 0.5718153119087219\n",
            "loss on batch 5 in epoch 58 for the simple nn is: 0.6219160556793213\n",
            "loss on batch 6 in epoch 58 for the simple nn is: 0.5274213552474976\n",
            "loss on batch 7 in epoch 58 for the simple nn is: 0.5462916493415833\n",
            "loss on batch 8 in epoch 58 for the simple nn is: 0.513576865196228\n",
            "loss on batch 9 in epoch 58 for the simple nn is: 0.5070212483406067\n",
            "loss on batch 10 in epoch 58 for the simple nn is: 0.44344067573547363\n",
            "loss on batch 11 in epoch 58 for the simple nn is: 0.522728443145752\n",
            "loss on batch 12 in epoch 58 for the simple nn is: 0.548078179359436\n",
            "loss on batch 13 in epoch 58 for the simple nn is: 0.5485926270484924\n",
            "loss on batch 14 in epoch 58 for the simple nn is: 0.547264575958252\n",
            "loss on batch 15 in epoch 58 for the simple nn is: 0.5653829574584961\n",
            "loss on batch 16 in epoch 58 for the simple nn is: 0.5066739320755005\n",
            "loss on batch 17 in epoch 58 for the simple nn is: 0.5037745237350464\n",
            "loss on batch 18 in epoch 58 for the simple nn is: 0.48530757427215576\n",
            "loss on batch 19 in epoch 58 for the simple nn is: 0.48975110054016113\n",
            "loss on batch 20 in epoch 58 for the simple nn is: 0.5142996907234192\n",
            "loss on batch 21 in epoch 58 for the simple nn is: 0.5355657935142517\n",
            "loss on batch 22 in epoch 58 for the simple nn is: 0.4934390187263489\n",
            "loss on batch 23 in epoch 58 for the simple nn is: 0.5149523019790649\n",
            "loss on batch 24 in epoch 58 for the simple nn is: 0.43794041872024536\n",
            "loss on batch 25 in epoch 58 for the simple nn is: 0.46040236949920654\n",
            "loss on batch 26 in epoch 58 for the simple nn is: 0.5290692448616028\n",
            "loss on batch 27 in epoch 58 for the simple nn is: 0.7068098783493042\n",
            "loss on batch 28 in epoch 58 for the simple nn is: 0.48522794246673584\n",
            "loss on batch 29 in epoch 58 for the simple nn is: 0.5846318602561951\n",
            "loss on batch 30 in epoch 58 for the simple nn is: 0.4908706545829773\n",
            "loss on batch 31 in epoch 58 for the simple nn is: 0.5231115818023682\n",
            "loss on batch 32 in epoch 58 for the simple nn is: 0.45573413372039795\n",
            "loss on batch 33 in epoch 58 for the simple nn is: 0.48470160365104675\n",
            "loss on batch 34 in epoch 58 for the simple nn is: 0.5046820044517517\n",
            "loss on batch 35 in epoch 58 for the simple nn is: 0.5620413422584534\n",
            "loss on batch 36 in epoch 58 for the simple nn is: 0.5188161134719849\n",
            "loss on batch 37 in epoch 58 for the simple nn is: 0.48046982288360596\n",
            "loss on batch 38 in epoch 58 for the simple nn is: 0.6790931820869446\n",
            "loss on batch 39 in epoch 58 for the simple nn is: 0.582220733165741\n",
            "loss on batch 40 in epoch 58 for the simple nn is: 0.5512115359306335\n",
            "loss on batch 41 in epoch 58 for the simple nn is: 0.44777712225914\n",
            "loss on batch 42 in epoch 58 for the simple nn is: 0.5427494645118713\n",
            "loss on batch 43 in epoch 58 for the simple nn is: 0.5114679336547852\n",
            "loss on batch 44 in epoch 58 for the simple nn is: 0.5156248807907104\n",
            "loss on batch 45 in epoch 58 for the simple nn is: 0.526223361492157\n",
            "loss on batch 46 in epoch 58 for the simple nn is: 0.45029348134994507\n",
            "loss on batch 47 in epoch 58 for the simple nn is: 0.5119715929031372\n",
            "loss on batch 48 in epoch 58 for the simple nn is: 0.4967402219772339\n",
            "loss on batch 49 in epoch 58 for the simple nn is: 0.5258780121803284\n",
            "loss on batch 50 in epoch 58 for the simple nn is: 0.5001177191734314\n",
            "loss on batch 51 in epoch 58 for the simple nn is: 0.6017804741859436\n",
            "loss on batch 52 in epoch 58 for the simple nn is: 0.42507776618003845\n",
            "loss on batch 53 in epoch 58 for the simple nn is: 0.4179263710975647\n",
            "loss on batch 54 in epoch 58 for the simple nn is: 0.5653237700462341\n",
            "loss on batch 55 in epoch 58 for the simple nn is: 0.5097315311431885\n",
            "loss on batch 56 in epoch 58 for the simple nn is: 0.5487797260284424\n",
            "loss on batch 57 in epoch 58 for the simple nn is: 0.5365293622016907\n",
            "loss on batch 58 in epoch 58 for the simple nn is: 0.5503446459770203\n",
            "loss on batch 59 in epoch 58 for the simple nn is: 0.5117987394332886\n",
            "loss on batch 60 in epoch 58 for the simple nn is: 0.5406794548034668\n",
            "loss on batch 61 in epoch 58 for the simple nn is: 0.45679083466529846\n",
            "loss on batch 62 in epoch 58 for the simple nn is: 0.49816903471946716\n",
            "loss on batch 63 in epoch 58 for the simple nn is: 0.4992300271987915\n",
            "loss on batch 64 in epoch 58 for the simple nn is: 0.4461866319179535\n",
            "loss on batch 65 in epoch 58 for the simple nn is: 0.4294206500053406\n",
            "loss on batch 66 in epoch 58 for the simple nn is: 0.5090929865837097\n",
            "loss on batch 67 in epoch 58 for the simple nn is: 0.4282950460910797\n",
            "loss on batch 68 in epoch 58 for the simple nn is: 0.4604150354862213\n",
            "loss on batch 69 in epoch 58 for the simple nn is: 0.48749756813049316\n",
            "loss on batch 70 in epoch 58 for the simple nn is: 0.49886393547058105\n",
            "loss on batch 71 in epoch 58 for the simple nn is: 0.4857870936393738\n",
            "loss on batch 72 in epoch 58 for the simple nn is: 0.4826999604701996\n",
            "loss on batch 73 in epoch 58 for the simple nn is: 0.6504905819892883\n",
            "loss on batch 74 in epoch 58 for the simple nn is: 0.5021373629570007\n",
            "loss on batch 75 in epoch 58 for the simple nn is: 0.49950388073921204\n",
            "loss on batch 76 in epoch 58 for the simple nn is: 0.5531034469604492\n",
            "loss on batch 77 in epoch 58 for the simple nn is: 0.44826939702033997\n",
            "loss on batch 78 in epoch 58 for the simple nn is: 0.5094580054283142\n",
            "loss on batch 79 in epoch 58 for the simple nn is: 0.43657854199409485\n",
            "loss on batch 80 in epoch 58 for the simple nn is: 0.48369795083999634\n",
            "loss on batch 81 in epoch 58 for the simple nn is: 0.4358086585998535\n",
            "loss on batch 82 in epoch 58 for the simple nn is: 0.49756237864494324\n",
            "loss on batch 83 in epoch 58 for the simple nn is: 0.5332276821136475\n",
            "loss on batch 84 in epoch 58 for the simple nn is: 0.48977896571159363\n",
            "loss on batch 85 in epoch 58 for the simple nn is: 0.5122607946395874\n",
            "loss on batch 86 in epoch 58 for the simple nn is: 0.48108065128326416\n",
            "loss on batch 87 in epoch 58 for the simple nn is: 0.49258506298065186\n",
            "loss on batch 88 in epoch 58 for the simple nn is: 0.4228666424751282\n",
            "loss on batch 89 in epoch 58 for the simple nn is: 0.5215216279029846\n",
            "loss on batch 90 in epoch 58 for the simple nn is: 0.47340989112854004\n",
            "loss on batch 91 in epoch 58 for the simple nn is: 0.4586859345436096\n",
            "loss on batch 92 in epoch 58 for the simple nn is: 0.565733015537262\n",
            "loss on batch 93 in epoch 58 for the simple nn is: 0.4945594072341919\n",
            "loss on batch 94 in epoch 58 for the simple nn is: 0.3919300138950348\n",
            "loss on batch 95 in epoch 58 for the simple nn is: 0.5046905279159546\n",
            "loss on batch 96 in epoch 58 for the simple nn is: 0.4255047142505646\n",
            "loss on batch 97 in epoch 58 for the simple nn is: 0.4982975423336029\n",
            "loss on batch 98 in epoch 58 for the simple nn is: 0.4906167685985565\n",
            "loss on batch 99 in epoch 58 for the simple nn is: 0.47683262825012207\n",
            "loss on batch 100 in epoch 58 for the simple nn is: 0.4847055971622467\n",
            "loss on batch 101 in epoch 58 for the simple nn is: 0.4139253497123718\n",
            "loss on batch 102 in epoch 58 for the simple nn is: 0.3768000900745392\n",
            "loss on batch 103 in epoch 58 for the simple nn is: 0.523502767086029\n",
            "loss on batch 104 in epoch 58 for the simple nn is: 0.501380443572998\n",
            "loss on batch 105 in epoch 58 for the simple nn is: 0.4249594211578369\n",
            "loss on batch 106 in epoch 58 for the simple nn is: 0.4648345112800598\n",
            "loss on batch 107 in epoch 58 for the simple nn is: 0.4369998574256897\n",
            "loss on batch 108 in epoch 58 for the simple nn is: 0.4438169598579407\n",
            "loss on batch 109 in epoch 58 for the simple nn is: 0.5507219433784485\n",
            "loss on batch 110 in epoch 58 for the simple nn is: 0.3452388346195221\n",
            "loss on batch 111 in epoch 58 for the simple nn is: 0.4451586902141571\n",
            "loss on batch 112 in epoch 58 for the simple nn is: 0.5167185664176941\n",
            "loss on batch 113 in epoch 58 for the simple nn is: 0.5556483268737793\n",
            "loss on batch 114 in epoch 58 for the simple nn is: 0.534704327583313\n",
            "loss on batch 115 in epoch 58 for the simple nn is: 0.45247212052345276\n",
            "loss on batch 116 in epoch 58 for the simple nn is: 0.5914813280105591\n",
            "loss on batch 117 in epoch 58 for the simple nn is: 0.5471327304840088\n",
            "loss on batch 118 in epoch 58 for the simple nn is: 0.5604444742202759\n",
            "loss on batch 119 in epoch 58 for the simple nn is: 0.5346310138702393\n",
            "loss on batch 120 in epoch 58 for the simple nn is: 0.5131546258926392\n",
            "loss on batch 0 in epoch 59 for the simple nn is: 0.4820781946182251\n",
            "loss on batch 1 in epoch 59 for the simple nn is: 0.5972913503646851\n",
            "loss on batch 2 in epoch 59 for the simple nn is: 0.5725012421607971\n",
            "loss on batch 3 in epoch 59 for the simple nn is: 0.5425312519073486\n",
            "loss on batch 4 in epoch 59 for the simple nn is: 0.5505911111831665\n",
            "loss on batch 5 in epoch 59 for the simple nn is: 0.5818739533424377\n",
            "loss on batch 6 in epoch 59 for the simple nn is: 0.5721609592437744\n",
            "loss on batch 7 in epoch 59 for the simple nn is: 0.6058064103126526\n",
            "loss on batch 8 in epoch 59 for the simple nn is: 0.4508817791938782\n",
            "loss on batch 9 in epoch 59 for the simple nn is: 0.48896437883377075\n",
            "loss on batch 10 in epoch 59 for the simple nn is: 0.4449542760848999\n",
            "loss on batch 11 in epoch 59 for the simple nn is: 0.5043536424636841\n",
            "loss on batch 12 in epoch 59 for the simple nn is: 0.5199525952339172\n",
            "loss on batch 13 in epoch 59 for the simple nn is: 0.5300552248954773\n",
            "loss on batch 14 in epoch 59 for the simple nn is: 0.5703136920928955\n",
            "loss on batch 15 in epoch 59 for the simple nn is: 0.45430848002433777\n",
            "loss on batch 16 in epoch 59 for the simple nn is: 0.5361246466636658\n",
            "loss on batch 17 in epoch 59 for the simple nn is: 0.4920071065425873\n",
            "loss on batch 18 in epoch 59 for the simple nn is: 0.47107502818107605\n",
            "loss on batch 19 in epoch 59 for the simple nn is: 0.4839318096637726\n",
            "loss on batch 20 in epoch 59 for the simple nn is: 0.5321448445320129\n",
            "loss on batch 21 in epoch 59 for the simple nn is: 0.5253471732139587\n",
            "loss on batch 22 in epoch 59 for the simple nn is: 0.6170434951782227\n",
            "loss on batch 23 in epoch 59 for the simple nn is: 0.49262210726737976\n",
            "loss on batch 24 in epoch 59 for the simple nn is: 0.4173348844051361\n",
            "loss on batch 25 in epoch 59 for the simple nn is: 0.48582902550697327\n",
            "loss on batch 26 in epoch 59 for the simple nn is: 0.5436062216758728\n",
            "loss on batch 27 in epoch 59 for the simple nn is: 0.49716174602508545\n",
            "loss on batch 28 in epoch 59 for the simple nn is: 0.501180112361908\n",
            "loss on batch 29 in epoch 59 for the simple nn is: 0.5761045217514038\n",
            "loss on batch 30 in epoch 59 for the simple nn is: 0.5424600839614868\n",
            "loss on batch 31 in epoch 59 for the simple nn is: 0.5259814262390137\n",
            "loss on batch 32 in epoch 59 for the simple nn is: 0.46964651346206665\n",
            "loss on batch 33 in epoch 59 for the simple nn is: 0.4804500341415405\n",
            "loss on batch 34 in epoch 59 for the simple nn is: 0.4472161531448364\n",
            "loss on batch 35 in epoch 59 for the simple nn is: 0.5063663125038147\n",
            "loss on batch 36 in epoch 59 for the simple nn is: 0.5080309510231018\n",
            "loss on batch 37 in epoch 59 for the simple nn is: 0.46499934792518616\n",
            "loss on batch 38 in epoch 59 for the simple nn is: 0.5502138137817383\n",
            "loss on batch 39 in epoch 59 for the simple nn is: 0.4393024444580078\n",
            "loss on batch 40 in epoch 59 for the simple nn is: 0.5314946174621582\n",
            "loss on batch 41 in epoch 59 for the simple nn is: 0.43154674768447876\n",
            "loss on batch 42 in epoch 59 for the simple nn is: 0.5008689165115356\n",
            "loss on batch 43 in epoch 59 for the simple nn is: 0.5180543065071106\n",
            "loss on batch 44 in epoch 59 for the simple nn is: 0.5206713676452637\n",
            "loss on batch 45 in epoch 59 for the simple nn is: 0.5036703944206238\n",
            "loss on batch 46 in epoch 59 for the simple nn is: 0.4388308823108673\n",
            "loss on batch 47 in epoch 59 for the simple nn is: 0.4701637029647827\n",
            "loss on batch 48 in epoch 59 for the simple nn is: 0.5276029706001282\n",
            "loss on batch 49 in epoch 59 for the simple nn is: 0.5277323722839355\n",
            "loss on batch 50 in epoch 59 for the simple nn is: 0.4793858528137207\n",
            "loss on batch 51 in epoch 59 for the simple nn is: 0.5387239456176758\n",
            "loss on batch 52 in epoch 59 for the simple nn is: 0.3996601998806\n",
            "loss on batch 53 in epoch 59 for the simple nn is: 0.38675034046173096\n",
            "loss on batch 54 in epoch 59 for the simple nn is: 0.5514761209487915\n",
            "loss on batch 55 in epoch 59 for the simple nn is: 0.48405781388282776\n",
            "loss on batch 56 in epoch 59 for the simple nn is: 0.5380799174308777\n",
            "loss on batch 57 in epoch 59 for the simple nn is: 0.504503071308136\n",
            "loss on batch 58 in epoch 59 for the simple nn is: 0.5712969303131104\n",
            "loss on batch 59 in epoch 59 for the simple nn is: 0.48919981718063354\n",
            "loss on batch 60 in epoch 59 for the simple nn is: 0.5668723583221436\n",
            "loss on batch 61 in epoch 59 for the simple nn is: 0.5516946911811829\n",
            "loss on batch 62 in epoch 59 for the simple nn is: 0.49134066700935364\n",
            "loss on batch 63 in epoch 59 for the simple nn is: 0.4935389757156372\n",
            "loss on batch 64 in epoch 59 for the simple nn is: 0.42768335342407227\n",
            "loss on batch 65 in epoch 59 for the simple nn is: 0.43427854776382446\n",
            "loss on batch 66 in epoch 59 for the simple nn is: 0.49525734782218933\n",
            "loss on batch 67 in epoch 59 for the simple nn is: 0.43387851119041443\n",
            "loss on batch 68 in epoch 59 for the simple nn is: 0.4864298701286316\n",
            "loss on batch 69 in epoch 59 for the simple nn is: 0.48746103048324585\n",
            "loss on batch 70 in epoch 59 for the simple nn is: 0.48577094078063965\n",
            "loss on batch 71 in epoch 59 for the simple nn is: 0.48013362288475037\n",
            "loss on batch 72 in epoch 59 for the simple nn is: 0.48429927229881287\n",
            "loss on batch 73 in epoch 59 for the simple nn is: 0.5348632335662842\n",
            "loss on batch 74 in epoch 59 for the simple nn is: 0.5121834874153137\n",
            "loss on batch 75 in epoch 59 for the simple nn is: 0.48785823583602905\n",
            "loss on batch 76 in epoch 59 for the simple nn is: 0.5000386238098145\n",
            "loss on batch 77 in epoch 59 for the simple nn is: 0.4336874485015869\n",
            "loss on batch 78 in epoch 59 for the simple nn is: 0.485658198595047\n",
            "loss on batch 79 in epoch 59 for the simple nn is: 0.44656655192375183\n",
            "loss on batch 80 in epoch 59 for the simple nn is: 0.5039259195327759\n",
            "loss on batch 81 in epoch 59 for the simple nn is: 0.4052157700061798\n",
            "loss on batch 82 in epoch 59 for the simple nn is: 0.4896336793899536\n",
            "loss on batch 83 in epoch 59 for the simple nn is: 0.5231057405471802\n",
            "loss on batch 84 in epoch 59 for the simple nn is: 0.48066446185112\n",
            "loss on batch 85 in epoch 59 for the simple nn is: 0.49872300028800964\n",
            "loss on batch 86 in epoch 59 for the simple nn is: 0.4066685438156128\n",
            "loss on batch 87 in epoch 59 for the simple nn is: 0.5251155495643616\n",
            "loss on batch 88 in epoch 59 for the simple nn is: 0.3893657624721527\n",
            "loss on batch 89 in epoch 59 for the simple nn is: 0.5087944269180298\n",
            "loss on batch 90 in epoch 59 for the simple nn is: 0.4584542512893677\n",
            "loss on batch 91 in epoch 59 for the simple nn is: 0.4566667973995209\n",
            "loss on batch 92 in epoch 59 for the simple nn is: 0.563186526298523\n",
            "loss on batch 93 in epoch 59 for the simple nn is: 0.46437567472457886\n",
            "loss on batch 94 in epoch 59 for the simple nn is: 0.5298632383346558\n",
            "loss on batch 95 in epoch 59 for the simple nn is: 0.4931081533432007\n",
            "loss on batch 96 in epoch 59 for the simple nn is: 0.409351110458374\n",
            "loss on batch 97 in epoch 59 for the simple nn is: 0.512884795665741\n",
            "loss on batch 98 in epoch 59 for the simple nn is: 0.43924492597579956\n",
            "loss on batch 99 in epoch 59 for the simple nn is: 0.4830090403556824\n",
            "loss on batch 100 in epoch 59 for the simple nn is: 0.4783971905708313\n",
            "loss on batch 101 in epoch 59 for the simple nn is: 0.4048478901386261\n",
            "loss on batch 102 in epoch 59 for the simple nn is: 0.38060200214385986\n",
            "loss on batch 103 in epoch 59 for the simple nn is: 0.5065601468086243\n",
            "loss on batch 104 in epoch 59 for the simple nn is: 0.46180784702301025\n",
            "loss on batch 105 in epoch 59 for the simple nn is: 0.48458945751190186\n",
            "loss on batch 106 in epoch 59 for the simple nn is: 0.4758535921573639\n",
            "loss on batch 107 in epoch 59 for the simple nn is: 0.4512439966201782\n",
            "loss on batch 108 in epoch 59 for the simple nn is: 0.4498043954372406\n",
            "loss on batch 109 in epoch 59 for the simple nn is: 0.548194944858551\n",
            "loss on batch 110 in epoch 59 for the simple nn is: 0.3750113248825073\n",
            "loss on batch 111 in epoch 59 for the simple nn is: 0.44607383012771606\n",
            "loss on batch 112 in epoch 59 for the simple nn is: 0.5416597723960876\n",
            "loss on batch 113 in epoch 59 for the simple nn is: 0.5410606861114502\n",
            "loss on batch 114 in epoch 59 for the simple nn is: 0.5301213264465332\n",
            "loss on batch 115 in epoch 59 for the simple nn is: 0.4543910622596741\n",
            "loss on batch 116 in epoch 59 for the simple nn is: 0.5891700387001038\n",
            "loss on batch 117 in epoch 59 for the simple nn is: 0.5317582488059998\n",
            "loss on batch 118 in epoch 59 for the simple nn is: 0.6256257891654968\n",
            "loss on batch 119 in epoch 59 for the simple nn is: 0.5348098874092102\n",
            "loss on batch 120 in epoch 59 for the simple nn is: 0.7312682271003723\n",
            "loss on batch 0 in epoch 60 for the simple nn is: 0.49316415190696716\n",
            "loss on batch 1 in epoch 60 for the simple nn is: 0.5803037881851196\n",
            "loss on batch 2 in epoch 60 for the simple nn is: 0.5627027153968811\n",
            "loss on batch 3 in epoch 60 for the simple nn is: 0.5429155230522156\n",
            "loss on batch 4 in epoch 60 for the simple nn is: 0.5518143177032471\n",
            "loss on batch 5 in epoch 60 for the simple nn is: 0.5948261618614197\n",
            "loss on batch 6 in epoch 60 for the simple nn is: 0.5356187224388123\n",
            "loss on batch 7 in epoch 60 for the simple nn is: 0.5220396518707275\n",
            "loss on batch 8 in epoch 60 for the simple nn is: 0.4779641628265381\n",
            "loss on batch 9 in epoch 60 for the simple nn is: 0.47646427154541016\n",
            "loss on batch 10 in epoch 60 for the simple nn is: 0.4452143907546997\n",
            "loss on batch 11 in epoch 60 for the simple nn is: 0.5377154350280762\n",
            "loss on batch 12 in epoch 60 for the simple nn is: 0.5111830234527588\n",
            "loss on batch 13 in epoch 60 for the simple nn is: 0.5184693932533264\n",
            "loss on batch 14 in epoch 60 for the simple nn is: 0.514587938785553\n",
            "loss on batch 15 in epoch 60 for the simple nn is: 0.4575140178203583\n",
            "loss on batch 16 in epoch 60 for the simple nn is: 0.5021787881851196\n",
            "loss on batch 17 in epoch 60 for the simple nn is: 0.4769513010978699\n",
            "loss on batch 18 in epoch 60 for the simple nn is: 0.5021385550498962\n",
            "loss on batch 19 in epoch 60 for the simple nn is: 0.5008944272994995\n",
            "loss on batch 20 in epoch 60 for the simple nn is: 0.4621056318283081\n",
            "loss on batch 21 in epoch 60 for the simple nn is: 0.526641309261322\n",
            "loss on batch 22 in epoch 60 for the simple nn is: 0.488720178604126\n",
            "loss on batch 23 in epoch 60 for the simple nn is: 0.4708644151687622\n",
            "loss on batch 24 in epoch 60 for the simple nn is: 0.41116976737976074\n",
            "loss on batch 25 in epoch 60 for the simple nn is: 0.45741787552833557\n",
            "loss on batch 26 in epoch 60 for the simple nn is: 0.6601629257202148\n",
            "loss on batch 27 in epoch 60 for the simple nn is: 0.49477216601371765\n",
            "loss on batch 28 in epoch 60 for the simple nn is: 0.46156415343284607\n",
            "loss on batch 29 in epoch 60 for the simple nn is: 0.5578130483627319\n",
            "loss on batch 30 in epoch 60 for the simple nn is: 0.4894881248474121\n",
            "loss on batch 31 in epoch 60 for the simple nn is: 0.5192551612854004\n",
            "loss on batch 32 in epoch 60 for the simple nn is: 0.4453822672367096\n",
            "loss on batch 33 in epoch 60 for the simple nn is: 0.47493186593055725\n",
            "loss on batch 34 in epoch 60 for the simple nn is: 0.4383853077888489\n",
            "loss on batch 35 in epoch 60 for the simple nn is: 0.5099328756332397\n",
            "loss on batch 36 in epoch 60 for the simple nn is: 0.48945313692092896\n",
            "loss on batch 37 in epoch 60 for the simple nn is: 0.4596390426158905\n",
            "loss on batch 38 in epoch 60 for the simple nn is: 0.5394687652587891\n",
            "loss on batch 39 in epoch 60 for the simple nn is: 0.43708550930023193\n",
            "loss on batch 40 in epoch 60 for the simple nn is: 0.5133333206176758\n",
            "loss on batch 41 in epoch 60 for the simple nn is: 0.4312669634819031\n",
            "loss on batch 42 in epoch 60 for the simple nn is: 0.4990589916706085\n",
            "loss on batch 43 in epoch 60 for the simple nn is: 0.5121390223503113\n",
            "loss on batch 44 in epoch 60 for the simple nn is: 0.5106553435325623\n",
            "loss on batch 45 in epoch 60 for the simple nn is: 0.49778836965560913\n",
            "loss on batch 46 in epoch 60 for the simple nn is: 0.43018004298210144\n",
            "loss on batch 47 in epoch 60 for the simple nn is: 0.46767908334732056\n",
            "loss on batch 48 in epoch 60 for the simple nn is: 0.4880754351615906\n",
            "loss on batch 49 in epoch 60 for the simple nn is: 0.5558412671089172\n",
            "loss on batch 50 in epoch 60 for the simple nn is: 0.4603414237499237\n",
            "loss on batch 51 in epoch 60 for the simple nn is: 0.5654816031455994\n",
            "loss on batch 52 in epoch 60 for the simple nn is: 0.39922666549682617\n",
            "loss on batch 53 in epoch 60 for the simple nn is: 0.3862050473690033\n",
            "loss on batch 54 in epoch 60 for the simple nn is: 0.5922272801399231\n",
            "loss on batch 55 in epoch 60 for the simple nn is: 0.4597945809364319\n",
            "loss on batch 56 in epoch 60 for the simple nn is: 0.5201027393341064\n",
            "loss on batch 57 in epoch 60 for the simple nn is: 0.4970672130584717\n",
            "loss on batch 58 in epoch 60 for the simple nn is: 0.45246291160583496\n",
            "loss on batch 59 in epoch 60 for the simple nn is: 0.4965735673904419\n",
            "loss on batch 60 in epoch 60 for the simple nn is: 0.48318353295326233\n",
            "loss on batch 61 in epoch 60 for the simple nn is: 0.44704630970954895\n",
            "loss on batch 62 in epoch 60 for the simple nn is: 0.483059287071228\n",
            "loss on batch 63 in epoch 60 for the simple nn is: 0.4951305389404297\n",
            "loss on batch 64 in epoch 60 for the simple nn is: 0.4153110682964325\n",
            "loss on batch 65 in epoch 60 for the simple nn is: 0.4476417601108551\n",
            "loss on batch 66 in epoch 60 for the simple nn is: 0.4999580681324005\n",
            "loss on batch 67 in epoch 60 for the simple nn is: 0.4367760419845581\n",
            "loss on batch 68 in epoch 60 for the simple nn is: 0.4444349408149719\n",
            "loss on batch 69 in epoch 60 for the simple nn is: 0.47347891330718994\n",
            "loss on batch 70 in epoch 60 for the simple nn is: 0.47506678104400635\n",
            "loss on batch 71 in epoch 60 for the simple nn is: 0.48047491908073425\n",
            "loss on batch 72 in epoch 60 for the simple nn is: 0.4834270477294922\n",
            "loss on batch 73 in epoch 60 for the simple nn is: 0.5345494151115417\n",
            "loss on batch 74 in epoch 60 for the simple nn is: 0.4868484139442444\n",
            "loss on batch 75 in epoch 60 for the simple nn is: 0.48868703842163086\n",
            "loss on batch 76 in epoch 60 for the simple nn is: 0.4783777594566345\n",
            "loss on batch 77 in epoch 60 for the simple nn is: 0.44047194719314575\n",
            "loss on batch 78 in epoch 60 for the simple nn is: 0.5084564685821533\n",
            "loss on batch 79 in epoch 60 for the simple nn is: 0.5176668167114258\n",
            "loss on batch 80 in epoch 60 for the simple nn is: 0.49534571170806885\n",
            "loss on batch 81 in epoch 60 for the simple nn is: 0.6208763122558594\n",
            "loss on batch 82 in epoch 60 for the simple nn is: 0.4807754158973694\n",
            "loss on batch 83 in epoch 60 for the simple nn is: 0.5176146626472473\n",
            "loss on batch 84 in epoch 60 for the simple nn is: 0.4827101528644562\n",
            "loss on batch 85 in epoch 60 for the simple nn is: 0.5045117735862732\n",
            "loss on batch 86 in epoch 60 for the simple nn is: 0.4085295498371124\n",
            "loss on batch 87 in epoch 60 for the simple nn is: 0.5391375422477722\n",
            "loss on batch 88 in epoch 60 for the simple nn is: 0.4098912179470062\n",
            "loss on batch 89 in epoch 60 for the simple nn is: 0.5083918571472168\n",
            "loss on batch 90 in epoch 60 for the simple nn is: 0.4632478952407837\n",
            "loss on batch 91 in epoch 60 for the simple nn is: 0.44949015974998474\n",
            "loss on batch 92 in epoch 60 for the simple nn is: 0.5405694246292114\n",
            "loss on batch 93 in epoch 60 for the simple nn is: 0.46156948804855347\n",
            "loss on batch 94 in epoch 60 for the simple nn is: 0.4000644087791443\n",
            "loss on batch 95 in epoch 60 for the simple nn is: 0.5013929605484009\n",
            "loss on batch 96 in epoch 60 for the simple nn is: 0.4182944893836975\n",
            "loss on batch 97 in epoch 60 for the simple nn is: 0.4892423450946808\n",
            "loss on batch 98 in epoch 60 for the simple nn is: 0.45365601778030396\n",
            "loss on batch 99 in epoch 60 for the simple nn is: 0.4795204997062683\n",
            "loss on batch 100 in epoch 60 for the simple nn is: 0.5045037865638733\n",
            "loss on batch 101 in epoch 60 for the simple nn is: 0.42982205748558044\n",
            "loss on batch 102 in epoch 60 for the simple nn is: 0.3899066746234894\n",
            "loss on batch 103 in epoch 60 for the simple nn is: 0.5187332034111023\n",
            "loss on batch 104 in epoch 60 for the simple nn is: 0.43752408027648926\n",
            "loss on batch 105 in epoch 60 for the simple nn is: 0.413931667804718\n",
            "loss on batch 106 in epoch 60 for the simple nn is: 0.4588349461555481\n",
            "loss on batch 107 in epoch 60 for the simple nn is: 0.46639999747276306\n",
            "loss on batch 108 in epoch 60 for the simple nn is: 0.45733994245529175\n",
            "loss on batch 109 in epoch 60 for the simple nn is: 0.5451273918151855\n",
            "loss on batch 110 in epoch 60 for the simple nn is: 0.3759237229824066\n",
            "loss on batch 111 in epoch 60 for the simple nn is: 0.44483238458633423\n",
            "loss on batch 112 in epoch 60 for the simple nn is: 0.5123302936553955\n",
            "loss on batch 113 in epoch 60 for the simple nn is: 0.528243899345398\n",
            "loss on batch 114 in epoch 60 for the simple nn is: 0.536420464515686\n",
            "loss on batch 115 in epoch 60 for the simple nn is: 0.45427146553993225\n",
            "loss on batch 116 in epoch 60 for the simple nn is: 0.5898723006248474\n",
            "loss on batch 117 in epoch 60 for the simple nn is: 0.5382266640663147\n",
            "loss on batch 118 in epoch 60 for the simple nn is: 0.5593591928482056\n",
            "loss on batch 119 in epoch 60 for the simple nn is: 0.5368561148643494\n",
            "loss on batch 120 in epoch 60 for the simple nn is: 0.4991198778152466\n",
            "loss on batch 0 in epoch 61 for the simple nn is: 0.4896860718727112\n",
            "loss on batch 1 in epoch 61 for the simple nn is: 0.5935591459274292\n",
            "loss on batch 2 in epoch 61 for the simple nn is: 0.5730923414230347\n",
            "loss on batch 3 in epoch 61 for the simple nn is: 0.532949686050415\n",
            "loss on batch 4 in epoch 61 for the simple nn is: 0.5514186024665833\n",
            "loss on batch 5 in epoch 61 for the simple nn is: 0.5772123336791992\n",
            "loss on batch 6 in epoch 61 for the simple nn is: 0.5201392769813538\n",
            "loss on batch 7 in epoch 61 for the simple nn is: 0.5129052400588989\n",
            "loss on batch 8 in epoch 61 for the simple nn is: 0.5051706433296204\n",
            "loss on batch 9 in epoch 61 for the simple nn is: 0.5412351489067078\n",
            "loss on batch 10 in epoch 61 for the simple nn is: 0.4430209696292877\n",
            "loss on batch 11 in epoch 61 for the simple nn is: 0.5072765350341797\n",
            "loss on batch 12 in epoch 61 for the simple nn is: 0.5196016430854797\n",
            "loss on batch 13 in epoch 61 for the simple nn is: 0.5068213939666748\n",
            "loss on batch 14 in epoch 61 for the simple nn is: 0.5144884586334229\n",
            "loss on batch 15 in epoch 61 for the simple nn is: 0.4884891211986542\n",
            "loss on batch 16 in epoch 61 for the simple nn is: 0.6175150275230408\n",
            "loss on batch 17 in epoch 61 for the simple nn is: 0.4766611158847809\n",
            "loss on batch 18 in epoch 61 for the simple nn is: 0.46696528792381287\n",
            "loss on batch 19 in epoch 61 for the simple nn is: 0.48435816168785095\n",
            "loss on batch 20 in epoch 61 for the simple nn is: 0.46896910667419434\n",
            "loss on batch 21 in epoch 61 for the simple nn is: 0.5320377349853516\n",
            "loss on batch 22 in epoch 61 for the simple nn is: 0.48584216833114624\n",
            "loss on batch 23 in epoch 61 for the simple nn is: 0.5300918221473694\n",
            "loss on batch 24 in epoch 61 for the simple nn is: 0.42212316393852234\n",
            "loss on batch 25 in epoch 61 for the simple nn is: 0.5135498642921448\n",
            "loss on batch 26 in epoch 61 for the simple nn is: 0.5303376317024231\n",
            "loss on batch 27 in epoch 61 for the simple nn is: 0.543551504611969\n",
            "loss on batch 28 in epoch 61 for the simple nn is: 0.466220498085022\n",
            "loss on batch 29 in epoch 61 for the simple nn is: 0.5587340593338013\n",
            "loss on batch 30 in epoch 61 for the simple nn is: 0.49523797631263733\n",
            "loss on batch 31 in epoch 61 for the simple nn is: 0.5191211104393005\n",
            "loss on batch 32 in epoch 61 for the simple nn is: 0.45873185992240906\n",
            "loss on batch 33 in epoch 61 for the simple nn is: 0.5292297005653381\n",
            "loss on batch 34 in epoch 61 for the simple nn is: 0.47129499912261963\n",
            "loss on batch 35 in epoch 61 for the simple nn is: 0.4850769340991974\n",
            "loss on batch 36 in epoch 61 for the simple nn is: 0.4935382306575775\n",
            "loss on batch 37 in epoch 61 for the simple nn is: 0.44029009342193604\n",
            "loss on batch 38 in epoch 61 for the simple nn is: 0.5347395539283752\n",
            "loss on batch 39 in epoch 61 for the simple nn is: 0.4363759458065033\n",
            "loss on batch 40 in epoch 61 for the simple nn is: 0.5030003786087036\n",
            "loss on batch 41 in epoch 61 for the simple nn is: 0.43324992060661316\n",
            "loss on batch 42 in epoch 61 for the simple nn is: 0.5406904816627502\n",
            "loss on batch 43 in epoch 61 for the simple nn is: 0.5049728155136108\n",
            "loss on batch 44 in epoch 61 for the simple nn is: 0.517948567867279\n",
            "loss on batch 45 in epoch 61 for the simple nn is: 0.5002016425132751\n",
            "loss on batch 46 in epoch 61 for the simple nn is: 0.4343321919441223\n",
            "loss on batch 47 in epoch 61 for the simple nn is: 0.4679318368434906\n",
            "loss on batch 48 in epoch 61 for the simple nn is: 0.48861053586006165\n",
            "loss on batch 49 in epoch 61 for the simple nn is: 0.5450649857521057\n",
            "loss on batch 50 in epoch 61 for the simple nn is: 0.48515936732292175\n",
            "loss on batch 51 in epoch 61 for the simple nn is: 0.5279141664505005\n",
            "loss on batch 52 in epoch 61 for the simple nn is: 0.4331943094730377\n",
            "loss on batch 53 in epoch 61 for the simple nn is: 0.40293920040130615\n",
            "loss on batch 54 in epoch 61 for the simple nn is: 0.5578120350837708\n",
            "loss on batch 55 in epoch 61 for the simple nn is: 0.4476415812969208\n",
            "loss on batch 56 in epoch 61 for the simple nn is: 0.5102449655532837\n",
            "loss on batch 57 in epoch 61 for the simple nn is: 0.4926659166812897\n",
            "loss on batch 58 in epoch 61 for the simple nn is: 0.451911598443985\n",
            "loss on batch 59 in epoch 61 for the simple nn is: 0.5483672022819519\n",
            "loss on batch 60 in epoch 61 for the simple nn is: 0.47278258204460144\n",
            "loss on batch 61 in epoch 61 for the simple nn is: 0.4383467137813568\n",
            "loss on batch 62 in epoch 61 for the simple nn is: 0.4780612885951996\n",
            "loss on batch 63 in epoch 61 for the simple nn is: 0.4915159046649933\n",
            "loss on batch 64 in epoch 61 for the simple nn is: 0.4238470196723938\n",
            "loss on batch 65 in epoch 61 for the simple nn is: 0.41670775413513184\n",
            "loss on batch 66 in epoch 61 for the simple nn is: 0.4751357138156891\n",
            "loss on batch 67 in epoch 61 for the simple nn is: 0.4204674959182739\n",
            "loss on batch 68 in epoch 61 for the simple nn is: 0.6283189058303833\n",
            "loss on batch 69 in epoch 61 for the simple nn is: 0.48334479331970215\n",
            "loss on batch 70 in epoch 61 for the simple nn is: 0.48057907819747925\n",
            "loss on batch 71 in epoch 61 for the simple nn is: 0.4985513687133789\n",
            "loss on batch 72 in epoch 61 for the simple nn is: 0.4749951660633087\n",
            "loss on batch 73 in epoch 61 for the simple nn is: 0.5364023447036743\n",
            "loss on batch 74 in epoch 61 for the simple nn is: 0.5077303051948547\n",
            "loss on batch 75 in epoch 61 for the simple nn is: 0.48022887110710144\n",
            "loss on batch 76 in epoch 61 for the simple nn is: 0.4925270974636078\n",
            "loss on batch 77 in epoch 61 for the simple nn is: 0.4375109076499939\n",
            "loss on batch 78 in epoch 61 for the simple nn is: 0.4780784249305725\n",
            "loss on batch 79 in epoch 61 for the simple nn is: 0.4629477858543396\n",
            "loss on batch 80 in epoch 61 for the simple nn is: 0.5987302660942078\n",
            "loss on batch 81 in epoch 61 for the simple nn is: 0.402940958738327\n",
            "loss on batch 82 in epoch 61 for the simple nn is: 0.4882165491580963\n",
            "loss on batch 83 in epoch 61 for the simple nn is: 0.5124977827072144\n",
            "loss on batch 84 in epoch 61 for the simple nn is: 0.48720160126686096\n",
            "loss on batch 85 in epoch 61 for the simple nn is: 0.4990715980529785\n",
            "loss on batch 86 in epoch 61 for the simple nn is: 0.3883257806301117\n",
            "loss on batch 87 in epoch 61 for the simple nn is: 0.5088732838630676\n",
            "loss on batch 88 in epoch 61 for the simple nn is: 0.3958660960197449\n",
            "loss on batch 89 in epoch 61 for the simple nn is: 0.5039452910423279\n",
            "loss on batch 90 in epoch 61 for the simple nn is: 0.48736196756362915\n",
            "loss on batch 91 in epoch 61 for the simple nn is: 0.5431582927703857\n",
            "loss on batch 92 in epoch 61 for the simple nn is: 0.5454449653625488\n",
            "loss on batch 93 in epoch 61 for the simple nn is: 0.4565030038356781\n",
            "loss on batch 94 in epoch 61 for the simple nn is: 0.405539333820343\n",
            "loss on batch 95 in epoch 61 for the simple nn is: 0.5053479075431824\n",
            "loss on batch 96 in epoch 61 for the simple nn is: 0.4231261610984802\n",
            "loss on batch 97 in epoch 61 for the simple nn is: 0.507975697517395\n",
            "loss on batch 98 in epoch 61 for the simple nn is: 0.4597522020339966\n",
            "loss on batch 99 in epoch 61 for the simple nn is: 0.4850969612598419\n",
            "loss on batch 100 in epoch 61 for the simple nn is: 0.4855169951915741\n",
            "loss on batch 101 in epoch 61 for the simple nn is: 0.40760308504104614\n",
            "loss on batch 102 in epoch 61 for the simple nn is: 0.3665148615837097\n",
            "loss on batch 103 in epoch 61 for the simple nn is: 0.5145008563995361\n",
            "loss on batch 104 in epoch 61 for the simple nn is: 0.43432947993278503\n",
            "loss on batch 105 in epoch 61 for the simple nn is: 0.41383805871009827\n",
            "loss on batch 106 in epoch 61 for the simple nn is: 0.4758961498737335\n",
            "loss on batch 107 in epoch 61 for the simple nn is: 0.4296315014362335\n",
            "loss on batch 108 in epoch 61 for the simple nn is: 0.4804791510105133\n",
            "loss on batch 109 in epoch 61 for the simple nn is: 0.6404879689216614\n",
            "loss on batch 110 in epoch 61 for the simple nn is: 0.3477168679237366\n",
            "loss on batch 111 in epoch 61 for the simple nn is: 0.4627513587474823\n",
            "loss on batch 112 in epoch 61 for the simple nn is: 0.5122697353363037\n",
            "loss on batch 113 in epoch 61 for the simple nn is: 0.514358401298523\n",
            "loss on batch 114 in epoch 61 for the simple nn is: 0.5227213501930237\n",
            "loss on batch 115 in epoch 61 for the simple nn is: 0.46048587560653687\n",
            "loss on batch 116 in epoch 61 for the simple nn is: 0.5878897309303284\n",
            "loss on batch 117 in epoch 61 for the simple nn is: 0.5384624004364014\n",
            "loss on batch 118 in epoch 61 for the simple nn is: 0.5757321715354919\n",
            "loss on batch 119 in epoch 61 for the simple nn is: 0.5436128973960876\n",
            "loss on batch 120 in epoch 61 for the simple nn is: 0.5196313261985779\n",
            "loss on batch 0 in epoch 62 for the simple nn is: 0.5124130249023438\n",
            "loss on batch 1 in epoch 62 for the simple nn is: 0.6083846688270569\n",
            "loss on batch 2 in epoch 62 for the simple nn is: 0.5855045318603516\n",
            "loss on batch 3 in epoch 62 for the simple nn is: 0.541939914226532\n",
            "loss on batch 4 in epoch 62 for the simple nn is: 0.6158286929130554\n",
            "loss on batch 5 in epoch 62 for the simple nn is: 0.5905025601387024\n",
            "loss on batch 6 in epoch 62 for the simple nn is: 0.532953143119812\n",
            "loss on batch 7 in epoch 62 for the simple nn is: 0.5169769525527954\n",
            "loss on batch 8 in epoch 62 for the simple nn is: 0.5017752051353455\n",
            "loss on batch 9 in epoch 62 for the simple nn is: 0.48870912194252014\n",
            "loss on batch 10 in epoch 62 for the simple nn is: 0.4522574245929718\n",
            "loss on batch 11 in epoch 62 for the simple nn is: 0.5293859839439392\n",
            "loss on batch 12 in epoch 62 for the simple nn is: 0.5349088907241821\n",
            "loss on batch 13 in epoch 62 for the simple nn is: 0.507927417755127\n",
            "loss on batch 14 in epoch 62 for the simple nn is: 0.5290165543556213\n",
            "loss on batch 15 in epoch 62 for the simple nn is: 0.46453097462654114\n",
            "loss on batch 16 in epoch 62 for the simple nn is: 0.5310004949569702\n",
            "loss on batch 17 in epoch 62 for the simple nn is: 0.47171464562416077\n",
            "loss on batch 18 in epoch 62 for the simple nn is: 0.466781884431839\n",
            "loss on batch 19 in epoch 62 for the simple nn is: 0.49279066920280457\n",
            "loss on batch 20 in epoch 62 for the simple nn is: 0.501333475112915\n",
            "loss on batch 21 in epoch 62 for the simple nn is: 0.5401506423950195\n",
            "loss on batch 22 in epoch 62 for the simple nn is: 0.6799840331077576\n",
            "loss on batch 23 in epoch 62 for the simple nn is: 0.5256664752960205\n",
            "loss on batch 24 in epoch 62 for the simple nn is: 0.40147489309310913\n",
            "loss on batch 25 in epoch 62 for the simple nn is: 0.4784449338912964\n",
            "loss on batch 26 in epoch 62 for the simple nn is: 0.526046633720398\n",
            "loss on batch 27 in epoch 62 for the simple nn is: 0.5416297316551208\n",
            "loss on batch 28 in epoch 62 for the simple nn is: 0.4730781018733978\n",
            "loss on batch 29 in epoch 62 for the simple nn is: 0.5411587953567505\n",
            "loss on batch 30 in epoch 62 for the simple nn is: 0.4892439842224121\n",
            "loss on batch 31 in epoch 62 for the simple nn is: 0.5368239879608154\n",
            "loss on batch 32 in epoch 62 for the simple nn is: 0.4489608108997345\n",
            "loss on batch 33 in epoch 62 for the simple nn is: 0.47011876106262207\n",
            "loss on batch 34 in epoch 62 for the simple nn is: 0.4425157904624939\n",
            "loss on batch 35 in epoch 62 for the simple nn is: 0.503381073474884\n",
            "loss on batch 36 in epoch 62 for the simple nn is: 0.48125705122947693\n",
            "loss on batch 37 in epoch 62 for the simple nn is: 0.43629154562950134\n",
            "loss on batch 38 in epoch 62 for the simple nn is: 0.5452867746353149\n",
            "loss on batch 39 in epoch 62 for the simple nn is: 0.4216926693916321\n",
            "loss on batch 40 in epoch 62 for the simple nn is: 0.5171616673469543\n",
            "loss on batch 41 in epoch 62 for the simple nn is: 0.5241789221763611\n",
            "loss on batch 42 in epoch 62 for the simple nn is: 0.507756233215332\n",
            "loss on batch 43 in epoch 62 for the simple nn is: 0.5202703475952148\n",
            "loss on batch 44 in epoch 62 for the simple nn is: 0.573165774345398\n",
            "loss on batch 45 in epoch 62 for the simple nn is: 0.493905633687973\n",
            "loss on batch 46 in epoch 62 for the simple nn is: 0.43313318490982056\n",
            "loss on batch 47 in epoch 62 for the simple nn is: 0.45409223437309265\n",
            "loss on batch 48 in epoch 62 for the simple nn is: 0.5009297132492065\n",
            "loss on batch 49 in epoch 62 for the simple nn is: 0.5316358804702759\n",
            "loss on batch 50 in epoch 62 for the simple nn is: 0.49150100350379944\n",
            "loss on batch 51 in epoch 62 for the simple nn is: 0.5181111693382263\n",
            "loss on batch 52 in epoch 62 for the simple nn is: 0.39946243166923523\n",
            "loss on batch 53 in epoch 62 for the simple nn is: 0.4393891394138336\n",
            "loss on batch 54 in epoch 62 for the simple nn is: 0.5638128519058228\n",
            "loss on batch 55 in epoch 62 for the simple nn is: 0.47476765513420105\n",
            "loss on batch 56 in epoch 62 for the simple nn is: 0.5271493196487427\n",
            "loss on batch 57 in epoch 62 for the simple nn is: 0.5750776529312134\n",
            "loss on batch 58 in epoch 62 for the simple nn is: 0.46782901883125305\n",
            "loss on batch 59 in epoch 62 for the simple nn is: 0.5402548313140869\n",
            "loss on batch 60 in epoch 62 for the simple nn is: 0.4892004728317261\n",
            "loss on batch 61 in epoch 62 for the simple nn is: 0.45919761061668396\n",
            "loss on batch 62 in epoch 62 for the simple nn is: 0.5382593274116516\n",
            "loss on batch 63 in epoch 62 for the simple nn is: 0.5222201943397522\n",
            "loss on batch 64 in epoch 62 for the simple nn is: 0.4318692982196808\n",
            "loss on batch 65 in epoch 62 for the simple nn is: 0.42548298835754395\n",
            "loss on batch 66 in epoch 62 for the simple nn is: 0.4879685342311859\n",
            "loss on batch 67 in epoch 62 for the simple nn is: 0.4365551471710205\n",
            "loss on batch 68 in epoch 62 for the simple nn is: 0.4477198123931885\n",
            "loss on batch 69 in epoch 62 for the simple nn is: 0.4827507734298706\n",
            "loss on batch 70 in epoch 62 for the simple nn is: 0.48369839787483215\n",
            "loss on batch 71 in epoch 62 for the simple nn is: 0.47227907180786133\n",
            "loss on batch 72 in epoch 62 for the simple nn is: 0.49837586283683777\n",
            "loss on batch 73 in epoch 62 for the simple nn is: 0.5318993926048279\n",
            "loss on batch 74 in epoch 62 for the simple nn is: 0.5079196691513062\n",
            "loss on batch 75 in epoch 62 for the simple nn is: 0.47651126980781555\n",
            "loss on batch 76 in epoch 62 for the simple nn is: 0.4899071753025055\n",
            "loss on batch 77 in epoch 62 for the simple nn is: 0.4526516795158386\n",
            "loss on batch 78 in epoch 62 for the simple nn is: 0.4781831204891205\n",
            "loss on batch 79 in epoch 62 for the simple nn is: 0.42264047265052795\n",
            "loss on batch 80 in epoch 62 for the simple nn is: 0.5176823139190674\n",
            "loss on batch 81 in epoch 62 for the simple nn is: 0.39716121554374695\n",
            "loss on batch 82 in epoch 62 for the simple nn is: 0.4860222637653351\n",
            "loss on batch 83 in epoch 62 for the simple nn is: 0.5761943459510803\n",
            "loss on batch 84 in epoch 62 for the simple nn is: 0.47167348861694336\n",
            "loss on batch 85 in epoch 62 for the simple nn is: 0.49741441011428833\n",
            "loss on batch 86 in epoch 62 for the simple nn is: 0.40634918212890625\n",
            "loss on batch 87 in epoch 62 for the simple nn is: 0.5097678899765015\n",
            "loss on batch 88 in epoch 62 for the simple nn is: 0.3768669068813324\n",
            "loss on batch 89 in epoch 62 for the simple nn is: 0.5036519765853882\n",
            "loss on batch 90 in epoch 62 for the simple nn is: 0.4463091194629669\n",
            "loss on batch 91 in epoch 62 for the simple nn is: 0.4615356922149658\n",
            "loss on batch 92 in epoch 62 for the simple nn is: 0.5409250855445862\n",
            "loss on batch 93 in epoch 62 for the simple nn is: 0.45025092363357544\n",
            "loss on batch 94 in epoch 62 for the simple nn is: 0.37298163771629333\n",
            "loss on batch 95 in epoch 62 for the simple nn is: 0.5027799606323242\n",
            "loss on batch 96 in epoch 62 for the simple nn is: 0.40219780802726746\n",
            "loss on batch 97 in epoch 62 for the simple nn is: 0.48196083307266235\n",
            "loss on batch 98 in epoch 62 for the simple nn is: 0.4397635757923126\n",
            "loss on batch 99 in epoch 62 for the simple nn is: 0.45329564809799194\n",
            "loss on batch 100 in epoch 62 for the simple nn is: 0.48129352927207947\n",
            "loss on batch 101 in epoch 62 for the simple nn is: 0.4057250916957855\n",
            "loss on batch 102 in epoch 62 for the simple nn is: 0.3651584982872009\n",
            "loss on batch 103 in epoch 62 for the simple nn is: 0.4970993995666504\n",
            "loss on batch 104 in epoch 62 for the simple nn is: 0.4499199390411377\n",
            "loss on batch 105 in epoch 62 for the simple nn is: 0.41783037781715393\n",
            "loss on batch 106 in epoch 62 for the simple nn is: 0.43928587436676025\n",
            "loss on batch 107 in epoch 62 for the simple nn is: 0.4665904939174652\n",
            "loss on batch 108 in epoch 62 for the simple nn is: 0.47670456767082214\n",
            "loss on batch 109 in epoch 62 for the simple nn is: 0.5379135012626648\n",
            "loss on batch 110 in epoch 62 for the simple nn is: 0.38573622703552246\n",
            "loss on batch 111 in epoch 62 for the simple nn is: 0.573930025100708\n",
            "loss on batch 112 in epoch 62 for the simple nn is: 0.5278130769729614\n",
            "loss on batch 113 in epoch 62 for the simple nn is: 0.5221595764160156\n",
            "loss on batch 114 in epoch 62 for the simple nn is: 0.5434473752975464\n",
            "loss on batch 115 in epoch 62 for the simple nn is: 0.4491662085056305\n",
            "loss on batch 116 in epoch 62 for the simple nn is: 0.5800744891166687\n",
            "loss on batch 117 in epoch 62 for the simple nn is: 0.5517882108688354\n",
            "loss on batch 118 in epoch 62 for the simple nn is: 0.6171708106994629\n",
            "loss on batch 119 in epoch 62 for the simple nn is: 0.5481348037719727\n",
            "loss on batch 120 in epoch 62 for the simple nn is: 0.50564044713974\n",
            "loss on batch 0 in epoch 63 for the simple nn is: 0.48306962847709656\n",
            "loss on batch 1 in epoch 63 for the simple nn is: 0.5939790606498718\n",
            "loss on batch 2 in epoch 63 for the simple nn is: 0.572303056716919\n",
            "loss on batch 3 in epoch 63 for the simple nn is: 0.6190232634544373\n",
            "loss on batch 4 in epoch 63 for the simple nn is: 0.5671276450157166\n",
            "loss on batch 5 in epoch 63 for the simple nn is: 0.5779370069503784\n",
            "loss on batch 6 in epoch 63 for the simple nn is: 0.5388107895851135\n",
            "loss on batch 7 in epoch 63 for the simple nn is: 0.5301557779312134\n",
            "loss on batch 8 in epoch 63 for the simple nn is: 0.4606459438800812\n",
            "loss on batch 9 in epoch 63 for the simple nn is: 0.48769494891166687\n",
            "loss on batch 10 in epoch 63 for the simple nn is: 0.4705231487751007\n",
            "loss on batch 11 in epoch 63 for the simple nn is: 0.5301464200019836\n",
            "loss on batch 12 in epoch 63 for the simple nn is: 0.5535471439361572\n",
            "loss on batch 13 in epoch 63 for the simple nn is: 0.5219979286193848\n",
            "loss on batch 14 in epoch 63 for the simple nn is: 0.5130713582038879\n",
            "loss on batch 15 in epoch 63 for the simple nn is: 0.46886298060417175\n",
            "loss on batch 16 in epoch 63 for the simple nn is: 0.5301316380500793\n",
            "loss on batch 17 in epoch 63 for the simple nn is: 0.49972113966941833\n",
            "loss on batch 18 in epoch 63 for the simple nn is: 0.47299888730049133\n",
            "loss on batch 19 in epoch 63 for the simple nn is: 0.4837459325790405\n",
            "loss on batch 20 in epoch 63 for the simple nn is: 0.46517494320869446\n",
            "loss on batch 21 in epoch 63 for the simple nn is: 0.5364555716514587\n",
            "loss on batch 22 in epoch 63 for the simple nn is: 0.4758155643939972\n",
            "loss on batch 23 in epoch 63 for the simple nn is: 0.47036656737327576\n",
            "loss on batch 24 in epoch 63 for the simple nn is: 0.40481165051460266\n",
            "loss on batch 25 in epoch 63 for the simple nn is: 0.45288246870040894\n",
            "loss on batch 26 in epoch 63 for the simple nn is: 0.5300890803337097\n",
            "loss on batch 27 in epoch 63 for the simple nn is: 0.4963206648826599\n",
            "loss on batch 28 in epoch 63 for the simple nn is: 0.4538417458534241\n",
            "loss on batch 29 in epoch 63 for the simple nn is: 0.5870516300201416\n",
            "loss on batch 30 in epoch 63 for the simple nn is: 0.49188268184661865\n",
            "loss on batch 31 in epoch 63 for the simple nn is: 0.5400809049606323\n",
            "loss on batch 32 in epoch 63 for the simple nn is: 0.4426788091659546\n",
            "loss on batch 33 in epoch 63 for the simple nn is: 0.5607297420501709\n",
            "loss on batch 34 in epoch 63 for the simple nn is: 0.4369962811470032\n",
            "loss on batch 35 in epoch 63 for the simple nn is: 0.49318256974220276\n",
            "loss on batch 36 in epoch 63 for the simple nn is: 0.48155710101127625\n",
            "loss on batch 37 in epoch 63 for the simple nn is: 0.42698800563812256\n",
            "loss on batch 38 in epoch 63 for the simple nn is: 0.5133340358734131\n",
            "loss on batch 39 in epoch 63 for the simple nn is: 0.4217672646045685\n",
            "loss on batch 40 in epoch 63 for the simple nn is: 0.5098462700843811\n",
            "loss on batch 41 in epoch 63 for the simple nn is: 0.41475552320480347\n",
            "loss on batch 42 in epoch 63 for the simple nn is: 0.4845874309539795\n",
            "loss on batch 43 in epoch 63 for the simple nn is: 0.5048593282699585\n",
            "loss on batch 44 in epoch 63 for the simple nn is: 0.5149524807929993\n",
            "loss on batch 45 in epoch 63 for the simple nn is: 0.49384263157844543\n",
            "loss on batch 46 in epoch 63 for the simple nn is: 0.43008407950401306\n",
            "loss on batch 47 in epoch 63 for the simple nn is: 0.47715356945991516\n",
            "loss on batch 48 in epoch 63 for the simple nn is: 0.4921221137046814\n",
            "loss on batch 49 in epoch 63 for the simple nn is: 0.5300284624099731\n",
            "loss on batch 50 in epoch 63 for the simple nn is: 0.46302250027656555\n",
            "loss on batch 51 in epoch 63 for the simple nn is: 0.5095508694648743\n",
            "loss on batch 52 in epoch 63 for the simple nn is: 0.3886415958404541\n",
            "loss on batch 53 in epoch 63 for the simple nn is: 0.3951069712638855\n",
            "loss on batch 54 in epoch 63 for the simple nn is: 0.5924155712127686\n",
            "loss on batch 55 in epoch 63 for the simple nn is: 0.44820234179496765\n",
            "loss on batch 56 in epoch 63 for the simple nn is: 0.5060497522354126\n",
            "loss on batch 57 in epoch 63 for the simple nn is: 0.5045154094696045\n",
            "loss on batch 58 in epoch 63 for the simple nn is: 0.45897117257118225\n",
            "loss on batch 59 in epoch 63 for the simple nn is: 0.49751773476600647\n",
            "loss on batch 60 in epoch 63 for the simple nn is: 0.47340497374534607\n",
            "loss on batch 61 in epoch 63 for the simple nn is: 0.4298125207424164\n",
            "loss on batch 62 in epoch 63 for the simple nn is: 0.477581650018692\n",
            "loss on batch 63 in epoch 63 for the simple nn is: 0.47868722677230835\n",
            "loss on batch 64 in epoch 63 for the simple nn is: 0.4389726519584656\n",
            "loss on batch 65 in epoch 63 for the simple nn is: 0.41482412815093994\n",
            "loss on batch 66 in epoch 63 for the simple nn is: 0.4809264540672302\n",
            "loss on batch 67 in epoch 63 for the simple nn is: 0.43889063596725464\n",
            "loss on batch 68 in epoch 63 for the simple nn is: 0.43053877353668213\n",
            "loss on batch 69 in epoch 63 for the simple nn is: 0.4688701927661896\n",
            "loss on batch 70 in epoch 63 for the simple nn is: 0.4800686836242676\n",
            "loss on batch 71 in epoch 63 for the simple nn is: 0.472280889749527\n",
            "loss on batch 72 in epoch 63 for the simple nn is: 0.47618913650512695\n",
            "loss on batch 73 in epoch 63 for the simple nn is: 0.5267238020896912\n",
            "loss on batch 74 in epoch 63 for the simple nn is: 0.5084336996078491\n",
            "loss on batch 75 in epoch 63 for the simple nn is: 0.49949318170547485\n",
            "loss on batch 76 in epoch 63 for the simple nn is: 0.478672057390213\n",
            "loss on batch 77 in epoch 63 for the simple nn is: 0.43443217873573303\n",
            "loss on batch 78 in epoch 63 for the simple nn is: 0.4853091537952423\n",
            "loss on batch 79 in epoch 63 for the simple nn is: 0.4163646399974823\n",
            "loss on batch 80 in epoch 63 for the simple nn is: 0.47155436873435974\n",
            "loss on batch 81 in epoch 63 for the simple nn is: 0.4186074733734131\n",
            "loss on batch 82 in epoch 63 for the simple nn is: 0.473556250333786\n",
            "loss on batch 83 in epoch 63 for the simple nn is: 0.4971269965171814\n",
            "loss on batch 84 in epoch 63 for the simple nn is: 0.4727103114128113\n",
            "loss on batch 85 in epoch 63 for the simple nn is: 0.5037926435470581\n",
            "loss on batch 86 in epoch 63 for the simple nn is: 0.41285720467567444\n",
            "loss on batch 87 in epoch 63 for the simple nn is: 0.5059628486633301\n",
            "loss on batch 88 in epoch 63 for the simple nn is: 0.37905922532081604\n",
            "loss on batch 89 in epoch 63 for the simple nn is: 0.5202227234840393\n",
            "loss on batch 90 in epoch 63 for the simple nn is: 0.4513537883758545\n",
            "loss on batch 91 in epoch 63 for the simple nn is: 0.4490267038345337\n",
            "loss on batch 92 in epoch 63 for the simple nn is: 0.5300273895263672\n",
            "loss on batch 93 in epoch 63 for the simple nn is: 0.45701926946640015\n",
            "loss on batch 94 in epoch 63 for the simple nn is: 0.36795395612716675\n",
            "loss on batch 95 in epoch 63 for the simple nn is: 0.5088261365890503\n",
            "loss on batch 96 in epoch 63 for the simple nn is: 0.40305227041244507\n",
            "loss on batch 97 in epoch 63 for the simple nn is: 0.49776986241340637\n",
            "loss on batch 98 in epoch 63 for the simple nn is: 0.44155168533325195\n",
            "loss on batch 99 in epoch 63 for the simple nn is: 0.45507583022117615\n",
            "loss on batch 100 in epoch 63 for the simple nn is: 0.47932955622673035\n",
            "loss on batch 101 in epoch 63 for the simple nn is: 0.40206000208854675\n",
            "loss on batch 102 in epoch 63 for the simple nn is: 0.3642418086528778\n",
            "loss on batch 103 in epoch 63 for the simple nn is: 0.5218250751495361\n",
            "loss on batch 104 in epoch 63 for the simple nn is: 0.47369521856307983\n",
            "loss on batch 105 in epoch 63 for the simple nn is: 0.4208552837371826\n",
            "loss on batch 106 in epoch 63 for the simple nn is: 0.44026297330856323\n",
            "loss on batch 107 in epoch 63 for the simple nn is: 0.4291552007198334\n",
            "loss on batch 108 in epoch 63 for the simple nn is: 0.4226650893688202\n",
            "loss on batch 109 in epoch 63 for the simple nn is: 0.5206279158592224\n",
            "loss on batch 110 in epoch 63 for the simple nn is: 0.35907435417175293\n",
            "loss on batch 111 in epoch 63 for the simple nn is: 0.4551619589328766\n",
            "loss on batch 112 in epoch 63 for the simple nn is: 0.5156934261322021\n",
            "loss on batch 113 in epoch 63 for the simple nn is: 0.5113421082496643\n",
            "loss on batch 114 in epoch 63 for the simple nn is: 0.5233351588249207\n",
            "loss on batch 115 in epoch 63 for the simple nn is: 0.44573312997817993\n",
            "loss on batch 116 in epoch 63 for the simple nn is: 0.5867770314216614\n",
            "loss on batch 117 in epoch 63 for the simple nn is: 0.5472725033760071\n",
            "loss on batch 118 in epoch 63 for the simple nn is: 0.5786790251731873\n",
            "loss on batch 119 in epoch 63 for the simple nn is: 0.5340999364852905\n",
            "loss on batch 120 in epoch 63 for the simple nn is: 0.4849618971347809\n",
            "loss on batch 0 in epoch 64 for the simple nn is: 0.4739166498184204\n",
            "loss on batch 1 in epoch 64 for the simple nn is: 0.593483030796051\n",
            "loss on batch 2 in epoch 64 for the simple nn is: 0.677890419960022\n",
            "loss on batch 3 in epoch 64 for the simple nn is: 0.5360966324806213\n",
            "loss on batch 4 in epoch 64 for the simple nn is: 0.551245391368866\n",
            "loss on batch 5 in epoch 64 for the simple nn is: 0.5679681897163391\n",
            "loss on batch 6 in epoch 64 for the simple nn is: 0.5202949047088623\n",
            "loss on batch 7 in epoch 64 for the simple nn is: 0.5130280256271362\n",
            "loss on batch 8 in epoch 64 for the simple nn is: 0.4544232189655304\n",
            "loss on batch 9 in epoch 64 for the simple nn is: 0.4704561233520508\n",
            "loss on batch 10 in epoch 64 for the simple nn is: 0.43503448367118835\n",
            "loss on batch 11 in epoch 64 for the simple nn is: 0.5163479447364807\n",
            "loss on batch 12 in epoch 64 for the simple nn is: 0.5100136399269104\n",
            "loss on batch 13 in epoch 64 for the simple nn is: 0.5807972550392151\n",
            "loss on batch 14 in epoch 64 for the simple nn is: 0.5016176700592041\n",
            "loss on batch 15 in epoch 64 for the simple nn is: 0.4526561200618744\n",
            "loss on batch 16 in epoch 64 for the simple nn is: 0.5100576877593994\n",
            "loss on batch 17 in epoch 64 for the simple nn is: 0.4779781401157379\n",
            "loss on batch 18 in epoch 64 for the simple nn is: 0.46664923429489136\n",
            "loss on batch 19 in epoch 64 for the simple nn is: 0.46210312843322754\n",
            "loss on batch 20 in epoch 64 for the simple nn is: 0.459399938583374\n",
            "loss on batch 21 in epoch 64 for the simple nn is: 0.5173788070678711\n",
            "loss on batch 22 in epoch 64 for the simple nn is: 0.4644930064678192\n",
            "loss on batch 23 in epoch 64 for the simple nn is: 0.4929213225841522\n",
            "loss on batch 24 in epoch 64 for the simple nn is: 0.389165997505188\n",
            "loss on batch 25 in epoch 64 for the simple nn is: 0.45144349336624146\n",
            "loss on batch 26 in epoch 64 for the simple nn is: 0.5260378122329712\n",
            "loss on batch 27 in epoch 64 for the simple nn is: 0.4771966338157654\n",
            "loss on batch 28 in epoch 64 for the simple nn is: 0.45281484723091125\n",
            "loss on batch 29 in epoch 64 for the simple nn is: 0.5608934760093689\n",
            "loss on batch 30 in epoch 64 for the simple nn is: 0.489308625459671\n",
            "loss on batch 31 in epoch 64 for the simple nn is: 0.5197038054466248\n",
            "loss on batch 32 in epoch 64 for the simple nn is: 0.4476214647293091\n",
            "loss on batch 33 in epoch 64 for the simple nn is: 0.4528841972351074\n",
            "loss on batch 34 in epoch 64 for the simple nn is: 0.452251672744751\n",
            "loss on batch 35 in epoch 64 for the simple nn is: 0.49127626419067383\n",
            "loss on batch 36 in epoch 64 for the simple nn is: 0.4851851761341095\n",
            "loss on batch 37 in epoch 64 for the simple nn is: 0.4577195644378662\n",
            "loss on batch 38 in epoch 64 for the simple nn is: 0.5125216245651245\n",
            "loss on batch 39 in epoch 64 for the simple nn is: 0.41503679752349854\n",
            "loss on batch 40 in epoch 64 for the simple nn is: 0.5180553793907166\n",
            "loss on batch 41 in epoch 64 for the simple nn is: 0.4270057678222656\n",
            "loss on batch 42 in epoch 64 for the simple nn is: 0.4706938862800598\n",
            "loss on batch 43 in epoch 64 for the simple nn is: 0.4971666932106018\n",
            "loss on batch 44 in epoch 64 for the simple nn is: 0.5179497003555298\n",
            "loss on batch 45 in epoch 64 for the simple nn is: 0.5063910484313965\n",
            "loss on batch 46 in epoch 64 for the simple nn is: 0.4283773601055145\n",
            "loss on batch 47 in epoch 64 for the simple nn is: 0.4665423333644867\n",
            "loss on batch 48 in epoch 64 for the simple nn is: 0.5275064706802368\n",
            "loss on batch 49 in epoch 64 for the simple nn is: 0.5390394330024719\n",
            "loss on batch 50 in epoch 64 for the simple nn is: 0.44616127014160156\n",
            "loss on batch 51 in epoch 64 for the simple nn is: 0.4821731150150299\n",
            "loss on batch 52 in epoch 64 for the simple nn is: 0.39133790135383606\n",
            "loss on batch 53 in epoch 64 for the simple nn is: 0.3547108471393585\n",
            "loss on batch 54 in epoch 64 for the simple nn is: 0.5271289944648743\n",
            "loss on batch 55 in epoch 64 for the simple nn is: 0.4538165330886841\n",
            "loss on batch 56 in epoch 64 for the simple nn is: 0.4892231523990631\n",
            "loss on batch 57 in epoch 64 for the simple nn is: 0.4965609610080719\n",
            "loss on batch 58 in epoch 64 for the simple nn is: 0.5495710968971252\n",
            "loss on batch 59 in epoch 64 for the simple nn is: 0.6066437363624573\n",
            "loss on batch 60 in epoch 64 for the simple nn is: 0.4728100895881653\n",
            "loss on batch 61 in epoch 64 for the simple nn is: 0.4299754798412323\n",
            "loss on batch 62 in epoch 64 for the simple nn is: 0.48692166805267334\n",
            "loss on batch 63 in epoch 64 for the simple nn is: 0.5259414911270142\n",
            "loss on batch 64 in epoch 64 for the simple nn is: 0.41047248244285583\n",
            "loss on batch 65 in epoch 64 for the simple nn is: 0.4646438956260681\n",
            "loss on batch 66 in epoch 64 for the simple nn is: 0.5434830188751221\n",
            "loss on batch 67 in epoch 64 for the simple nn is: 0.4395345449447632\n",
            "loss on batch 68 in epoch 64 for the simple nn is: 0.4419243037700653\n",
            "loss on batch 69 in epoch 64 for the simple nn is: 0.47784924507141113\n",
            "loss on batch 70 in epoch 64 for the simple nn is: 0.6440834999084473\n",
            "loss on batch 71 in epoch 64 for the simple nn is: 0.47173383831977844\n",
            "loss on batch 72 in epoch 64 for the simple nn is: 0.5100322365760803\n",
            "loss on batch 73 in epoch 64 for the simple nn is: 0.5642297863960266\n",
            "loss on batch 74 in epoch 64 for the simple nn is: 0.5082895159721375\n",
            "loss on batch 75 in epoch 64 for the simple nn is: 0.4793775975704193\n",
            "loss on batch 76 in epoch 64 for the simple nn is: 0.501243531703949\n",
            "loss on batch 77 in epoch 64 for the simple nn is: 0.4363721013069153\n",
            "loss on batch 78 in epoch 64 for the simple nn is: 0.49245545268058777\n",
            "loss on batch 79 in epoch 64 for the simple nn is: 0.42852652072906494\n",
            "loss on batch 80 in epoch 64 for the simple nn is: 0.49681857228279114\n",
            "loss on batch 81 in epoch 64 for the simple nn is: 0.45355719327926636\n",
            "loss on batch 82 in epoch 64 for the simple nn is: 0.47125422954559326\n",
            "loss on batch 83 in epoch 64 for the simple nn is: 0.5105720162391663\n",
            "loss on batch 84 in epoch 64 for the simple nn is: 0.48698094487190247\n",
            "loss on batch 85 in epoch 64 for the simple nn is: 0.5128515958786011\n",
            "loss on batch 86 in epoch 64 for the simple nn is: 0.4078383445739746\n",
            "loss on batch 87 in epoch 64 for the simple nn is: 0.5006968379020691\n",
            "loss on batch 88 in epoch 64 for the simple nn is: 0.41613903641700745\n",
            "loss on batch 89 in epoch 64 for the simple nn is: 0.5267115831375122\n",
            "loss on batch 90 in epoch 64 for the simple nn is: 0.4626971185207367\n",
            "loss on batch 91 in epoch 64 for the simple nn is: 0.44626984000205994\n",
            "loss on batch 92 in epoch 64 for the simple nn is: 0.5400429368019104\n",
            "loss on batch 93 in epoch 64 for the simple nn is: 0.4578090310096741\n",
            "loss on batch 94 in epoch 64 for the simple nn is: 0.4230978786945343\n",
            "loss on batch 95 in epoch 64 for the simple nn is: 0.5090970396995544\n",
            "loss on batch 96 in epoch 64 for the simple nn is: 0.4149792492389679\n",
            "loss on batch 97 in epoch 64 for the simple nn is: 0.49151694774627686\n",
            "loss on batch 98 in epoch 64 for the simple nn is: 0.43994343280792236\n",
            "loss on batch 99 in epoch 64 for the simple nn is: 0.470909059047699\n",
            "loss on batch 100 in epoch 64 for the simple nn is: 0.5149558186531067\n",
            "loss on batch 101 in epoch 64 for the simple nn is: 0.4029317796230316\n",
            "loss on batch 102 in epoch 64 for the simple nn is: 0.39364656805992126\n",
            "loss on batch 103 in epoch 64 for the simple nn is: 0.5228073000907898\n",
            "loss on batch 104 in epoch 64 for the simple nn is: 0.477022260427475\n",
            "loss on batch 105 in epoch 64 for the simple nn is: 0.42961904406547546\n",
            "loss on batch 106 in epoch 64 for the simple nn is: 0.4490031898021698\n",
            "loss on batch 107 in epoch 64 for the simple nn is: 0.4503038823604584\n",
            "loss on batch 108 in epoch 64 for the simple nn is: 0.4964140057563782\n",
            "loss on batch 109 in epoch 64 for the simple nn is: 0.5847428441047668\n",
            "loss on batch 110 in epoch 64 for the simple nn is: 0.34952497482299805\n",
            "loss on batch 111 in epoch 64 for the simple nn is: 0.4719291925430298\n",
            "loss on batch 112 in epoch 64 for the simple nn is: 0.5126022100448608\n",
            "loss on batch 113 in epoch 64 for the simple nn is: 0.5200519561767578\n",
            "loss on batch 114 in epoch 64 for the simple nn is: 0.5261374711990356\n",
            "loss on batch 115 in epoch 64 for the simple nn is: 0.4481104612350464\n",
            "loss on batch 116 in epoch 64 for the simple nn is: 0.6010215282440186\n",
            "loss on batch 117 in epoch 64 for the simple nn is: 0.5400095582008362\n",
            "loss on batch 118 in epoch 64 for the simple nn is: 0.5921058654785156\n",
            "loss on batch 119 in epoch 64 for the simple nn is: 0.5290032625198364\n",
            "loss on batch 120 in epoch 64 for the simple nn is: 0.49353331327438354\n",
            "loss on batch 0 in epoch 65 for the simple nn is: 0.4916492700576782\n",
            "loss on batch 1 in epoch 65 for the simple nn is: 0.6086189150810242\n",
            "loss on batch 2 in epoch 65 for the simple nn is: 0.564529538154602\n",
            "loss on batch 3 in epoch 65 for the simple nn is: 0.5430893898010254\n",
            "loss on batch 4 in epoch 65 for the simple nn is: 0.5678884983062744\n",
            "loss on batch 5 in epoch 65 for the simple nn is: 0.5656922459602356\n",
            "loss on batch 6 in epoch 65 for the simple nn is: 0.5384224653244019\n",
            "loss on batch 7 in epoch 65 for the simple nn is: 0.49796149134635925\n",
            "loss on batch 8 in epoch 65 for the simple nn is: 0.5091081857681274\n",
            "loss on batch 9 in epoch 65 for the simple nn is: 0.631470263004303\n",
            "loss on batch 10 in epoch 65 for the simple nn is: 0.4585892856121063\n",
            "loss on batch 11 in epoch 65 for the simple nn is: 0.5217918753623962\n",
            "loss on batch 12 in epoch 65 for the simple nn is: 0.5857235193252563\n",
            "loss on batch 13 in epoch 65 for the simple nn is: 0.49999910593032837\n",
            "loss on batch 14 in epoch 65 for the simple nn is: 0.49681782722473145\n",
            "loss on batch 15 in epoch 65 for the simple nn is: 0.4595734477043152\n",
            "loss on batch 16 in epoch 65 for the simple nn is: 0.5022414922714233\n",
            "loss on batch 17 in epoch 65 for the simple nn is: 0.4824239909648895\n",
            "loss on batch 18 in epoch 65 for the simple nn is: 0.5204984545707703\n",
            "loss on batch 19 in epoch 65 for the simple nn is: 0.48804908990859985\n",
            "loss on batch 20 in epoch 65 for the simple nn is: 0.4853493571281433\n",
            "loss on batch 21 in epoch 65 for the simple nn is: 0.5318020582199097\n",
            "loss on batch 22 in epoch 65 for the simple nn is: 0.4928191900253296\n",
            "loss on batch 23 in epoch 65 for the simple nn is: 0.4680864214897156\n",
            "loss on batch 24 in epoch 65 for the simple nn is: 0.3976038992404938\n",
            "loss on batch 25 in epoch 65 for the simple nn is: 0.4719196856021881\n",
            "loss on batch 26 in epoch 65 for the simple nn is: 0.519681453704834\n",
            "loss on batch 27 in epoch 65 for the simple nn is: 0.496499240398407\n",
            "loss on batch 28 in epoch 65 for the simple nn is: 0.4814741611480713\n",
            "loss on batch 29 in epoch 65 for the simple nn is: 0.5716062784194946\n",
            "loss on batch 30 in epoch 65 for the simple nn is: 0.49013715982437134\n",
            "loss on batch 31 in epoch 65 for the simple nn is: 0.516386866569519\n",
            "loss on batch 32 in epoch 65 for the simple nn is: 0.468534916639328\n",
            "loss on batch 33 in epoch 65 for the simple nn is: 0.48040395975112915\n",
            "loss on batch 34 in epoch 65 for the simple nn is: 0.45957475900650024\n",
            "loss on batch 35 in epoch 65 for the simple nn is: 0.5136229395866394\n",
            "loss on batch 36 in epoch 65 for the simple nn is: 0.5785857439041138\n",
            "loss on batch 37 in epoch 65 for the simple nn is: 0.4350714385509491\n",
            "loss on batch 38 in epoch 65 for the simple nn is: 0.5290355086326599\n",
            "loss on batch 39 in epoch 65 for the simple nn is: 0.4371863901615143\n",
            "loss on batch 40 in epoch 65 for the simple nn is: 0.5209030508995056\n",
            "loss on batch 41 in epoch 65 for the simple nn is: 0.428928017616272\n",
            "loss on batch 42 in epoch 65 for the simple nn is: 0.4891756772994995\n",
            "loss on batch 43 in epoch 65 for the simple nn is: 0.49762290716171265\n",
            "loss on batch 44 in epoch 65 for the simple nn is: 0.5202301144599915\n",
            "loss on batch 45 in epoch 65 for the simple nn is: 0.508823573589325\n",
            "loss on batch 46 in epoch 65 for the simple nn is: 0.4511217176914215\n",
            "loss on batch 47 in epoch 65 for the simple nn is: 0.4599662721157074\n",
            "loss on batch 48 in epoch 65 for the simple nn is: 0.4834381639957428\n",
            "loss on batch 49 in epoch 65 for the simple nn is: 0.5270711183547974\n",
            "loss on batch 50 in epoch 65 for the simple nn is: 0.44769904017448425\n",
            "loss on batch 51 in epoch 65 for the simple nn is: 0.49547603726387024\n",
            "loss on batch 52 in epoch 65 for the simple nn is: 0.39280417561531067\n",
            "loss on batch 53 in epoch 65 for the simple nn is: 0.3685339689254761\n",
            "loss on batch 54 in epoch 65 for the simple nn is: 0.5414144992828369\n",
            "loss on batch 55 in epoch 65 for the simple nn is: 0.4632345736026764\n",
            "loss on batch 56 in epoch 65 for the simple nn is: 0.48928889632225037\n",
            "loss on batch 57 in epoch 65 for the simple nn is: 0.4968658685684204\n",
            "loss on batch 58 in epoch 65 for the simple nn is: 0.4732167720794678\n",
            "loss on batch 59 in epoch 65 for the simple nn is: 0.4808432459831238\n",
            "loss on batch 60 in epoch 65 for the simple nn is: 0.4774501323699951\n",
            "loss on batch 61 in epoch 65 for the simple nn is: 0.444128155708313\n",
            "loss on batch 62 in epoch 65 for the simple nn is: 0.4726864993572235\n",
            "loss on batch 63 in epoch 65 for the simple nn is: 0.4804363548755646\n",
            "loss on batch 64 in epoch 65 for the simple nn is: 0.45063886046409607\n",
            "loss on batch 65 in epoch 65 for the simple nn is: 0.41658565402030945\n",
            "loss on batch 66 in epoch 65 for the simple nn is: 0.4896811544895172\n",
            "loss on batch 67 in epoch 65 for the simple nn is: 0.4263256788253784\n",
            "loss on batch 68 in epoch 65 for the simple nn is: 0.43061813712120056\n",
            "loss on batch 69 in epoch 65 for the simple nn is: 0.46396782994270325\n",
            "loss on batch 70 in epoch 65 for the simple nn is: 0.4683811068534851\n",
            "loss on batch 71 in epoch 65 for the simple nn is: 0.4953034818172455\n",
            "loss on batch 72 in epoch 65 for the simple nn is: 0.5168470144271851\n",
            "loss on batch 73 in epoch 65 for the simple nn is: 0.5640045404434204\n",
            "loss on batch 74 in epoch 65 for the simple nn is: 0.5068662166595459\n",
            "loss on batch 75 in epoch 65 for the simple nn is: 0.486524760723114\n",
            "loss on batch 76 in epoch 65 for the simple nn is: 0.4767358601093292\n",
            "loss on batch 77 in epoch 65 for the simple nn is: 0.49225741624832153\n",
            "loss on batch 78 in epoch 65 for the simple nn is: 0.49692678451538086\n",
            "loss on batch 79 in epoch 65 for the simple nn is: 0.40506893396377563\n",
            "loss on batch 80 in epoch 65 for the simple nn is: 0.48269596695899963\n",
            "loss on batch 81 in epoch 65 for the simple nn is: 0.40234261751174927\n",
            "loss on batch 82 in epoch 65 for the simple nn is: 0.4715364873409271\n",
            "loss on batch 83 in epoch 65 for the simple nn is: 0.5067257285118103\n",
            "loss on batch 84 in epoch 65 for the simple nn is: 0.48926499485969543\n",
            "loss on batch 85 in epoch 65 for the simple nn is: 0.5091976523399353\n",
            "loss on batch 86 in epoch 65 for the simple nn is: 0.4199739694595337\n",
            "loss on batch 87 in epoch 65 for the simple nn is: 0.4898887276649475\n",
            "loss on batch 88 in epoch 65 for the simple nn is: 0.4189029037952423\n",
            "loss on batch 89 in epoch 65 for the simple nn is: 0.511364758014679\n",
            "loss on batch 90 in epoch 65 for the simple nn is: 0.49747440218925476\n",
            "loss on batch 91 in epoch 65 for the simple nn is: 0.4442138671875\n",
            "loss on batch 92 in epoch 65 for the simple nn is: 0.5334399938583374\n",
            "loss on batch 93 in epoch 65 for the simple nn is: 0.5639532804489136\n",
            "loss on batch 94 in epoch 65 for the simple nn is: 0.40017375349998474\n",
            "loss on batch 95 in epoch 65 for the simple nn is: 0.5190572142601013\n",
            "loss on batch 96 in epoch 65 for the simple nn is: 0.42982402443885803\n",
            "loss on batch 97 in epoch 65 for the simple nn is: 0.49064478278160095\n",
            "loss on batch 98 in epoch 65 for the simple nn is: 0.4410459101200104\n",
            "loss on batch 99 in epoch 65 for the simple nn is: 0.46482107043266296\n",
            "loss on batch 100 in epoch 65 for the simple nn is: 0.5038350224494934\n",
            "loss on batch 101 in epoch 65 for the simple nn is: 0.397643119096756\n",
            "loss on batch 102 in epoch 65 for the simple nn is: 0.37877655029296875\n",
            "loss on batch 103 in epoch 65 for the simple nn is: 0.49672994017601013\n",
            "loss on batch 104 in epoch 65 for the simple nn is: 0.44706931710243225\n",
            "loss on batch 105 in epoch 65 for the simple nn is: 0.423187255859375\n",
            "loss on batch 106 in epoch 65 for the simple nn is: 0.446811705827713\n",
            "loss on batch 107 in epoch 65 for the simple nn is: 0.4396701753139496\n",
            "loss on batch 108 in epoch 65 for the simple nn is: 0.43966740369796753\n",
            "loss on batch 109 in epoch 65 for the simple nn is: 0.5193631649017334\n",
            "loss on batch 110 in epoch 65 for the simple nn is: 0.3390461206436157\n",
            "loss on batch 111 in epoch 65 for the simple nn is: 0.44487708806991577\n",
            "loss on batch 112 in epoch 65 for the simple nn is: 0.49733951687812805\n",
            "loss on batch 113 in epoch 65 for the simple nn is: 0.6247238516807556\n",
            "loss on batch 114 in epoch 65 for the simple nn is: 0.620410144329071\n",
            "loss on batch 115 in epoch 65 for the simple nn is: 0.45193690061569214\n",
            "loss on batch 116 in epoch 65 for the simple nn is: 0.6499950885772705\n",
            "loss on batch 117 in epoch 65 for the simple nn is: 0.5497465133666992\n",
            "loss on batch 118 in epoch 65 for the simple nn is: 0.5939075350761414\n",
            "loss on batch 119 in epoch 65 for the simple nn is: 0.5450503826141357\n",
            "loss on batch 120 in epoch 65 for the simple nn is: 0.5624513030052185\n",
            "loss on batch 0 in epoch 66 for the simple nn is: 0.49174004793167114\n",
            "loss on batch 1 in epoch 66 for the simple nn is: 0.6080017685890198\n",
            "loss on batch 2 in epoch 66 for the simple nn is: 0.5694409608840942\n",
            "loss on batch 3 in epoch 66 for the simple nn is: 0.5327546000480652\n",
            "loss on batch 4 in epoch 66 for the simple nn is: 0.5665752291679382\n",
            "loss on batch 5 in epoch 66 for the simple nn is: 0.5591828227043152\n",
            "loss on batch 6 in epoch 66 for the simple nn is: 0.5481032133102417\n",
            "loss on batch 7 in epoch 66 for the simple nn is: 0.4973156750202179\n",
            "loss on batch 8 in epoch 66 for the simple nn is: 0.46580877900123596\n",
            "loss on batch 9 in epoch 66 for the simple nn is: 0.46310845017433167\n",
            "loss on batch 10 in epoch 66 for the simple nn is: 0.45811864733695984\n",
            "loss on batch 11 in epoch 66 for the simple nn is: 0.5291219353675842\n",
            "loss on batch 12 in epoch 66 for the simple nn is: 0.8980344533920288\n",
            "loss on batch 13 in epoch 66 for the simple nn is: 0.507484495639801\n",
            "loss on batch 14 in epoch 66 for the simple nn is: 0.545772910118103\n",
            "loss on batch 15 in epoch 66 for the simple nn is: 0.49137502908706665\n",
            "loss on batch 16 in epoch 66 for the simple nn is: 0.517120897769928\n",
            "loss on batch 17 in epoch 66 for the simple nn is: 0.5046125650405884\n",
            "loss on batch 18 in epoch 66 for the simple nn is: 0.5081447958946228\n",
            "loss on batch 19 in epoch 66 for the simple nn is: 0.4795883297920227\n",
            "loss on batch 20 in epoch 66 for the simple nn is: 0.6010763049125671\n",
            "loss on batch 21 in epoch 66 for the simple nn is: 0.5179251432418823\n",
            "loss on batch 22 in epoch 66 for the simple nn is: 0.6792904734611511\n",
            "loss on batch 23 in epoch 66 for the simple nn is: 0.47185343503952026\n",
            "loss on batch 24 in epoch 66 for the simple nn is: 0.4293076992034912\n",
            "loss on batch 25 in epoch 66 for the simple nn is: 0.4938427209854126\n",
            "loss on batch 26 in epoch 66 for the simple nn is: 0.6538367867469788\n",
            "loss on batch 27 in epoch 66 for the simple nn is: 0.4896821081638336\n",
            "loss on batch 28 in epoch 66 for the simple nn is: 0.4758273959159851\n",
            "loss on batch 29 in epoch 66 for the simple nn is: 0.585765540599823\n",
            "loss on batch 30 in epoch 66 for the simple nn is: 0.5158794522285461\n",
            "loss on batch 31 in epoch 66 for the simple nn is: 0.577307403087616\n",
            "loss on batch 32 in epoch 66 for the simple nn is: 0.5914444923400879\n",
            "loss on batch 33 in epoch 66 for the simple nn is: 0.5267214179039001\n",
            "loss on batch 34 in epoch 66 for the simple nn is: 0.7164592742919922\n",
            "loss on batch 35 in epoch 66 for the simple nn is: 0.5543912053108215\n",
            "loss on batch 36 in epoch 66 for the simple nn is: 0.522794783115387\n",
            "loss on batch 37 in epoch 66 for the simple nn is: 0.5918190479278564\n",
            "loss on batch 38 in epoch 66 for the simple nn is: 0.5682172179222107\n",
            "loss on batch 39 in epoch 66 for the simple nn is: 0.6942622065544128\n",
            "loss on batch 40 in epoch 66 for the simple nn is: 0.5692813992500305\n",
            "loss on batch 41 in epoch 66 for the simple nn is: 0.6955575942993164\n",
            "loss on batch 42 in epoch 66 for the simple nn is: 0.525174617767334\n",
            "loss on batch 43 in epoch 66 for the simple nn is: 0.5087406039237976\n",
            "loss on batch 44 in epoch 66 for the simple nn is: 0.5474346280097961\n",
            "loss on batch 45 in epoch 66 for the simple nn is: 0.5424586534500122\n",
            "loss on batch 46 in epoch 66 for the simple nn is: 0.6266475319862366\n",
            "loss on batch 47 in epoch 66 for the simple nn is: 0.5430986881256104\n",
            "loss on batch 48 in epoch 66 for the simple nn is: 0.5830575823783875\n",
            "loss on batch 49 in epoch 66 for the simple nn is: 0.5481523871421814\n",
            "loss on batch 50 in epoch 66 for the simple nn is: 0.5558496117591858\n",
            "loss on batch 51 in epoch 66 for the simple nn is: 0.6702519655227661\n",
            "loss on batch 52 in epoch 66 for the simple nn is: 0.4261506199836731\n",
            "loss on batch 53 in epoch 66 for the simple nn is: 0.4514986276626587\n",
            "loss on batch 54 in epoch 66 for the simple nn is: 0.557622492313385\n",
            "loss on batch 55 in epoch 66 for the simple nn is: 0.508049726486206\n",
            "loss on batch 56 in epoch 66 for the simple nn is: 0.5274406671524048\n",
            "loss on batch 57 in epoch 66 for the simple nn is: 0.5330848693847656\n",
            "loss on batch 58 in epoch 66 for the simple nn is: 0.5731157660484314\n",
            "loss on batch 59 in epoch 66 for the simple nn is: 0.5395756959915161\n",
            "loss on batch 60 in epoch 66 for the simple nn is: 0.5326219797134399\n",
            "loss on batch 61 in epoch 66 for the simple nn is: 0.5853045582771301\n",
            "loss on batch 62 in epoch 66 for the simple nn is: 0.4968702793121338\n",
            "loss on batch 63 in epoch 66 for the simple nn is: 0.5269419550895691\n",
            "loss on batch 64 in epoch 66 for the simple nn is: 0.4490443468093872\n",
            "loss on batch 65 in epoch 66 for the simple nn is: 0.5074353218078613\n",
            "loss on batch 66 in epoch 66 for the simple nn is: 0.5932363271713257\n",
            "loss on batch 67 in epoch 66 for the simple nn is: 0.47601622343063354\n",
            "loss on batch 68 in epoch 66 for the simple nn is: 0.5391338467597961\n",
            "loss on batch 69 in epoch 66 for the simple nn is: 0.5560310482978821\n",
            "loss on batch 70 in epoch 66 for the simple nn is: 0.5381499528884888\n",
            "loss on batch 71 in epoch 66 for the simple nn is: 0.6972792148590088\n",
            "loss on batch 72 in epoch 66 for the simple nn is: 0.51808100938797\n",
            "loss on batch 73 in epoch 66 for the simple nn is: 0.5890066623687744\n",
            "loss on batch 74 in epoch 66 for the simple nn is: 0.527826189994812\n",
            "loss on batch 75 in epoch 66 for the simple nn is: 0.5421004891395569\n",
            "loss on batch 76 in epoch 66 for the simple nn is: 0.5986150503158569\n",
            "loss on batch 77 in epoch 66 for the simple nn is: 0.4960446059703827\n",
            "loss on batch 78 in epoch 66 for the simple nn is: 0.5420747995376587\n",
            "loss on batch 79 in epoch 66 for the simple nn is: 0.4970247745513916\n",
            "loss on batch 80 in epoch 66 for the simple nn is: 0.5798214673995972\n",
            "loss on batch 81 in epoch 66 for the simple nn is: 0.5158386826515198\n",
            "loss on batch 82 in epoch 66 for the simple nn is: 0.5333542227745056\n",
            "loss on batch 83 in epoch 66 for the simple nn is: 0.5432473421096802\n",
            "loss on batch 84 in epoch 66 for the simple nn is: 0.6629521250724792\n",
            "loss on batch 85 in epoch 66 for the simple nn is: 0.5223476886749268\n",
            "loss on batch 86 in epoch 66 for the simple nn is: 0.43930795788764954\n",
            "loss on batch 87 in epoch 66 for the simple nn is: 0.5647240281105042\n",
            "loss on batch 88 in epoch 66 for the simple nn is: 0.5160924792289734\n",
            "loss on batch 89 in epoch 66 for the simple nn is: 0.558419942855835\n",
            "loss on batch 90 in epoch 66 for the simple nn is: 0.47548708319664\n",
            "loss on batch 91 in epoch 66 for the simple nn is: 0.4683133363723755\n",
            "loss on batch 92 in epoch 66 for the simple nn is: 0.5452346801757812\n",
            "loss on batch 93 in epoch 66 for the simple nn is: 0.4757508933544159\n",
            "loss on batch 94 in epoch 66 for the simple nn is: 0.4838995337486267\n",
            "loss on batch 95 in epoch 66 for the simple nn is: 0.527823269367218\n",
            "loss on batch 96 in epoch 66 for the simple nn is: 0.46872663497924805\n",
            "loss on batch 97 in epoch 66 for the simple nn is: 0.5214210152626038\n",
            "loss on batch 98 in epoch 66 for the simple nn is: 0.5356112122535706\n",
            "loss on batch 99 in epoch 66 for the simple nn is: 0.49390918016433716\n",
            "loss on batch 100 in epoch 66 for the simple nn is: 0.526597797870636\n",
            "loss on batch 101 in epoch 66 for the simple nn is: 0.451871782541275\n",
            "loss on batch 102 in epoch 66 for the simple nn is: 0.4184218943119049\n",
            "loss on batch 103 in epoch 66 for the simple nn is: 0.6449568271636963\n",
            "loss on batch 104 in epoch 66 for the simple nn is: 0.47850289940834045\n",
            "loss on batch 105 in epoch 66 for the simple nn is: 0.49915653467178345\n",
            "loss on batch 106 in epoch 66 for the simple nn is: 0.47043558955192566\n",
            "loss on batch 107 in epoch 66 for the simple nn is: 0.5032021999359131\n",
            "loss on batch 108 in epoch 66 for the simple nn is: 0.5233315229415894\n",
            "loss on batch 109 in epoch 66 for the simple nn is: 0.5656495690345764\n",
            "loss on batch 110 in epoch 66 for the simple nn is: 0.3771260380744934\n",
            "loss on batch 111 in epoch 66 for the simple nn is: 0.4804590344429016\n",
            "loss on batch 112 in epoch 66 for the simple nn is: 0.5285815000534058\n",
            "loss on batch 113 in epoch 66 for the simple nn is: 0.5410824418067932\n",
            "loss on batch 114 in epoch 66 for the simple nn is: 0.5363619327545166\n",
            "loss on batch 115 in epoch 66 for the simple nn is: 0.551487386226654\n",
            "loss on batch 116 in epoch 66 for the simple nn is: 1.0726733207702637\n",
            "loss on batch 117 in epoch 66 for the simple nn is: 0.5479817390441895\n",
            "loss on batch 118 in epoch 66 for the simple nn is: 0.6355558633804321\n",
            "loss on batch 119 in epoch 66 for the simple nn is: 0.5508955121040344\n",
            "loss on batch 120 in epoch 66 for the simple nn is: 0.5600980520248413\n",
            "loss on batch 0 in epoch 67 for the simple nn is: 0.5226515531539917\n",
            "loss on batch 1 in epoch 67 for the simple nn is: 0.6370204091072083\n",
            "loss on batch 2 in epoch 67 for the simple nn is: 0.6094157099723816\n",
            "loss on batch 3 in epoch 67 for the simple nn is: 0.5575243830680847\n",
            "loss on batch 4 in epoch 67 for the simple nn is: 0.579424262046814\n",
            "loss on batch 5 in epoch 67 for the simple nn is: 0.6278815269470215\n",
            "loss on batch 6 in epoch 67 for the simple nn is: 0.5722259879112244\n",
            "loss on batch 7 in epoch 67 for the simple nn is: 0.5625985264778137\n",
            "loss on batch 8 in epoch 67 for the simple nn is: 0.49903470277786255\n",
            "loss on batch 9 in epoch 67 for the simple nn is: 0.5032097101211548\n",
            "loss on batch 10 in epoch 67 for the simple nn is: 0.47971057891845703\n",
            "loss on batch 11 in epoch 67 for the simple nn is: 0.534309446811676\n",
            "loss on batch 12 in epoch 67 for the simple nn is: 0.5856200456619263\n",
            "loss on batch 13 in epoch 67 for the simple nn is: 0.5193879008293152\n",
            "loss on batch 14 in epoch 67 for the simple nn is: 0.5284662246704102\n",
            "loss on batch 15 in epoch 67 for the simple nn is: 0.5570732355117798\n",
            "loss on batch 16 in epoch 67 for the simple nn is: 0.5436367988586426\n",
            "loss on batch 17 in epoch 67 for the simple nn is: 0.4830545485019684\n",
            "loss on batch 18 in epoch 67 for the simple nn is: 0.47691264748573303\n",
            "loss on batch 19 in epoch 67 for the simple nn is: 0.5347861647605896\n",
            "loss on batch 20 in epoch 67 for the simple nn is: 0.5294290781021118\n",
            "loss on batch 21 in epoch 67 for the simple nn is: 0.5129570364952087\n",
            "loss on batch 22 in epoch 67 for the simple nn is: 0.5063550472259521\n",
            "loss on batch 23 in epoch 67 for the simple nn is: 0.5317243933677673\n",
            "loss on batch 24 in epoch 67 for the simple nn is: 0.42516860365867615\n",
            "loss on batch 25 in epoch 67 for the simple nn is: 0.48532092571258545\n",
            "loss on batch 26 in epoch 67 for the simple nn is: 0.534321129322052\n",
            "loss on batch 27 in epoch 67 for the simple nn is: 0.5122279524803162\n",
            "loss on batch 28 in epoch 67 for the simple nn is: 0.4941181242465973\n",
            "loss on batch 29 in epoch 67 for the simple nn is: 0.618597149848938\n",
            "loss on batch 30 in epoch 67 for the simple nn is: 0.5056862235069275\n",
            "loss on batch 31 in epoch 67 for the simple nn is: 0.5696741342544556\n",
            "loss on batch 32 in epoch 67 for the simple nn is: 0.5112796425819397\n",
            "loss on batch 33 in epoch 67 for the simple nn is: 0.4684053361415863\n",
            "loss on batch 34 in epoch 67 for the simple nn is: 0.46747010946273804\n",
            "loss on batch 35 in epoch 67 for the simple nn is: 0.5105237364768982\n",
            "loss on batch 36 in epoch 67 for the simple nn is: 0.4989382028579712\n",
            "loss on batch 37 in epoch 67 for the simple nn is: 0.43656477332115173\n",
            "loss on batch 38 in epoch 67 for the simple nn is: 0.5460066795349121\n",
            "loss on batch 39 in epoch 67 for the simple nn is: 0.44902095198631287\n",
            "loss on batch 40 in epoch 67 for the simple nn is: 0.5475367307662964\n",
            "loss on batch 41 in epoch 67 for the simple nn is: 0.4693370461463928\n",
            "loss on batch 42 in epoch 67 for the simple nn is: 0.587425708770752\n",
            "loss on batch 43 in epoch 67 for the simple nn is: 0.5198957920074463\n",
            "loss on batch 44 in epoch 67 for the simple nn is: 0.5328249335289001\n",
            "loss on batch 45 in epoch 67 for the simple nn is: 0.4966387450695038\n",
            "loss on batch 46 in epoch 67 for the simple nn is: 0.4436713457107544\n",
            "loss on batch 47 in epoch 67 for the simple nn is: 0.47192758321762085\n",
            "loss on batch 48 in epoch 67 for the simple nn is: 0.5384561419487\n",
            "loss on batch 49 in epoch 67 for the simple nn is: 0.5541441440582275\n",
            "loss on batch 50 in epoch 67 for the simple nn is: 0.4765681326389313\n",
            "loss on batch 51 in epoch 67 for the simple nn is: 0.5050999522209167\n",
            "loss on batch 52 in epoch 67 for the simple nn is: 0.4640623927116394\n",
            "loss on batch 53 in epoch 67 for the simple nn is: 0.37900158762931824\n",
            "loss on batch 54 in epoch 67 for the simple nn is: 0.5665393471717834\n",
            "loss on batch 55 in epoch 67 for the simple nn is: 0.44777417182922363\n",
            "loss on batch 56 in epoch 67 for the simple nn is: 0.8604981303215027\n",
            "loss on batch 57 in epoch 67 for the simple nn is: 0.49703943729400635\n",
            "loss on batch 58 in epoch 67 for the simple nn is: 0.4644995927810669\n",
            "loss on batch 59 in epoch 67 for the simple nn is: 0.5239948630332947\n",
            "loss on batch 60 in epoch 67 for the simple nn is: 0.5046284198760986\n",
            "loss on batch 61 in epoch 67 for the simple nn is: 0.5374139547348022\n",
            "loss on batch 62 in epoch 67 for the simple nn is: 0.7457432746887207\n",
            "loss on batch 63 in epoch 67 for the simple nn is: 0.5141885280609131\n",
            "loss on batch 64 in epoch 67 for the simple nn is: 0.43694382905960083\n",
            "loss on batch 65 in epoch 67 for the simple nn is: 0.4455074071884155\n",
            "loss on batch 66 in epoch 67 for the simple nn is: 0.48825520277023315\n",
            "loss on batch 67 in epoch 67 for the simple nn is: 0.448024719953537\n",
            "loss on batch 68 in epoch 67 for the simple nn is: 0.5479589104652405\n",
            "loss on batch 69 in epoch 67 for the simple nn is: 0.47941961884498596\n",
            "loss on batch 70 in epoch 67 for the simple nn is: 0.4939972758293152\n",
            "loss on batch 71 in epoch 67 for the simple nn is: 0.5262804627418518\n",
            "loss on batch 72 in epoch 67 for the simple nn is: 0.5199134349822998\n",
            "loss on batch 73 in epoch 67 for the simple nn is: 0.5420635342597961\n",
            "loss on batch 74 in epoch 67 for the simple nn is: 0.5258067846298218\n",
            "loss on batch 75 in epoch 67 for the simple nn is: 0.5253760814666748\n",
            "loss on batch 76 in epoch 67 for the simple nn is: 0.5114148855209351\n",
            "loss on batch 77 in epoch 67 for the simple nn is: 0.4469878375530243\n",
            "loss on batch 78 in epoch 67 for the simple nn is: 0.5217450261116028\n",
            "loss on batch 79 in epoch 67 for the simple nn is: 0.4846547842025757\n",
            "loss on batch 80 in epoch 67 for the simple nn is: 0.5175312757492065\n",
            "loss on batch 81 in epoch 67 for the simple nn is: 0.4292551279067993\n",
            "loss on batch 82 in epoch 67 for the simple nn is: 0.4840019941329956\n",
            "loss on batch 83 in epoch 67 for the simple nn is: 0.5087828040122986\n",
            "loss on batch 84 in epoch 67 for the simple nn is: 0.5281894207000732\n",
            "loss on batch 85 in epoch 67 for the simple nn is: 0.513253927230835\n",
            "loss on batch 86 in epoch 67 for the simple nn is: 0.40913620591163635\n",
            "loss on batch 87 in epoch 67 for the simple nn is: 0.5269672870635986\n",
            "loss on batch 88 in epoch 67 for the simple nn is: 0.44716063141822815\n",
            "loss on batch 89 in epoch 67 for the simple nn is: 0.5195052027702332\n",
            "loss on batch 90 in epoch 67 for the simple nn is: 0.4707987904548645\n",
            "loss on batch 91 in epoch 67 for the simple nn is: 0.4434846043586731\n",
            "loss on batch 92 in epoch 67 for the simple nn is: 0.5180709362030029\n",
            "loss on batch 93 in epoch 67 for the simple nn is: 0.4674491584300995\n",
            "loss on batch 94 in epoch 67 for the simple nn is: 0.4252406656742096\n",
            "loss on batch 95 in epoch 67 for the simple nn is: 0.5138237476348877\n",
            "loss on batch 96 in epoch 67 for the simple nn is: 0.4540822505950928\n",
            "loss on batch 97 in epoch 67 for the simple nn is: 0.49589163064956665\n",
            "loss on batch 98 in epoch 67 for the simple nn is: 0.4422735869884491\n",
            "loss on batch 99 in epoch 67 for the simple nn is: 0.4727906584739685\n",
            "loss on batch 100 in epoch 67 for the simple nn is: 0.5041327476501465\n",
            "loss on batch 101 in epoch 67 for the simple nn is: 0.4241035580635071\n",
            "loss on batch 102 in epoch 67 for the simple nn is: 0.3756667673587799\n",
            "loss on batch 103 in epoch 67 for the simple nn is: 0.5103886127471924\n",
            "loss on batch 104 in epoch 67 for the simple nn is: 0.4714999794960022\n",
            "loss on batch 105 in epoch 67 for the simple nn is: 0.44435855746269226\n",
            "loss on batch 106 in epoch 67 for the simple nn is: 0.45941105484962463\n",
            "loss on batch 107 in epoch 67 for the simple nn is: 0.4540178179740906\n",
            "loss on batch 108 in epoch 67 for the simple nn is: 0.43409302830696106\n",
            "loss on batch 109 in epoch 67 for the simple nn is: 0.5886024832725525\n",
            "loss on batch 110 in epoch 67 for the simple nn is: 0.3393491506576538\n",
            "loss on batch 111 in epoch 67 for the simple nn is: 0.44462117552757263\n",
            "loss on batch 112 in epoch 67 for the simple nn is: 0.6866967082023621\n",
            "loss on batch 113 in epoch 67 for the simple nn is: 0.5113237500190735\n",
            "loss on batch 114 in epoch 67 for the simple nn is: 0.5573583841323853\n",
            "loss on batch 115 in epoch 67 for the simple nn is: 0.45397135615348816\n",
            "loss on batch 116 in epoch 67 for the simple nn is: 0.5761768221855164\n",
            "loss on batch 117 in epoch 67 for the simple nn is: 0.539100706577301\n",
            "loss on batch 118 in epoch 67 for the simple nn is: 0.6061608195304871\n",
            "loss on batch 119 in epoch 67 for the simple nn is: 0.5426830053329468\n",
            "loss on batch 120 in epoch 67 for the simple nn is: 0.4915528893470764\n",
            "loss on batch 0 in epoch 68 for the simple nn is: 0.5016423463821411\n",
            "loss on batch 1 in epoch 68 for the simple nn is: 0.6145280599594116\n",
            "loss on batch 2 in epoch 68 for the simple nn is: 0.5808051228523254\n",
            "loss on batch 3 in epoch 68 for the simple nn is: 0.5448435544967651\n",
            "loss on batch 4 in epoch 68 for the simple nn is: 0.5592396259307861\n",
            "loss on batch 5 in epoch 68 for the simple nn is: 0.5783278346061707\n",
            "loss on batch 6 in epoch 68 for the simple nn is: 0.545328676700592\n",
            "loss on batch 7 in epoch 68 for the simple nn is: 0.5370044708251953\n",
            "loss on batch 8 in epoch 68 for the simple nn is: 0.5115066170692444\n",
            "loss on batch 9 in epoch 68 for the simple nn is: 0.526266872882843\n",
            "loss on batch 10 in epoch 68 for the simple nn is: 0.45057913661003113\n",
            "loss on batch 11 in epoch 68 for the simple nn is: 0.5230599045753479\n",
            "loss on batch 12 in epoch 68 for the simple nn is: 0.5125072002410889\n",
            "loss on batch 13 in epoch 68 for the simple nn is: 0.5014045238494873\n",
            "loss on batch 14 in epoch 68 for the simple nn is: 0.4999576807022095\n",
            "loss on batch 15 in epoch 68 for the simple nn is: 0.4631841778755188\n",
            "loss on batch 16 in epoch 68 for the simple nn is: 0.533657968044281\n",
            "loss on batch 17 in epoch 68 for the simple nn is: 0.6185685992240906\n",
            "loss on batch 18 in epoch 68 for the simple nn is: 0.48930925130844116\n",
            "loss on batch 19 in epoch 68 for the simple nn is: 0.4895707070827484\n",
            "loss on batch 20 in epoch 68 for the simple nn is: 0.46036386489868164\n",
            "loss on batch 21 in epoch 68 for the simple nn is: 0.5096251368522644\n",
            "loss on batch 22 in epoch 68 for the simple nn is: 0.4535221755504608\n",
            "loss on batch 23 in epoch 68 for the simple nn is: 0.42537155747413635\n",
            "loss on batch 24 in epoch 68 for the simple nn is: 0.3999103307723999\n",
            "loss on batch 25 in epoch 68 for the simple nn is: 0.47930908203125\n",
            "loss on batch 26 in epoch 68 for the simple nn is: 0.5311065912246704\n",
            "loss on batch 27 in epoch 68 for the simple nn is: 0.4986184239387512\n",
            "loss on batch 28 in epoch 68 for the simple nn is: 0.47238364815711975\n",
            "loss on batch 29 in epoch 68 for the simple nn is: 0.549956202507019\n",
            "loss on batch 30 in epoch 68 for the simple nn is: 0.6079296469688416\n",
            "loss on batch 31 in epoch 68 for the simple nn is: 0.6132510304450989\n",
            "loss on batch 32 in epoch 68 for the simple nn is: 0.4636538028717041\n",
            "loss on batch 33 in epoch 68 for the simple nn is: 0.4684305787086487\n",
            "loss on batch 34 in epoch 68 for the simple nn is: 0.43274688720703125\n",
            "loss on batch 35 in epoch 68 for the simple nn is: 0.5099088549613953\n",
            "loss on batch 36 in epoch 68 for the simple nn is: 0.45954301953315735\n",
            "loss on batch 37 in epoch 68 for the simple nn is: 0.4523082375526428\n",
            "loss on batch 38 in epoch 68 for the simple nn is: 0.5131419897079468\n",
            "loss on batch 39 in epoch 68 for the simple nn is: 0.43514880537986755\n",
            "loss on batch 40 in epoch 68 for the simple nn is: 0.5440045595169067\n",
            "loss on batch 41 in epoch 68 for the simple nn is: 0.4175944924354553\n",
            "loss on batch 42 in epoch 68 for the simple nn is: 0.5703029036521912\n",
            "loss on batch 43 in epoch 68 for the simple nn is: 0.5197859406471252\n",
            "loss on batch 44 in epoch 68 for the simple nn is: 0.504125714302063\n",
            "loss on batch 45 in epoch 68 for the simple nn is: 0.5109620094299316\n",
            "loss on batch 46 in epoch 68 for the simple nn is: 0.4565509557723999\n",
            "loss on batch 47 in epoch 68 for the simple nn is: 0.46977198123931885\n",
            "loss on batch 48 in epoch 68 for the simple nn is: 0.5043272972106934\n",
            "loss on batch 49 in epoch 68 for the simple nn is: 0.5651421546936035\n",
            "loss on batch 50 in epoch 68 for the simple nn is: 0.48275357484817505\n",
            "loss on batch 51 in epoch 68 for the simple nn is: 0.5404067635536194\n",
            "loss on batch 52 in epoch 68 for the simple nn is: 0.39122581481933594\n",
            "loss on batch 53 in epoch 68 for the simple nn is: 0.3919377326965332\n",
            "loss on batch 54 in epoch 68 for the simple nn is: 0.5527582168579102\n",
            "loss on batch 55 in epoch 68 for the simple nn is: 0.44361138343811035\n",
            "loss on batch 56 in epoch 68 for the simple nn is: 0.5145216584205627\n",
            "loss on batch 57 in epoch 68 for the simple nn is: 0.516514241695404\n",
            "loss on batch 58 in epoch 68 for the simple nn is: 0.4635104835033417\n",
            "loss on batch 59 in epoch 68 for the simple nn is: 0.49814221262931824\n",
            "loss on batch 60 in epoch 68 for the simple nn is: 0.473594605922699\n",
            "loss on batch 61 in epoch 68 for the simple nn is: 0.45347365736961365\n",
            "loss on batch 62 in epoch 68 for the simple nn is: 0.4701521396636963\n",
            "loss on batch 63 in epoch 68 for the simple nn is: 0.48577234148979187\n",
            "loss on batch 64 in epoch 68 for the simple nn is: 0.41757529973983765\n",
            "loss on batch 65 in epoch 68 for the simple nn is: 0.4398147463798523\n",
            "loss on batch 66 in epoch 68 for the simple nn is: 0.4790685772895813\n",
            "loss on batch 67 in epoch 68 for the simple nn is: 0.41895753145217896\n",
            "loss on batch 68 in epoch 68 for the simple nn is: 0.41905665397644043\n",
            "loss on batch 69 in epoch 68 for the simple nn is: 0.45748060941696167\n",
            "loss on batch 70 in epoch 68 for the simple nn is: 0.4985102117061615\n",
            "loss on batch 71 in epoch 68 for the simple nn is: 0.5078572034835815\n",
            "loss on batch 72 in epoch 68 for the simple nn is: 0.4698740839958191\n",
            "loss on batch 73 in epoch 68 for the simple nn is: 0.5930808782577515\n",
            "loss on batch 74 in epoch 68 for the simple nn is: 0.49256008863449097\n",
            "loss on batch 75 in epoch 68 for the simple nn is: 0.4723166525363922\n",
            "loss on batch 76 in epoch 68 for the simple nn is: 0.4788687527179718\n",
            "loss on batch 77 in epoch 68 for the simple nn is: 0.47298648953437805\n",
            "loss on batch 78 in epoch 68 for the simple nn is: 0.49219539761543274\n",
            "loss on batch 79 in epoch 68 for the simple nn is: 0.4260464310646057\n",
            "loss on batch 80 in epoch 68 for the simple nn is: 0.48213255405426025\n",
            "loss on batch 81 in epoch 68 for the simple nn is: 0.41231656074523926\n",
            "loss on batch 82 in epoch 68 for the simple nn is: 0.4834269881248474\n",
            "loss on batch 83 in epoch 68 for the simple nn is: 0.49155861139297485\n",
            "loss on batch 84 in epoch 68 for the simple nn is: 0.4989926517009735\n",
            "loss on batch 85 in epoch 68 for the simple nn is: 0.49354663491249084\n",
            "loss on batch 86 in epoch 68 for the simple nn is: 0.40990445017814636\n",
            "loss on batch 87 in epoch 68 for the simple nn is: 0.4929630756378174\n",
            "loss on batch 88 in epoch 68 for the simple nn is: 0.3782808482646942\n",
            "loss on batch 89 in epoch 68 for the simple nn is: 0.5083632469177246\n",
            "loss on batch 90 in epoch 68 for the simple nn is: 0.4883175194263458\n",
            "loss on batch 91 in epoch 68 for the simple nn is: 0.4540916383266449\n",
            "loss on batch 92 in epoch 68 for the simple nn is: 0.5324941277503967\n",
            "loss on batch 93 in epoch 68 for the simple nn is: 0.4679711163043976\n",
            "loss on batch 94 in epoch 68 for the simple nn is: 0.3768886923789978\n",
            "loss on batch 95 in epoch 68 for the simple nn is: 0.5923687815666199\n",
            "loss on batch 96 in epoch 68 for the simple nn is: 0.4077106714248657\n",
            "loss on batch 97 in epoch 68 for the simple nn is: 0.49396026134490967\n",
            "loss on batch 98 in epoch 68 for the simple nn is: 0.4386010468006134\n",
            "loss on batch 99 in epoch 68 for the simple nn is: 0.47480717301368713\n",
            "loss on batch 100 in epoch 68 for the simple nn is: 0.5052435994148254\n",
            "loss on batch 101 in epoch 68 for the simple nn is: 0.4008342921733856\n",
            "loss on batch 102 in epoch 68 for the simple nn is: 0.3823499381542206\n",
            "loss on batch 103 in epoch 68 for the simple nn is: 0.49762412905693054\n",
            "loss on batch 104 in epoch 68 for the simple nn is: 0.45090097188949585\n",
            "loss on batch 105 in epoch 68 for the simple nn is: 0.4240206480026245\n",
            "loss on batch 106 in epoch 68 for the simple nn is: 0.48878681659698486\n",
            "loss on batch 107 in epoch 68 for the simple nn is: 0.5917760729789734\n",
            "loss on batch 108 in epoch 68 for the simple nn is: 0.44361281394958496\n",
            "loss on batch 109 in epoch 68 for the simple nn is: 0.5364593863487244\n",
            "loss on batch 110 in epoch 68 for the simple nn is: 0.3369896113872528\n",
            "loss on batch 111 in epoch 68 for the simple nn is: 0.4600973427295685\n",
            "loss on batch 112 in epoch 68 for the simple nn is: 0.5326122641563416\n",
            "loss on batch 113 in epoch 68 for the simple nn is: 0.49652209877967834\n",
            "loss on batch 114 in epoch 68 for the simple nn is: 0.512758195400238\n",
            "loss on batch 115 in epoch 68 for the simple nn is: 0.4448626637458801\n",
            "loss on batch 116 in epoch 68 for the simple nn is: 0.5812066197395325\n",
            "loss on batch 117 in epoch 68 for the simple nn is: 0.5293972492218018\n",
            "loss on batch 118 in epoch 68 for the simple nn is: 0.5711076855659485\n",
            "loss on batch 119 in epoch 68 for the simple nn is: 0.5278857946395874\n",
            "loss on batch 120 in epoch 68 for the simple nn is: 0.4790990948677063\n",
            "loss on batch 0 in epoch 69 for the simple nn is: 0.5099168419837952\n",
            "loss on batch 1 in epoch 69 for the simple nn is: 0.5985228419303894\n",
            "loss on batch 2 in epoch 69 for the simple nn is: 0.5670393109321594\n",
            "loss on batch 3 in epoch 69 for the simple nn is: 0.5380889773368835\n",
            "loss on batch 4 in epoch 69 for the simple nn is: 0.551673173904419\n",
            "loss on batch 5 in epoch 69 for the simple nn is: 0.5491586327552795\n",
            "loss on batch 6 in epoch 69 for the simple nn is: 0.5257063508033752\n",
            "loss on batch 7 in epoch 69 for the simple nn is: 0.5124435424804688\n",
            "loss on batch 8 in epoch 69 for the simple nn is: 0.4707149565219879\n",
            "loss on batch 9 in epoch 69 for the simple nn is: 0.4780676066875458\n",
            "loss on batch 10 in epoch 69 for the simple nn is: 0.46608322858810425\n",
            "loss on batch 11 in epoch 69 for the simple nn is: 0.5535430312156677\n",
            "loss on batch 12 in epoch 69 for the simple nn is: 0.5168079733848572\n",
            "loss on batch 13 in epoch 69 for the simple nn is: 0.5230627655982971\n",
            "loss on batch 14 in epoch 69 for the simple nn is: 0.4989296495914459\n",
            "loss on batch 15 in epoch 69 for the simple nn is: 0.47167885303497314\n",
            "loss on batch 16 in epoch 69 for the simple nn is: 0.5302392840385437\n",
            "loss on batch 17 in epoch 69 for the simple nn is: 0.48004767298698425\n",
            "loss on batch 18 in epoch 69 for the simple nn is: 0.46900475025177\n",
            "loss on batch 19 in epoch 69 for the simple nn is: 0.5113288164138794\n",
            "loss on batch 20 in epoch 69 for the simple nn is: 0.4612540006637573\n",
            "loss on batch 21 in epoch 69 for the simple nn is: 0.513897180557251\n",
            "loss on batch 22 in epoch 69 for the simple nn is: 0.4613434672355652\n",
            "loss on batch 23 in epoch 69 for the simple nn is: 0.4471976161003113\n",
            "loss on batch 24 in epoch 69 for the simple nn is: 0.39480116963386536\n",
            "loss on batch 25 in epoch 69 for the simple nn is: 0.4655486047267914\n",
            "loss on batch 26 in epoch 69 for the simple nn is: 0.5183840394020081\n",
            "loss on batch 27 in epoch 69 for the simple nn is: 0.4923119843006134\n",
            "loss on batch 28 in epoch 69 for the simple nn is: 0.4762934446334839\n",
            "loss on batch 29 in epoch 69 for the simple nn is: 0.569160521030426\n",
            "loss on batch 30 in epoch 69 for the simple nn is: 0.4929489195346832\n",
            "loss on batch 31 in epoch 69 for the simple nn is: 0.5814242362976074\n",
            "loss on batch 32 in epoch 69 for the simple nn is: 0.45392629504203796\n",
            "loss on batch 33 in epoch 69 for the simple nn is: 0.5020197033882141\n",
            "loss on batch 34 in epoch 69 for the simple nn is: 0.43866971135139465\n",
            "loss on batch 35 in epoch 69 for the simple nn is: 0.5061532258987427\n",
            "loss on batch 36 in epoch 69 for the simple nn is: 0.48928362131118774\n",
            "loss on batch 37 in epoch 69 for the simple nn is: 0.41742536425590515\n",
            "loss on batch 38 in epoch 69 for the simple nn is: 0.5086772441864014\n",
            "loss on batch 39 in epoch 69 for the simple nn is: 0.5347217917442322\n",
            "loss on batch 40 in epoch 69 for the simple nn is: 0.5883884429931641\n",
            "loss on batch 41 in epoch 69 for the simple nn is: 0.45892128348350525\n",
            "loss on batch 42 in epoch 69 for the simple nn is: 0.48496213555336\n",
            "loss on batch 43 in epoch 69 for the simple nn is: 0.5131102204322815\n",
            "loss on batch 44 in epoch 69 for the simple nn is: 0.5166689157485962\n",
            "loss on batch 45 in epoch 69 for the simple nn is: 0.5008934140205383\n",
            "loss on batch 46 in epoch 69 for the simple nn is: 0.4284529387950897\n",
            "loss on batch 47 in epoch 69 for the simple nn is: 0.45022404193878174\n",
            "loss on batch 48 in epoch 69 for the simple nn is: 0.47655048966407776\n",
            "loss on batch 49 in epoch 69 for the simple nn is: 0.528266429901123\n",
            "loss on batch 50 in epoch 69 for the simple nn is: 0.48257210850715637\n",
            "loss on batch 51 in epoch 69 for the simple nn is: 0.5130174160003662\n",
            "loss on batch 52 in epoch 69 for the simple nn is: 0.38436028361320496\n",
            "loss on batch 53 in epoch 69 for the simple nn is: 0.3589441180229187\n",
            "loss on batch 54 in epoch 69 for the simple nn is: 0.6106666922569275\n",
            "loss on batch 55 in epoch 69 for the simple nn is: 0.4433805048465729\n",
            "loss on batch 56 in epoch 69 for the simple nn is: 0.5047566294670105\n",
            "loss on batch 57 in epoch 69 for the simple nn is: 0.5023478865623474\n",
            "loss on batch 58 in epoch 69 for the simple nn is: 0.4585479199886322\n",
            "loss on batch 59 in epoch 69 for the simple nn is: 0.4895058274269104\n",
            "loss on batch 60 in epoch 69 for the simple nn is: 0.4616594612598419\n",
            "loss on batch 61 in epoch 69 for the simple nn is: 0.42934152483940125\n",
            "loss on batch 62 in epoch 69 for the simple nn is: 0.49955591559410095\n",
            "loss on batch 63 in epoch 69 for the simple nn is: 0.4700038433074951\n",
            "loss on batch 64 in epoch 69 for the simple nn is: 0.41178813576698303\n",
            "loss on batch 65 in epoch 69 for the simple nn is: 0.5160936713218689\n",
            "loss on batch 66 in epoch 69 for the simple nn is: 0.4745343327522278\n",
            "loss on batch 67 in epoch 69 for the simple nn is: 0.41127365827560425\n",
            "loss on batch 68 in epoch 69 for the simple nn is: 0.4702245593070984\n",
            "loss on batch 69 in epoch 69 for the simple nn is: 0.46102288365364075\n",
            "loss on batch 70 in epoch 69 for the simple nn is: 0.48895254731178284\n",
            "loss on batch 71 in epoch 69 for the simple nn is: 0.47606658935546875\n",
            "loss on batch 72 in epoch 69 for the simple nn is: 0.49869126081466675\n",
            "loss on batch 73 in epoch 69 for the simple nn is: 0.5284107327461243\n",
            "loss on batch 74 in epoch 69 for the simple nn is: 0.5400915145874023\n",
            "loss on batch 75 in epoch 69 for the simple nn is: 0.5860853791236877\n",
            "loss on batch 76 in epoch 69 for the simple nn is: 0.49031707644462585\n",
            "loss on batch 77 in epoch 69 for the simple nn is: 0.43742823600769043\n",
            "loss on batch 78 in epoch 69 for the simple nn is: 0.4837435781955719\n",
            "loss on batch 79 in epoch 69 for the simple nn is: 0.42000919580459595\n",
            "loss on batch 80 in epoch 69 for the simple nn is: 0.4745897352695465\n",
            "loss on batch 81 in epoch 69 for the simple nn is: 0.4107954204082489\n",
            "loss on batch 82 in epoch 69 for the simple nn is: 0.47805386781692505\n",
            "loss on batch 83 in epoch 69 for the simple nn is: 0.5559274554252625\n",
            "loss on batch 84 in epoch 69 for the simple nn is: 0.4880215525627136\n",
            "loss on batch 85 in epoch 69 for the simple nn is: 0.5049225091934204\n",
            "loss on batch 86 in epoch 69 for the simple nn is: 0.4561101496219635\n",
            "loss on batch 87 in epoch 69 for the simple nn is: 0.5128376483917236\n",
            "loss on batch 88 in epoch 69 for the simple nn is: 0.3827841877937317\n",
            "loss on batch 89 in epoch 69 for the simple nn is: 0.5061782002449036\n",
            "loss on batch 90 in epoch 69 for the simple nn is: 0.4693078398704529\n",
            "loss on batch 91 in epoch 69 for the simple nn is: 0.45513224601745605\n",
            "loss on batch 92 in epoch 69 for the simple nn is: 0.530826210975647\n",
            "loss on batch 93 in epoch 69 for the simple nn is: 0.44534003734588623\n",
            "loss on batch 94 in epoch 69 for the simple nn is: 0.39882731437683105\n",
            "loss on batch 95 in epoch 69 for the simple nn is: 0.4919058382511139\n",
            "loss on batch 96 in epoch 69 for the simple nn is: 0.43136000633239746\n",
            "loss on batch 97 in epoch 69 for the simple nn is: 0.4821949601173401\n",
            "loss on batch 98 in epoch 69 for the simple nn is: 0.4382617473602295\n",
            "loss on batch 99 in epoch 69 for the simple nn is: 0.4816519021987915\n",
            "loss on batch 100 in epoch 69 for the simple nn is: 0.49373728036880493\n",
            "loss on batch 101 in epoch 69 for the simple nn is: 0.3969559669494629\n",
            "loss on batch 102 in epoch 69 for the simple nn is: 0.381754606962204\n",
            "loss on batch 103 in epoch 69 for the simple nn is: 0.5002943277359009\n",
            "loss on batch 104 in epoch 69 for the simple nn is: 0.4490925371646881\n",
            "loss on batch 105 in epoch 69 for the simple nn is: 0.4256792366504669\n",
            "loss on batch 106 in epoch 69 for the simple nn is: 0.4513864517211914\n",
            "loss on batch 107 in epoch 69 for the simple nn is: 0.45976191759109497\n",
            "loss on batch 108 in epoch 69 for the simple nn is: 0.42485368251800537\n",
            "loss on batch 109 in epoch 69 for the simple nn is: 0.5199781656265259\n",
            "loss on batch 110 in epoch 69 for the simple nn is: 0.34245753288269043\n",
            "loss on batch 111 in epoch 69 for the simple nn is: 0.45708397030830383\n",
            "loss on batch 112 in epoch 69 for the simple nn is: 0.5211939215660095\n",
            "loss on batch 113 in epoch 69 for the simple nn is: 0.49249571561813354\n",
            "loss on batch 114 in epoch 69 for the simple nn is: 0.5061826705932617\n",
            "loss on batch 115 in epoch 69 for the simple nn is: 0.43765756487846375\n",
            "loss on batch 116 in epoch 69 for the simple nn is: 0.570598840713501\n",
            "loss on batch 117 in epoch 69 for the simple nn is: 0.5338762402534485\n",
            "loss on batch 118 in epoch 69 for the simple nn is: 0.5559220314025879\n",
            "loss on batch 119 in epoch 69 for the simple nn is: 0.5245599746704102\n",
            "loss on batch 120 in epoch 69 for the simple nn is: 0.4747806787490845\n",
            "loss on batch 0 in epoch 70 for the simple nn is: 0.4746241867542267\n",
            "loss on batch 1 in epoch 70 for the simple nn is: 0.5961547493934631\n",
            "loss on batch 2 in epoch 70 for the simple nn is: 0.5526871681213379\n",
            "loss on batch 3 in epoch 70 for the simple nn is: 0.5322192311286926\n",
            "loss on batch 4 in epoch 70 for the simple nn is: 0.5384994745254517\n",
            "loss on batch 5 in epoch 70 for the simple nn is: 0.5449347496032715\n",
            "loss on batch 6 in epoch 70 for the simple nn is: 0.5578445196151733\n",
            "loss on batch 7 in epoch 70 for the simple nn is: 0.5249367356300354\n",
            "loss on batch 8 in epoch 70 for the simple nn is: 0.45520341396331787\n",
            "loss on batch 9 in epoch 70 for the simple nn is: 0.4724735617637634\n",
            "loss on batch 10 in epoch 70 for the simple nn is: 0.4488036334514618\n",
            "loss on batch 11 in epoch 70 for the simple nn is: 0.5148283243179321\n",
            "loss on batch 12 in epoch 70 for the simple nn is: 0.5118818283081055\n",
            "loss on batch 13 in epoch 70 for the simple nn is: 0.5339717864990234\n",
            "loss on batch 14 in epoch 70 for the simple nn is: 0.48760688304901123\n",
            "loss on batch 15 in epoch 70 for the simple nn is: 0.47110119462013245\n",
            "loss on batch 16 in epoch 70 for the simple nn is: 0.5177497863769531\n",
            "loss on batch 17 in epoch 70 for the simple nn is: 0.4732634127140045\n",
            "loss on batch 18 in epoch 70 for the simple nn is: 0.4593564569950104\n",
            "loss on batch 19 in epoch 70 for the simple nn is: 0.4927818477153778\n",
            "loss on batch 20 in epoch 70 for the simple nn is: 0.4562429189682007\n",
            "loss on batch 21 in epoch 70 for the simple nn is: 0.5123902559280396\n",
            "loss on batch 22 in epoch 70 for the simple nn is: 0.47079893946647644\n",
            "loss on batch 23 in epoch 70 for the simple nn is: 0.5185579657554626\n",
            "loss on batch 24 in epoch 70 for the simple nn is: 0.4390549659729004\n",
            "loss on batch 25 in epoch 70 for the simple nn is: 0.45660120248794556\n",
            "loss on batch 26 in epoch 70 for the simple nn is: 0.5360512137413025\n",
            "loss on batch 27 in epoch 70 for the simple nn is: 0.4647766649723053\n",
            "loss on batch 28 in epoch 70 for the simple nn is: 0.45461809635162354\n",
            "loss on batch 29 in epoch 70 for the simple nn is: 0.5410573482513428\n",
            "loss on batch 30 in epoch 70 for the simple nn is: 0.4868769943714142\n",
            "loss on batch 31 in epoch 70 for the simple nn is: 0.5319708585739136\n",
            "loss on batch 32 in epoch 70 for the simple nn is: 0.4501308798789978\n",
            "loss on batch 33 in epoch 70 for the simple nn is: 0.46870577335357666\n",
            "loss on batch 34 in epoch 70 for the simple nn is: 0.43875566124916077\n",
            "loss on batch 35 in epoch 70 for the simple nn is: 0.4833495020866394\n",
            "loss on batch 36 in epoch 70 for the simple nn is: 0.4883712828159332\n",
            "loss on batch 37 in epoch 70 for the simple nn is: 0.44817885756492615\n",
            "loss on batch 38 in epoch 70 for the simple nn is: 0.5041987299919128\n",
            "loss on batch 39 in epoch 70 for the simple nn is: 0.419709175825119\n",
            "loss on batch 40 in epoch 70 for the simple nn is: 0.504888653755188\n",
            "loss on batch 41 in epoch 70 for the simple nn is: 0.4068887233734131\n",
            "loss on batch 42 in epoch 70 for the simple nn is: 0.4858296513557434\n",
            "loss on batch 43 in epoch 70 for the simple nn is: 0.5074586868286133\n",
            "loss on batch 44 in epoch 70 for the simple nn is: 0.49054157733917236\n",
            "loss on batch 45 in epoch 70 for the simple nn is: 0.4904562830924988\n",
            "loss on batch 46 in epoch 70 for the simple nn is: 0.42859572172164917\n",
            "loss on batch 47 in epoch 70 for the simple nn is: 0.4468131363391876\n",
            "loss on batch 48 in epoch 70 for the simple nn is: 0.4904235601425171\n",
            "loss on batch 49 in epoch 70 for the simple nn is: 0.5305948853492737\n",
            "loss on batch 50 in epoch 70 for the simple nn is: 0.4537127912044525\n",
            "loss on batch 51 in epoch 70 for the simple nn is: 0.48043856024742126\n",
            "loss on batch 52 in epoch 70 for the simple nn is: 0.38053470849990845\n",
            "loss on batch 53 in epoch 70 for the simple nn is: 0.3480270206928253\n",
            "loss on batch 54 in epoch 70 for the simple nn is: 0.6752092838287354\n",
            "loss on batch 55 in epoch 70 for the simple nn is: 0.48267921805381775\n",
            "loss on batch 56 in epoch 70 for the simple nn is: 0.4925116300582886\n",
            "loss on batch 57 in epoch 70 for the simple nn is: 0.4887732267379761\n",
            "loss on batch 58 in epoch 70 for the simple nn is: 0.450918585062027\n",
            "loss on batch 59 in epoch 70 for the simple nn is: 0.48313161730766296\n",
            "loss on batch 60 in epoch 70 for the simple nn is: 0.457747220993042\n",
            "loss on batch 61 in epoch 70 for the simple nn is: 0.43000665307044983\n",
            "loss on batch 62 in epoch 70 for the simple nn is: 0.47953474521636963\n",
            "loss on batch 63 in epoch 70 for the simple nn is: 0.5638463497161865\n",
            "loss on batch 64 in epoch 70 for the simple nn is: 0.4967721998691559\n",
            "loss on batch 65 in epoch 70 for the simple nn is: 0.41381609439849854\n",
            "loss on batch 66 in epoch 70 for the simple nn is: 0.4912645220756531\n",
            "loss on batch 67 in epoch 70 for the simple nn is: 0.4315599501132965\n",
            "loss on batch 68 in epoch 70 for the simple nn is: 0.4127190113067627\n",
            "loss on batch 69 in epoch 70 for the simple nn is: 0.4574100971221924\n",
            "loss on batch 70 in epoch 70 for the simple nn is: 0.480754554271698\n",
            "loss on batch 71 in epoch 70 for the simple nn is: 0.4937257170677185\n",
            "loss on batch 72 in epoch 70 for the simple nn is: 0.4742581844329834\n",
            "loss on batch 73 in epoch 70 for the simple nn is: 0.5218842029571533\n",
            "loss on batch 74 in epoch 70 for the simple nn is: 0.5033046007156372\n",
            "loss on batch 75 in epoch 70 for the simple nn is: 0.4738767743110657\n",
            "loss on batch 76 in epoch 70 for the simple nn is: 0.48991623520851135\n",
            "loss on batch 77 in epoch 70 for the simple nn is: 0.4390650689601898\n",
            "loss on batch 78 in epoch 70 for the simple nn is: 0.47660884261131287\n",
            "loss on batch 79 in epoch 70 for the simple nn is: 0.42917194962501526\n",
            "loss on batch 80 in epoch 70 for the simple nn is: 0.483498215675354\n",
            "loss on batch 81 in epoch 70 for the simple nn is: 0.4282533824443817\n",
            "loss on batch 82 in epoch 70 for the simple nn is: 0.5125223398208618\n",
            "loss on batch 83 in epoch 70 for the simple nn is: 0.4780237674713135\n",
            "loss on batch 84 in epoch 70 for the simple nn is: 0.4850528836250305\n",
            "loss on batch 85 in epoch 70 for the simple nn is: 0.5052708983421326\n",
            "loss on batch 86 in epoch 70 for the simple nn is: 0.3965989947319031\n",
            "loss on batch 87 in epoch 70 for the simple nn is: 0.5075146555900574\n",
            "loss on batch 88 in epoch 70 for the simple nn is: 0.37459465861320496\n",
            "loss on batch 89 in epoch 70 for the simple nn is: 0.4963454008102417\n",
            "loss on batch 90 in epoch 70 for the simple nn is: 0.4744320809841156\n",
            "loss on batch 91 in epoch 70 for the simple nn is: 0.4467329978942871\n",
            "loss on batch 92 in epoch 70 for the simple nn is: 0.5373159646987915\n",
            "loss on batch 93 in epoch 70 for the simple nn is: 0.4534021019935608\n",
            "loss on batch 94 in epoch 70 for the simple nn is: 0.3797142505645752\n",
            "loss on batch 95 in epoch 70 for the simple nn is: 0.48765280842781067\n",
            "loss on batch 96 in epoch 70 for the simple nn is: 0.46678659319877625\n",
            "loss on batch 97 in epoch 70 for the simple nn is: 0.491251140832901\n",
            "loss on batch 98 in epoch 70 for the simple nn is: 0.4360673129558563\n",
            "loss on batch 99 in epoch 70 for the simple nn is: 0.4677737355232239\n",
            "loss on batch 100 in epoch 70 for the simple nn is: 0.47918614745140076\n",
            "loss on batch 101 in epoch 70 for the simple nn is: 0.4209744334220886\n",
            "loss on batch 102 in epoch 70 for the simple nn is: 0.356481671333313\n",
            "loss on batch 103 in epoch 70 for the simple nn is: 0.4940941631793976\n",
            "loss on batch 104 in epoch 70 for the simple nn is: 0.4270176291465759\n",
            "loss on batch 105 in epoch 70 for the simple nn is: 0.44469770789146423\n",
            "loss on batch 106 in epoch 70 for the simple nn is: 0.45091596245765686\n",
            "loss on batch 107 in epoch 70 for the simple nn is: 0.6053854823112488\n",
            "loss on batch 108 in epoch 70 for the simple nn is: 0.42290616035461426\n",
            "loss on batch 109 in epoch 70 for the simple nn is: 0.5199397206306458\n",
            "loss on batch 110 in epoch 70 for the simple nn is: 0.39296817779541016\n",
            "loss on batch 111 in epoch 70 for the simple nn is: 0.44120219349861145\n",
            "loss on batch 112 in epoch 70 for the simple nn is: 0.5112494230270386\n",
            "loss on batch 113 in epoch 70 for the simple nn is: 0.502238392829895\n",
            "loss on batch 114 in epoch 70 for the simple nn is: 0.5166904330253601\n",
            "loss on batch 115 in epoch 70 for the simple nn is: 0.4899660348892212\n",
            "loss on batch 116 in epoch 70 for the simple nn is: 0.6445215940475464\n",
            "loss on batch 117 in epoch 70 for the simple nn is: 0.5545668601989746\n",
            "loss on batch 118 in epoch 70 for the simple nn is: 0.5542604923248291\n",
            "loss on batch 119 in epoch 70 for the simple nn is: 0.5258647799491882\n",
            "loss on batch 120 in epoch 70 for the simple nn is: 0.4713729918003082\n",
            "loss on batch 0 in epoch 71 for the simple nn is: 0.525715172290802\n",
            "loss on batch 1 in epoch 71 for the simple nn is: 0.6008809208869934\n",
            "loss on batch 2 in epoch 71 for the simple nn is: 0.5748053789138794\n",
            "loss on batch 3 in epoch 71 for the simple nn is: 0.5471590161323547\n",
            "loss on batch 4 in epoch 71 for the simple nn is: 0.551070511341095\n",
            "loss on batch 5 in epoch 71 for the simple nn is: 0.5619089603424072\n",
            "loss on batch 6 in epoch 71 for the simple nn is: 0.5346857309341431\n",
            "loss on batch 7 in epoch 71 for the simple nn is: 0.5298064351081848\n",
            "loss on batch 8 in epoch 71 for the simple nn is: 0.47651195526123047\n",
            "loss on batch 9 in epoch 71 for the simple nn is: 0.47509920597076416\n",
            "loss on batch 10 in epoch 71 for the simple nn is: 0.4436163902282715\n",
            "loss on batch 11 in epoch 71 for the simple nn is: 0.5068210363388062\n",
            "loss on batch 12 in epoch 71 for the simple nn is: 0.5347915887832642\n",
            "loss on batch 13 in epoch 71 for the simple nn is: 0.5194019079208374\n",
            "loss on batch 14 in epoch 71 for the simple nn is: 0.4964199364185333\n",
            "loss on batch 15 in epoch 71 for the simple nn is: 0.48211875557899475\n",
            "loss on batch 16 in epoch 71 for the simple nn is: 0.4915125072002411\n",
            "loss on batch 17 in epoch 71 for the simple nn is: 0.49020546674728394\n",
            "loss on batch 18 in epoch 71 for the simple nn is: 0.46781399846076965\n",
            "loss on batch 19 in epoch 71 for the simple nn is: 0.49235183000564575\n",
            "loss on batch 20 in epoch 71 for the simple nn is: 0.5620322227478027\n",
            "loss on batch 21 in epoch 71 for the simple nn is: 0.5542556643486023\n",
            "loss on batch 22 in epoch 71 for the simple nn is: 0.437942773103714\n",
            "loss on batch 23 in epoch 71 for the simple nn is: 0.44932523369789124\n",
            "loss on batch 24 in epoch 71 for the simple nn is: 0.38914594054222107\n",
            "loss on batch 25 in epoch 71 for the simple nn is: 0.43839383125305176\n",
            "loss on batch 26 in epoch 71 for the simple nn is: 0.6115221381187439\n",
            "loss on batch 27 in epoch 71 for the simple nn is: 0.47041815519332886\n",
            "loss on batch 28 in epoch 71 for the simple nn is: 0.45690426230430603\n",
            "loss on batch 29 in epoch 71 for the simple nn is: 0.5541385412216187\n",
            "loss on batch 30 in epoch 71 for the simple nn is: 0.4904850423336029\n",
            "loss on batch 31 in epoch 71 for the simple nn is: 0.5254753232002258\n",
            "loss on batch 32 in epoch 71 for the simple nn is: 0.44900810718536377\n",
            "loss on batch 33 in epoch 71 for the simple nn is: 0.48722535371780396\n",
            "loss on batch 34 in epoch 71 for the simple nn is: 0.4382839500904083\n",
            "loss on batch 35 in epoch 71 for the simple nn is: 0.46841272711753845\n",
            "loss on batch 36 in epoch 71 for the simple nn is: 0.4755690395832062\n",
            "loss on batch 37 in epoch 71 for the simple nn is: 0.4041282832622528\n",
            "loss on batch 38 in epoch 71 for the simple nn is: 0.5198482275009155\n",
            "loss on batch 39 in epoch 71 for the simple nn is: 0.42115914821624756\n",
            "loss on batch 40 in epoch 71 for the simple nn is: 0.5039280652999878\n",
            "loss on batch 41 in epoch 71 for the simple nn is: 0.4069894254207611\n",
            "loss on batch 42 in epoch 71 for the simple nn is: 0.46804022789001465\n",
            "loss on batch 43 in epoch 71 for the simple nn is: 0.49131935834884644\n",
            "loss on batch 44 in epoch 71 for the simple nn is: 0.48126277327537537\n",
            "loss on batch 45 in epoch 71 for the simple nn is: 0.490857869386673\n",
            "loss on batch 46 in epoch 71 for the simple nn is: 0.4282037913799286\n",
            "loss on batch 47 in epoch 71 for the simple nn is: 0.44499170780181885\n",
            "loss on batch 48 in epoch 71 for the simple nn is: 0.516792356967926\n",
            "loss on batch 49 in epoch 71 for the simple nn is: 0.529889702796936\n",
            "loss on batch 50 in epoch 71 for the simple nn is: 0.44429484009742737\n",
            "loss on batch 51 in epoch 71 for the simple nn is: 0.4792316257953644\n",
            "loss on batch 52 in epoch 71 for the simple nn is: 0.38337442278862\n",
            "loss on batch 53 in epoch 71 for the simple nn is: 0.3394192159175873\n",
            "loss on batch 54 in epoch 71 for the simple nn is: 0.542029857635498\n",
            "loss on batch 55 in epoch 71 for the simple nn is: 0.4437425136566162\n",
            "loss on batch 56 in epoch 71 for the simple nn is: 0.48469167947769165\n",
            "loss on batch 57 in epoch 71 for the simple nn is: 0.4960910379886627\n",
            "loss on batch 58 in epoch 71 for the simple nn is: 0.4515624940395355\n",
            "loss on batch 59 in epoch 71 for the simple nn is: 0.49218788743019104\n",
            "loss on batch 60 in epoch 71 for the simple nn is: 0.46488454937934875\n",
            "loss on batch 61 in epoch 71 for the simple nn is: 0.4295744299888611\n",
            "loss on batch 62 in epoch 71 for the simple nn is: 0.45665377378463745\n",
            "loss on batch 63 in epoch 71 for the simple nn is: 0.46391528844833374\n",
            "loss on batch 64 in epoch 71 for the simple nn is: 0.4089565873146057\n",
            "loss on batch 65 in epoch 71 for the simple nn is: 0.41310015320777893\n",
            "loss on batch 66 in epoch 71 for the simple nn is: 0.4773154854774475\n",
            "loss on batch 67 in epoch 71 for the simple nn is: 0.4056240916252136\n",
            "loss on batch 68 in epoch 71 for the simple nn is: 0.42437565326690674\n",
            "loss on batch 69 in epoch 71 for the simple nn is: 0.45550936460494995\n",
            "loss on batch 70 in epoch 71 for the simple nn is: 0.4716450572013855\n",
            "loss on batch 71 in epoch 71 for the simple nn is: 0.4712405204772949\n",
            "loss on batch 72 in epoch 71 for the simple nn is: 0.5029544830322266\n",
            "loss on batch 73 in epoch 71 for the simple nn is: 0.5181339383125305\n",
            "loss on batch 74 in epoch 71 for the simple nn is: 0.48580512404441833\n",
            "loss on batch 75 in epoch 71 for the simple nn is: 0.4638207256793976\n",
            "loss on batch 76 in epoch 71 for the simple nn is: 0.4755744934082031\n",
            "loss on batch 77 in epoch 71 for the simple nn is: 0.410989373922348\n",
            "loss on batch 78 in epoch 71 for the simple nn is: 0.5275383591651917\n",
            "loss on batch 79 in epoch 71 for the simple nn is: 0.40564075112342834\n",
            "loss on batch 80 in epoch 71 for the simple nn is: 0.45725831389427185\n",
            "loss on batch 81 in epoch 71 for the simple nn is: 0.4042815864086151\n",
            "loss on batch 82 in epoch 71 for the simple nn is: 0.46304085850715637\n",
            "loss on batch 83 in epoch 71 for the simple nn is: 0.4793252944946289\n",
            "loss on batch 84 in epoch 71 for the simple nn is: 0.4720253050327301\n",
            "loss on batch 85 in epoch 71 for the simple nn is: 0.4903329908847809\n",
            "loss on batch 86 in epoch 71 for the simple nn is: 0.4095093011856079\n",
            "loss on batch 87 in epoch 71 for the simple nn is: 0.49798983335494995\n",
            "loss on batch 88 in epoch 71 for the simple nn is: 0.3585876226425171\n",
            "loss on batch 89 in epoch 71 for the simple nn is: 0.48074883222579956\n",
            "loss on batch 90 in epoch 71 for the simple nn is: 0.47474801540374756\n",
            "loss on batch 91 in epoch 71 for the simple nn is: 0.4276124835014343\n",
            "loss on batch 92 in epoch 71 for the simple nn is: 0.5284605622291565\n",
            "loss on batch 93 in epoch 71 for the simple nn is: 0.4512828588485718\n",
            "loss on batch 94 in epoch 71 for the simple nn is: 0.390171617269516\n",
            "loss on batch 95 in epoch 71 for the simple nn is: 0.47409191727638245\n",
            "loss on batch 96 in epoch 71 for the simple nn is: 0.418386310338974\n",
            "loss on batch 97 in epoch 71 for the simple nn is: 0.4835283160209656\n",
            "loss on batch 98 in epoch 71 for the simple nn is: 0.4340205192565918\n",
            "loss on batch 99 in epoch 71 for the simple nn is: 0.43460771441459656\n",
            "loss on batch 100 in epoch 71 for the simple nn is: 0.47501254081726074\n",
            "loss on batch 101 in epoch 71 for the simple nn is: 0.4057636260986328\n",
            "loss on batch 102 in epoch 71 for the simple nn is: 0.3699416220188141\n",
            "loss on batch 103 in epoch 71 for the simple nn is: 0.4866639971733093\n",
            "loss on batch 104 in epoch 71 for the simple nn is: 0.4298539161682129\n",
            "loss on batch 105 in epoch 71 for the simple nn is: 0.44828879833221436\n",
            "loss on batch 106 in epoch 71 for the simple nn is: 0.48455458879470825\n",
            "loss on batch 107 in epoch 71 for the simple nn is: 0.45195233821868896\n",
            "loss on batch 108 in epoch 71 for the simple nn is: 0.42306089401245117\n",
            "loss on batch 109 in epoch 71 for the simple nn is: 0.5172801613807678\n",
            "loss on batch 110 in epoch 71 for the simple nn is: 0.36182379722595215\n",
            "loss on batch 111 in epoch 71 for the simple nn is: 0.45505404472351074\n",
            "loss on batch 112 in epoch 71 for the simple nn is: 0.5047104358673096\n",
            "loss on batch 113 in epoch 71 for the simple nn is: 0.5201532244682312\n",
            "loss on batch 114 in epoch 71 for the simple nn is: 0.511866569519043\n",
            "loss on batch 115 in epoch 71 for the simple nn is: 0.47402632236480713\n",
            "loss on batch 116 in epoch 71 for the simple nn is: 0.5640495419502258\n",
            "loss on batch 117 in epoch 71 for the simple nn is: 0.54649418592453\n",
            "loss on batch 118 in epoch 71 for the simple nn is: 0.5760565996170044\n",
            "loss on batch 119 in epoch 71 for the simple nn is: 0.524728536605835\n",
            "loss on batch 120 in epoch 71 for the simple nn is: 0.45131656527519226\n",
            "loss on batch 0 in epoch 72 for the simple nn is: 0.4832576811313629\n",
            "loss on batch 1 in epoch 72 for the simple nn is: 0.5927605628967285\n",
            "loss on batch 2 in epoch 72 for the simple nn is: 0.5645138621330261\n",
            "loss on batch 3 in epoch 72 for the simple nn is: 0.552534818649292\n",
            "loss on batch 4 in epoch 72 for the simple nn is: 0.5422163009643555\n",
            "loss on batch 5 in epoch 72 for the simple nn is: 0.5486578941345215\n",
            "loss on batch 6 in epoch 72 for the simple nn is: 0.5203098058700562\n",
            "loss on batch 7 in epoch 72 for the simple nn is: 0.5385692715644836\n",
            "loss on batch 8 in epoch 72 for the simple nn is: 0.47827547788619995\n",
            "loss on batch 9 in epoch 72 for the simple nn is: 0.4730997681617737\n",
            "loss on batch 10 in epoch 72 for the simple nn is: 0.4323658347129822\n",
            "loss on batch 11 in epoch 72 for the simple nn is: 0.5109614133834839\n",
            "loss on batch 12 in epoch 72 for the simple nn is: 0.5136149525642395\n",
            "loss on batch 13 in epoch 72 for the simple nn is: 0.5027924180030823\n",
            "loss on batch 14 in epoch 72 for the simple nn is: 0.4915004074573517\n",
            "loss on batch 15 in epoch 72 for the simple nn is: 0.45364460349082947\n",
            "loss on batch 16 in epoch 72 for the simple nn is: 0.4882407486438751\n",
            "loss on batch 17 in epoch 72 for the simple nn is: 0.46857455372810364\n",
            "loss on batch 18 in epoch 72 for the simple nn is: 0.45624634623527527\n",
            "loss on batch 19 in epoch 72 for the simple nn is: 0.4658876955509186\n",
            "loss on batch 20 in epoch 72 for the simple nn is: 0.4596892297267914\n",
            "loss on batch 21 in epoch 72 for the simple nn is: 0.5144694447517395\n",
            "loss on batch 22 in epoch 72 for the simple nn is: 0.4444853365421295\n",
            "loss on batch 23 in epoch 72 for the simple nn is: 0.4452960193157196\n",
            "loss on batch 24 in epoch 72 for the simple nn is: 0.3912794291973114\n",
            "loss on batch 25 in epoch 72 for the simple nn is: 0.4356580674648285\n",
            "loss on batch 26 in epoch 72 for the simple nn is: 0.5345617532730103\n",
            "loss on batch 27 in epoch 72 for the simple nn is: 0.4677261412143707\n",
            "loss on batch 28 in epoch 72 for the simple nn is: 0.45012587308883667\n",
            "loss on batch 29 in epoch 72 for the simple nn is: 0.5312255024909973\n",
            "loss on batch 30 in epoch 72 for the simple nn is: 0.4894854426383972\n",
            "loss on batch 31 in epoch 72 for the simple nn is: 0.5234517455101013\n",
            "loss on batch 32 in epoch 72 for the simple nn is: 0.439286470413208\n",
            "loss on batch 33 in epoch 72 for the simple nn is: 0.468823105096817\n",
            "loss on batch 34 in epoch 72 for the simple nn is: 0.4371495842933655\n",
            "loss on batch 35 in epoch 72 for the simple nn is: 0.509931743144989\n",
            "loss on batch 36 in epoch 72 for the simple nn is: 0.5433696508407593\n",
            "loss on batch 37 in epoch 72 for the simple nn is: 0.3995402753353119\n",
            "loss on batch 38 in epoch 72 for the simple nn is: 0.5113861560821533\n",
            "loss on batch 39 in epoch 72 for the simple nn is: 0.41441643238067627\n",
            "loss on batch 40 in epoch 72 for the simple nn is: 0.5416585803031921\n",
            "loss on batch 41 in epoch 72 for the simple nn is: 0.40599027276039124\n",
            "loss on batch 42 in epoch 72 for the simple nn is: 0.47579339146614075\n",
            "loss on batch 43 in epoch 72 for the simple nn is: 0.49034106731414795\n",
            "loss on batch 44 in epoch 72 for the simple nn is: 0.48631101846694946\n",
            "loss on batch 45 in epoch 72 for the simple nn is: 0.49105653166770935\n",
            "loss on batch 46 in epoch 72 for the simple nn is: 0.4281391203403473\n",
            "loss on batch 47 in epoch 72 for the simple nn is: 0.44535109400749207\n",
            "loss on batch 48 in epoch 72 for the simple nn is: 0.5146370530128479\n",
            "loss on batch 49 in epoch 72 for the simple nn is: 0.5282444953918457\n",
            "loss on batch 50 in epoch 72 for the simple nn is: 0.4500328302383423\n",
            "loss on batch 51 in epoch 72 for the simple nn is: 0.49116596579551697\n",
            "loss on batch 52 in epoch 72 for the simple nn is: 0.3775955140590668\n",
            "loss on batch 53 in epoch 72 for the simple nn is: 0.3385239243507385\n",
            "loss on batch 54 in epoch 72 for the simple nn is: 0.5434306859970093\n",
            "loss on batch 55 in epoch 72 for the simple nn is: 0.42882755398750305\n",
            "loss on batch 56 in epoch 72 for the simple nn is: 0.48841285705566406\n",
            "loss on batch 57 in epoch 72 for the simple nn is: 0.5001026391983032\n",
            "loss on batch 58 in epoch 72 for the simple nn is: 0.4658590257167816\n",
            "loss on batch 59 in epoch 72 for the simple nn is: 0.47536322474479675\n",
            "loss on batch 60 in epoch 72 for the simple nn is: 0.46482282876968384\n",
            "loss on batch 61 in epoch 72 for the simple nn is: 0.5050511956214905\n",
            "loss on batch 62 in epoch 72 for the simple nn is: 0.4620889723300934\n",
            "loss on batch 63 in epoch 72 for the simple nn is: 0.4653106927871704\n",
            "loss on batch 64 in epoch 72 for the simple nn is: 0.4234665632247925\n",
            "loss on batch 65 in epoch 72 for the simple nn is: 0.40249401330947876\n",
            "loss on batch 66 in epoch 72 for the simple nn is: 0.47089457511901855\n",
            "loss on batch 67 in epoch 72 for the simple nn is: 0.39173367619514465\n",
            "loss on batch 68 in epoch 72 for the simple nn is: 0.4098939895629883\n",
            "loss on batch 69 in epoch 72 for the simple nn is: 0.46592792868614197\n",
            "loss on batch 70 in epoch 72 for the simple nn is: 0.4717767834663391\n",
            "loss on batch 71 in epoch 72 for the simple nn is: 0.7403271794319153\n",
            "loss on batch 72 in epoch 72 for the simple nn is: 0.46536555886268616\n",
            "loss on batch 73 in epoch 72 for the simple nn is: 0.5342731475830078\n",
            "loss on batch 74 in epoch 72 for the simple nn is: 0.48179352283477783\n",
            "loss on batch 75 in epoch 72 for the simple nn is: 0.4684129059314728\n",
            "loss on batch 76 in epoch 72 for the simple nn is: 0.48479679226875305\n",
            "loss on batch 77 in epoch 72 for the simple nn is: 0.4262627065181732\n",
            "loss on batch 78 in epoch 72 for the simple nn is: 0.6080125570297241\n",
            "loss on batch 79 in epoch 72 for the simple nn is: 0.4239734411239624\n",
            "loss on batch 80 in epoch 72 for the simple nn is: 0.4545460343360901\n",
            "loss on batch 81 in epoch 72 for the simple nn is: 0.40136033296585083\n",
            "loss on batch 82 in epoch 72 for the simple nn is: 0.4742359220981598\n",
            "loss on batch 83 in epoch 72 for the simple nn is: 0.48714786767959595\n",
            "loss on batch 84 in epoch 72 for the simple nn is: 0.491592139005661\n",
            "loss on batch 85 in epoch 72 for the simple nn is: 0.5131105184555054\n",
            "loss on batch 86 in epoch 72 for the simple nn is: 0.41120317578315735\n",
            "loss on batch 87 in epoch 72 for the simple nn is: 1.191544532775879\n",
            "loss on batch 88 in epoch 72 for the simple nn is: 0.3730858862400055\n",
            "loss on batch 89 in epoch 72 for the simple nn is: 0.48202046751976013\n",
            "loss on batch 90 in epoch 72 for the simple nn is: 0.598857045173645\n",
            "loss on batch 91 in epoch 72 for the simple nn is: 0.4737752676010132\n",
            "loss on batch 92 in epoch 72 for the simple nn is: 0.5377758741378784\n",
            "loss on batch 93 in epoch 72 for the simple nn is: 0.5226683616638184\n",
            "loss on batch 94 in epoch 72 for the simple nn is: 0.41354742646217346\n",
            "loss on batch 95 in epoch 72 for the simple nn is: 0.4886358380317688\n",
            "loss on batch 96 in epoch 72 for the simple nn is: 0.42820146679878235\n",
            "loss on batch 97 in epoch 72 for the simple nn is: 0.48490676283836365\n",
            "loss on batch 98 in epoch 72 for the simple nn is: 0.4328741431236267\n",
            "loss on batch 99 in epoch 72 for the simple nn is: 0.4309040904045105\n",
            "loss on batch 100 in epoch 72 for the simple nn is: 0.4772132635116577\n",
            "loss on batch 101 in epoch 72 for the simple nn is: 0.4068963825702667\n",
            "loss on batch 102 in epoch 72 for the simple nn is: 0.3683409094810486\n",
            "loss on batch 103 in epoch 72 for the simple nn is: 0.4824857711791992\n",
            "loss on batch 104 in epoch 72 for the simple nn is: 0.4449307322502136\n",
            "loss on batch 105 in epoch 72 for the simple nn is: 0.42854273319244385\n",
            "loss on batch 106 in epoch 72 for the simple nn is: 0.4684995710849762\n",
            "loss on batch 107 in epoch 72 for the simple nn is: 0.42391490936279297\n",
            "loss on batch 108 in epoch 72 for the simple nn is: 0.4899253249168396\n",
            "loss on batch 109 in epoch 72 for the simple nn is: 0.5136232972145081\n",
            "loss on batch 110 in epoch 72 for the simple nn is: 0.3444415330886841\n",
            "loss on batch 111 in epoch 72 for the simple nn is: 0.44670623540878296\n",
            "loss on batch 112 in epoch 72 for the simple nn is: 0.49497726559638977\n",
            "loss on batch 113 in epoch 72 for the simple nn is: 0.49374350905418396\n",
            "loss on batch 114 in epoch 72 for the simple nn is: 0.4971808195114136\n",
            "loss on batch 115 in epoch 72 for the simple nn is: 0.48062196373939514\n",
            "loss on batch 116 in epoch 72 for the simple nn is: 0.565153956413269\n",
            "loss on batch 117 in epoch 72 for the simple nn is: 0.5370203852653503\n",
            "loss on batch 118 in epoch 72 for the simple nn is: 0.5879555940628052\n",
            "loss on batch 119 in epoch 72 for the simple nn is: 0.5743263363838196\n",
            "loss on batch 120 in epoch 72 for the simple nn is: 0.4719785749912262\n",
            "loss on batch 0 in epoch 73 for the simple nn is: 0.4672064781188965\n",
            "loss on batch 1 in epoch 73 for the simple nn is: 0.6003830432891846\n",
            "loss on batch 2 in epoch 73 for the simple nn is: 0.5708113312721252\n",
            "loss on batch 3 in epoch 73 for the simple nn is: 0.5464635491371155\n",
            "loss on batch 4 in epoch 73 for the simple nn is: 0.5657750368118286\n",
            "loss on batch 5 in epoch 73 for the simple nn is: 0.5750654339790344\n",
            "loss on batch 6 in epoch 73 for the simple nn is: 0.5177150368690491\n",
            "loss on batch 7 in epoch 73 for the simple nn is: 0.5264326930046082\n",
            "loss on batch 8 in epoch 73 for the simple nn is: 0.47736433148384094\n",
            "loss on batch 9 in epoch 73 for the simple nn is: 0.5006241202354431\n",
            "loss on batch 10 in epoch 73 for the simple nn is: 0.4191938042640686\n",
            "loss on batch 11 in epoch 73 for the simple nn is: 0.5031989812850952\n",
            "loss on batch 12 in epoch 73 for the simple nn is: 0.5281307697296143\n",
            "loss on batch 13 in epoch 73 for the simple nn is: 0.5214638113975525\n",
            "loss on batch 14 in epoch 73 for the simple nn is: 0.47823411226272583\n",
            "loss on batch 15 in epoch 73 for the simple nn is: 0.45950791239738464\n",
            "loss on batch 16 in epoch 73 for the simple nn is: 0.5207732319831848\n",
            "loss on batch 17 in epoch 73 for the simple nn is: 0.6185864806175232\n",
            "loss on batch 18 in epoch 73 for the simple nn is: 0.5068817138671875\n",
            "loss on batch 19 in epoch 73 for the simple nn is: 0.46858805418014526\n",
            "loss on batch 20 in epoch 73 for the simple nn is: 0.4580594599246979\n",
            "loss on batch 21 in epoch 73 for the simple nn is: 0.6427000164985657\n",
            "loss on batch 22 in epoch 73 for the simple nn is: 0.4596576988697052\n",
            "loss on batch 23 in epoch 73 for the simple nn is: 0.43348318338394165\n",
            "loss on batch 24 in epoch 73 for the simple nn is: 0.39111316204071045\n",
            "loss on batch 25 in epoch 73 for the simple nn is: 0.4381689727306366\n",
            "loss on batch 26 in epoch 73 for the simple nn is: 0.510413408279419\n",
            "loss on batch 27 in epoch 73 for the simple nn is: 0.4815358519554138\n",
            "loss on batch 28 in epoch 73 for the simple nn is: 0.4789329469203949\n",
            "loss on batch 29 in epoch 73 for the simple nn is: 0.5918874740600586\n",
            "loss on batch 30 in epoch 73 for the simple nn is: 0.5020155906677246\n",
            "loss on batch 31 in epoch 73 for the simple nn is: 0.6083253026008606\n",
            "loss on batch 32 in epoch 73 for the simple nn is: 0.49310606718063354\n",
            "loss on batch 33 in epoch 73 for the simple nn is: 0.4785930812358856\n",
            "loss on batch 34 in epoch 73 for the simple nn is: 0.47136905789375305\n",
            "loss on batch 35 in epoch 73 for the simple nn is: 0.47515052556991577\n",
            "loss on batch 36 in epoch 73 for the simple nn is: 0.5168052911758423\n",
            "loss on batch 37 in epoch 73 for the simple nn is: 0.4416953921318054\n",
            "loss on batch 38 in epoch 73 for the simple nn is: 0.5166327357292175\n",
            "loss on batch 39 in epoch 73 for the simple nn is: 0.42309248447418213\n",
            "loss on batch 40 in epoch 73 for the simple nn is: 0.5036478638648987\n",
            "loss on batch 41 in epoch 73 for the simple nn is: 0.43464571237564087\n",
            "loss on batch 42 in epoch 73 for the simple nn is: 0.4724279046058655\n",
            "loss on batch 43 in epoch 73 for the simple nn is: 0.5028879046440125\n",
            "loss on batch 44 in epoch 73 for the simple nn is: 0.5113112926483154\n",
            "loss on batch 45 in epoch 73 for the simple nn is: 0.48451370000839233\n",
            "loss on batch 46 in epoch 73 for the simple nn is: 0.44506314396858215\n",
            "loss on batch 47 in epoch 73 for the simple nn is: 0.46133363246917725\n",
            "loss on batch 48 in epoch 73 for the simple nn is: 0.4760599136352539\n",
            "loss on batch 49 in epoch 73 for the simple nn is: 0.5434547662734985\n",
            "loss on batch 50 in epoch 73 for the simple nn is: 0.44062551856040955\n",
            "loss on batch 51 in epoch 73 for the simple nn is: 0.65220046043396\n",
            "loss on batch 52 in epoch 73 for the simple nn is: 0.383029580116272\n",
            "loss on batch 53 in epoch 73 for the simple nn is: 0.36955729126930237\n",
            "loss on batch 54 in epoch 73 for the simple nn is: 0.5474845767021179\n",
            "loss on batch 55 in epoch 73 for the simple nn is: 0.4617800712585449\n",
            "loss on batch 56 in epoch 73 for the simple nn is: 0.47162503004074097\n",
            "loss on batch 57 in epoch 73 for the simple nn is: 0.5380369424819946\n",
            "loss on batch 58 in epoch 73 for the simple nn is: 0.464003324508667\n",
            "loss on batch 59 in epoch 73 for the simple nn is: 0.47117879986763\n",
            "loss on batch 60 in epoch 73 for the simple nn is: 0.4412202537059784\n",
            "loss on batch 61 in epoch 73 for the simple nn is: 0.43973636627197266\n",
            "loss on batch 62 in epoch 73 for the simple nn is: 0.4534050524234772\n",
            "loss on batch 63 in epoch 73 for the simple nn is: 0.47270122170448303\n",
            "loss on batch 64 in epoch 73 for the simple nn is: 0.40662479400634766\n",
            "loss on batch 65 in epoch 73 for the simple nn is: 0.48180967569351196\n",
            "loss on batch 66 in epoch 73 for the simple nn is: 0.46775832772254944\n",
            "loss on batch 67 in epoch 73 for the simple nn is: 0.416652113199234\n",
            "loss on batch 68 in epoch 73 for the simple nn is: 0.4499862790107727\n",
            "loss on batch 69 in epoch 73 for the simple nn is: 0.4483740031719208\n",
            "loss on batch 70 in epoch 73 for the simple nn is: 0.5455488562583923\n",
            "loss on batch 71 in epoch 73 for the simple nn is: 0.47057828307151794\n",
            "loss on batch 72 in epoch 73 for the simple nn is: 0.48213082551956177\n",
            "loss on batch 73 in epoch 73 for the simple nn is: 0.5337206721305847\n",
            "loss on batch 74 in epoch 73 for the simple nn is: 0.5365003347396851\n",
            "loss on batch 75 in epoch 73 for the simple nn is: 0.5536951422691345\n",
            "loss on batch 76 in epoch 73 for the simple nn is: 0.4772130846977234\n",
            "loss on batch 77 in epoch 73 for the simple nn is: 0.7095592021942139\n",
            "loss on batch 78 in epoch 73 for the simple nn is: 0.47585999965667725\n",
            "loss on batch 79 in epoch 73 for the simple nn is: 0.417917937040329\n",
            "loss on batch 80 in epoch 73 for the simple nn is: 0.4829499125480652\n",
            "loss on batch 81 in epoch 73 for the simple nn is: 0.41611891984939575\n",
            "loss on batch 82 in epoch 73 for the simple nn is: 0.4901331067085266\n",
            "loss on batch 83 in epoch 73 for the simple nn is: 0.4959542155265808\n",
            "loss on batch 84 in epoch 73 for the simple nn is: 0.4704591631889343\n",
            "loss on batch 85 in epoch 73 for the simple nn is: 0.4912255108356476\n",
            "loss on batch 86 in epoch 73 for the simple nn is: 0.4115649461746216\n",
            "loss on batch 87 in epoch 73 for the simple nn is: 0.6432318687438965\n",
            "loss on batch 88 in epoch 73 for the simple nn is: 0.4831926226615906\n",
            "loss on batch 89 in epoch 73 for the simple nn is: 0.5191409587860107\n",
            "loss on batch 90 in epoch 73 for the simple nn is: 0.47708332538604736\n",
            "loss on batch 91 in epoch 73 for the simple nn is: 0.46827757358551025\n",
            "loss on batch 92 in epoch 73 for the simple nn is: 0.5573866963386536\n",
            "loss on batch 93 in epoch 73 for the simple nn is: 0.46944907307624817\n",
            "loss on batch 94 in epoch 73 for the simple nn is: 0.4585011601448059\n",
            "loss on batch 95 in epoch 73 for the simple nn is: 0.4960794746875763\n",
            "loss on batch 96 in epoch 73 for the simple nn is: 0.44685137271881104\n",
            "loss on batch 97 in epoch 73 for the simple nn is: 0.48403480648994446\n",
            "loss on batch 98 in epoch 73 for the simple nn is: 0.43669524788856506\n",
            "loss on batch 99 in epoch 73 for the simple nn is: 0.4623560309410095\n",
            "loss on batch 100 in epoch 73 for the simple nn is: 0.47616416215896606\n",
            "loss on batch 101 in epoch 73 for the simple nn is: 0.4414427578449249\n",
            "loss on batch 102 in epoch 73 for the simple nn is: 0.40970152616500854\n",
            "loss on batch 103 in epoch 73 for the simple nn is: 0.6301507353782654\n",
            "loss on batch 104 in epoch 73 for the simple nn is: 0.5648322701454163\n",
            "loss on batch 105 in epoch 73 for the simple nn is: 0.4581432640552521\n",
            "loss on batch 106 in epoch 73 for the simple nn is: 0.4950721561908722\n",
            "loss on batch 107 in epoch 73 for the simple nn is: 0.47116801142692566\n",
            "loss on batch 108 in epoch 73 for the simple nn is: 0.4739139974117279\n",
            "loss on batch 109 in epoch 73 for the simple nn is: 0.5429587960243225\n",
            "loss on batch 110 in epoch 73 for the simple nn is: 0.3824516832828522\n",
            "loss on batch 111 in epoch 73 for the simple nn is: 0.4714788496494293\n",
            "loss on batch 112 in epoch 73 for the simple nn is: 0.539042592048645\n",
            "loss on batch 113 in epoch 73 for the simple nn is: 0.6218802332878113\n",
            "loss on batch 114 in epoch 73 for the simple nn is: 0.5486883521080017\n",
            "loss on batch 115 in epoch 73 for the simple nn is: 0.46734556555747986\n",
            "loss on batch 116 in epoch 73 for the simple nn is: 0.5625916719436646\n",
            "loss on batch 117 in epoch 73 for the simple nn is: 0.554122269153595\n",
            "loss on batch 118 in epoch 73 for the simple nn is: 0.6101368069648743\n",
            "loss on batch 119 in epoch 73 for the simple nn is: 0.5617847442626953\n",
            "loss on batch 120 in epoch 73 for the simple nn is: 0.5736324787139893\n",
            "loss on batch 0 in epoch 74 for the simple nn is: 0.5060492157936096\n",
            "loss on batch 1 in epoch 74 for the simple nn is: 0.6184139847755432\n",
            "loss on batch 2 in epoch 74 for the simple nn is: 0.5829516649246216\n",
            "loss on batch 3 in epoch 74 for the simple nn is: 0.5563493967056274\n",
            "loss on batch 4 in epoch 74 for the simple nn is: 0.5495373606681824\n",
            "loss on batch 5 in epoch 74 for the simple nn is: 0.5775095820426941\n",
            "loss on batch 6 in epoch 74 for the simple nn is: 0.5406304597854614\n",
            "loss on batch 7 in epoch 74 for the simple nn is: 0.6912786364555359\n",
            "loss on batch 8 in epoch 74 for the simple nn is: 0.5479974746704102\n",
            "loss on batch 9 in epoch 74 for the simple nn is: 0.4791920781135559\n",
            "loss on batch 10 in epoch 74 for the simple nn is: 0.44241470098495483\n",
            "loss on batch 11 in epoch 74 for the simple nn is: 0.5494762063026428\n",
            "loss on batch 12 in epoch 74 for the simple nn is: 0.5814121961593628\n",
            "loss on batch 13 in epoch 74 for the simple nn is: 0.5053467750549316\n",
            "loss on batch 14 in epoch 74 for the simple nn is: 0.5136552453041077\n",
            "loss on batch 15 in epoch 74 for the simple nn is: 0.5011373162269592\n",
            "loss on batch 16 in epoch 74 for the simple nn is: 0.5409387350082397\n",
            "loss on batch 17 in epoch 74 for the simple nn is: 0.46804139018058777\n",
            "loss on batch 18 in epoch 74 for the simple nn is: 0.5564835071563721\n",
            "loss on batch 19 in epoch 74 for the simple nn is: 0.47451990842819214\n",
            "loss on batch 20 in epoch 74 for the simple nn is: 0.46096327900886536\n",
            "loss on batch 21 in epoch 74 for the simple nn is: 0.528456449508667\n",
            "loss on batch 22 in epoch 74 for the simple nn is: 0.4677903652191162\n",
            "loss on batch 23 in epoch 74 for the simple nn is: 0.46952009201049805\n",
            "loss on batch 24 in epoch 74 for the simple nn is: 0.4203540086746216\n",
            "loss on batch 25 in epoch 74 for the simple nn is: 0.4626373052597046\n",
            "loss on batch 26 in epoch 74 for the simple nn is: 0.5108269453048706\n",
            "loss on batch 27 in epoch 74 for the simple nn is: 0.8582689166069031\n",
            "loss on batch 28 in epoch 74 for the simple nn is: 0.4598758816719055\n",
            "loss on batch 29 in epoch 74 for the simple nn is: 0.5798509120941162\n",
            "loss on batch 30 in epoch 74 for the simple nn is: 0.4927675426006317\n",
            "loss on batch 31 in epoch 74 for the simple nn is: 0.55495285987854\n",
            "loss on batch 32 in epoch 74 for the simple nn is: 0.46229809522628784\n",
            "loss on batch 33 in epoch 74 for the simple nn is: 0.5807592868804932\n",
            "loss on batch 34 in epoch 74 for the simple nn is: 0.46420392394065857\n",
            "loss on batch 35 in epoch 74 for the simple nn is: 0.5455281138420105\n",
            "loss on batch 36 in epoch 74 for the simple nn is: 0.48751407861709595\n",
            "loss on batch 37 in epoch 74 for the simple nn is: 0.4523146152496338\n",
            "loss on batch 38 in epoch 74 for the simple nn is: 0.5216659307479858\n",
            "loss on batch 39 in epoch 74 for the simple nn is: 0.4243875741958618\n",
            "loss on batch 40 in epoch 74 for the simple nn is: 0.5032458901405334\n",
            "loss on batch 41 in epoch 74 for the simple nn is: 0.5610217452049255\n",
            "loss on batch 42 in epoch 74 for the simple nn is: 0.479556143283844\n",
            "loss on batch 43 in epoch 74 for the simple nn is: 0.5669968724250793\n",
            "loss on batch 44 in epoch 74 for the simple nn is: 0.5189491510391235\n",
            "loss on batch 45 in epoch 74 for the simple nn is: 0.47930121421813965\n",
            "loss on batch 46 in epoch 74 for the simple nn is: 0.45550796389579773\n",
            "loss on batch 47 in epoch 74 for the simple nn is: 0.4729531705379486\n",
            "loss on batch 48 in epoch 74 for the simple nn is: 0.4826011061668396\n",
            "loss on batch 49 in epoch 74 for the simple nn is: 0.6315225958824158\n",
            "loss on batch 50 in epoch 74 for the simple nn is: 0.4622008800506592\n",
            "loss on batch 51 in epoch 74 for the simple nn is: 0.541407585144043\n",
            "loss on batch 52 in epoch 74 for the simple nn is: 0.4847089648246765\n",
            "loss on batch 53 in epoch 74 for the simple nn is: 0.4088011085987091\n",
            "loss on batch 54 in epoch 74 for the simple nn is: 0.5401824116706848\n",
            "loss on batch 55 in epoch 74 for the simple nn is: 0.4690001606941223\n",
            "loss on batch 56 in epoch 74 for the simple nn is: 0.5952215194702148\n",
            "loss on batch 57 in epoch 74 for the simple nn is: 0.5769755244255066\n",
            "loss on batch 58 in epoch 74 for the simple nn is: 0.45485711097717285\n",
            "loss on batch 59 in epoch 74 for the simple nn is: 0.48651713132858276\n",
            "loss on batch 60 in epoch 74 for the simple nn is: 0.47445571422576904\n",
            "loss on batch 61 in epoch 74 for the simple nn is: 0.4312385320663452\n",
            "loss on batch 62 in epoch 74 for the simple nn is: 0.4593588709831238\n",
            "loss on batch 63 in epoch 74 for the simple nn is: 0.5541688203811646\n",
            "loss on batch 64 in epoch 74 for the simple nn is: 0.4315555989742279\n",
            "loss on batch 65 in epoch 74 for the simple nn is: 0.42772436141967773\n",
            "loss on batch 66 in epoch 74 for the simple nn is: 0.4970875680446625\n",
            "loss on batch 67 in epoch 74 for the simple nn is: 0.44207727909088135\n",
            "loss on batch 68 in epoch 74 for the simple nn is: 0.4557435214519501\n",
            "loss on batch 69 in epoch 74 for the simple nn is: 0.45070981979370117\n",
            "loss on batch 70 in epoch 74 for the simple nn is: 0.6342952847480774\n",
            "loss on batch 71 in epoch 74 for the simple nn is: 0.5308758616447449\n",
            "loss on batch 72 in epoch 74 for the simple nn is: 0.5065577626228333\n",
            "loss on batch 73 in epoch 74 for the simple nn is: 0.5384823083877563\n",
            "loss on batch 74 in epoch 74 for the simple nn is: 0.5348623991012573\n",
            "loss on batch 75 in epoch 74 for the simple nn is: 0.47474274039268494\n",
            "loss on batch 76 in epoch 74 for the simple nn is: 0.5525771379470825\n",
            "loss on batch 77 in epoch 74 for the simple nn is: 0.44985437393188477\n",
            "loss on batch 78 in epoch 74 for the simple nn is: 0.5317084789276123\n",
            "loss on batch 79 in epoch 74 for the simple nn is: 0.4258578419685364\n",
            "loss on batch 80 in epoch 74 for the simple nn is: 0.47251421213150024\n",
            "loss on batch 81 in epoch 74 for the simple nn is: 0.4713502526283264\n",
            "loss on batch 82 in epoch 74 for the simple nn is: 0.4878159761428833\n",
            "loss on batch 83 in epoch 74 for the simple nn is: 0.5036055445671082\n",
            "loss on batch 84 in epoch 74 for the simple nn is: 0.5578513145446777\n",
            "loss on batch 85 in epoch 74 for the simple nn is: 0.4987426996231079\n",
            "loss on batch 86 in epoch 74 for the simple nn is: 0.4045570194721222\n",
            "loss on batch 87 in epoch 74 for the simple nn is: 0.5261208415031433\n",
            "loss on batch 88 in epoch 74 for the simple nn is: 0.3745115101337433\n",
            "loss on batch 89 in epoch 74 for the simple nn is: 0.4949728548526764\n",
            "loss on batch 90 in epoch 74 for the simple nn is: 0.4581199586391449\n",
            "loss on batch 91 in epoch 74 for the simple nn is: 0.45431891083717346\n",
            "loss on batch 92 in epoch 74 for the simple nn is: 0.5292184352874756\n",
            "loss on batch 93 in epoch 74 for the simple nn is: 0.47224342823028564\n",
            "loss on batch 94 in epoch 74 for the simple nn is: 0.4104869067668915\n",
            "loss on batch 95 in epoch 74 for the simple nn is: 0.48373886942863464\n",
            "loss on batch 96 in epoch 74 for the simple nn is: 0.46524789929389954\n",
            "loss on batch 97 in epoch 74 for the simple nn is: 0.495796799659729\n",
            "loss on batch 98 in epoch 74 for the simple nn is: 0.46246224641799927\n",
            "loss on batch 99 in epoch 74 for the simple nn is: 0.44979092478752136\n",
            "loss on batch 100 in epoch 74 for the simple nn is: 0.5801778435707092\n",
            "loss on batch 101 in epoch 74 for the simple nn is: 0.44551724195480347\n",
            "loss on batch 102 in epoch 74 for the simple nn is: 0.49271708726882935\n",
            "loss on batch 103 in epoch 74 for the simple nn is: 0.5149964690208435\n",
            "loss on batch 104 in epoch 74 for the simple nn is: 0.4302382469177246\n",
            "loss on batch 105 in epoch 74 for the simple nn is: 0.4435145854949951\n",
            "loss on batch 106 in epoch 74 for the simple nn is: 0.49862614274024963\n",
            "loss on batch 107 in epoch 74 for the simple nn is: 0.46894943714141846\n",
            "loss on batch 108 in epoch 74 for the simple nn is: 0.48151880502700806\n",
            "loss on batch 109 in epoch 74 for the simple nn is: 0.5672247409820557\n",
            "loss on batch 110 in epoch 74 for the simple nn is: 0.4207800030708313\n",
            "loss on batch 111 in epoch 74 for the simple nn is: 0.45795938372612\n",
            "loss on batch 112 in epoch 74 for the simple nn is: 0.5197969079017639\n",
            "loss on batch 113 in epoch 74 for the simple nn is: 0.5253892540931702\n",
            "loss on batch 114 in epoch 74 for the simple nn is: 0.5342311263084412\n",
            "loss on batch 115 in epoch 74 for the simple nn is: 0.5189895629882812\n",
            "loss on batch 116 in epoch 74 for the simple nn is: 0.5896293520927429\n",
            "loss on batch 117 in epoch 74 for the simple nn is: 0.5321263074874878\n",
            "loss on batch 118 in epoch 74 for the simple nn is: 0.5802753567695618\n",
            "loss on batch 119 in epoch 74 for the simple nn is: 0.5398443937301636\n",
            "loss on batch 120 in epoch 74 for the simple nn is: 0.7023676633834839\n",
            "loss on batch 0 in epoch 75 for the simple nn is: 0.52293461561203\n",
            "loss on batch 1 in epoch 75 for the simple nn is: 0.6089383363723755\n",
            "loss on batch 2 in epoch 75 for the simple nn is: 0.5732817053794861\n",
            "loss on batch 3 in epoch 75 for the simple nn is: 0.5982139706611633\n",
            "loss on batch 4 in epoch 75 for the simple nn is: 0.5569413304328918\n",
            "loss on batch 5 in epoch 75 for the simple nn is: 0.5910120010375977\n",
            "loss on batch 6 in epoch 75 for the simple nn is: 0.5362523794174194\n",
            "loss on batch 7 in epoch 75 for the simple nn is: 0.5411028265953064\n",
            "loss on batch 8 in epoch 75 for the simple nn is: 0.538952112197876\n",
            "loss on batch 9 in epoch 75 for the simple nn is: 0.4998648464679718\n",
            "loss on batch 10 in epoch 75 for the simple nn is: 0.468248575925827\n",
            "loss on batch 11 in epoch 75 for the simple nn is: 0.5184441804885864\n",
            "loss on batch 12 in epoch 75 for the simple nn is: 0.5303531289100647\n",
            "loss on batch 13 in epoch 75 for the simple nn is: 0.5045480728149414\n",
            "loss on batch 14 in epoch 75 for the simple nn is: 0.5122679471969604\n",
            "loss on batch 15 in epoch 75 for the simple nn is: 0.5316970944404602\n",
            "loss on batch 16 in epoch 75 for the simple nn is: 0.5613744258880615\n",
            "loss on batch 17 in epoch 75 for the simple nn is: 0.485230028629303\n",
            "loss on batch 18 in epoch 75 for the simple nn is: 0.4855240285396576\n",
            "loss on batch 19 in epoch 75 for the simple nn is: 0.4738820493221283\n",
            "loss on batch 20 in epoch 75 for the simple nn is: 0.46897175908088684\n",
            "loss on batch 21 in epoch 75 for the simple nn is: 0.5344173312187195\n",
            "loss on batch 22 in epoch 75 for the simple nn is: 0.48043766617774963\n",
            "loss on batch 23 in epoch 75 for the simple nn is: 0.440713495016098\n",
            "loss on batch 24 in epoch 75 for the simple nn is: 0.4669962525367737\n",
            "loss on batch 25 in epoch 75 for the simple nn is: 0.4346904158592224\n",
            "loss on batch 26 in epoch 75 for the simple nn is: 0.5048760771751404\n",
            "loss on batch 27 in epoch 75 for the simple nn is: 0.4893122911453247\n",
            "loss on batch 28 in epoch 75 for the simple nn is: 0.555654764175415\n",
            "loss on batch 29 in epoch 75 for the simple nn is: 0.5648459196090698\n",
            "loss on batch 30 in epoch 75 for the simple nn is: 0.5173580050468445\n",
            "loss on batch 31 in epoch 75 for the simple nn is: 0.6244032979011536\n",
            "loss on batch 32 in epoch 75 for the simple nn is: 0.47374626994132996\n",
            "loss on batch 33 in epoch 75 for the simple nn is: 0.49020808935165405\n",
            "loss on batch 34 in epoch 75 for the simple nn is: 0.45083263516426086\n",
            "loss on batch 35 in epoch 75 for the simple nn is: 0.4885445833206177\n",
            "loss on batch 36 in epoch 75 for the simple nn is: 0.4857604205608368\n",
            "loss on batch 37 in epoch 75 for the simple nn is: 0.4393194019794464\n",
            "loss on batch 38 in epoch 75 for the simple nn is: 0.5127630829811096\n",
            "loss on batch 39 in epoch 75 for the simple nn is: 0.41973677277565\n",
            "loss on batch 40 in epoch 75 for the simple nn is: 0.5797948837280273\n",
            "loss on batch 41 in epoch 75 for the simple nn is: 0.6297637224197388\n",
            "loss on batch 42 in epoch 75 for the simple nn is: 0.5035545229911804\n",
            "loss on batch 43 in epoch 75 for the simple nn is: 0.5113028883934021\n",
            "loss on batch 44 in epoch 75 for the simple nn is: 0.5142377614974976\n",
            "loss on batch 45 in epoch 75 for the simple nn is: 0.48802289366722107\n",
            "loss on batch 46 in epoch 75 for the simple nn is: 0.4444158673286438\n",
            "loss on batch 47 in epoch 75 for the simple nn is: 0.44404149055480957\n",
            "loss on batch 48 in epoch 75 for the simple nn is: 0.506761908531189\n",
            "loss on batch 49 in epoch 75 for the simple nn is: 0.5534915924072266\n",
            "loss on batch 50 in epoch 75 for the simple nn is: 0.4970395565032959\n",
            "loss on batch 51 in epoch 75 for the simple nn is: 0.5064343810081482\n",
            "loss on batch 52 in epoch 75 for the simple nn is: 0.4113768935203552\n",
            "loss on batch 53 in epoch 75 for the simple nn is: 0.3733835220336914\n",
            "loss on batch 54 in epoch 75 for the simple nn is: 0.5621110796928406\n",
            "loss on batch 55 in epoch 75 for the simple nn is: 0.4625024199485779\n",
            "loss on batch 56 in epoch 75 for the simple nn is: 0.5008462071418762\n",
            "loss on batch 57 in epoch 75 for the simple nn is: 0.5466063618659973\n",
            "loss on batch 58 in epoch 75 for the simple nn is: 0.4677683413028717\n",
            "loss on batch 59 in epoch 75 for the simple nn is: 0.5651602745056152\n",
            "loss on batch 60 in epoch 75 for the simple nn is: 0.4664444327354431\n",
            "loss on batch 61 in epoch 75 for the simple nn is: 0.4339177906513214\n",
            "loss on batch 62 in epoch 75 for the simple nn is: 0.47604507207870483\n",
            "loss on batch 63 in epoch 75 for the simple nn is: 0.4830728769302368\n",
            "loss on batch 64 in epoch 75 for the simple nn is: 0.4430990219116211\n",
            "loss on batch 65 in epoch 75 for the simple nn is: 0.5314597487449646\n",
            "loss on batch 66 in epoch 75 for the simple nn is: 0.47750338912010193\n",
            "loss on batch 67 in epoch 75 for the simple nn is: 0.43970075249671936\n",
            "loss on batch 68 in epoch 75 for the simple nn is: 0.6176797151565552\n",
            "loss on batch 69 in epoch 75 for the simple nn is: 0.47161155939102173\n",
            "loss on batch 70 in epoch 75 for the simple nn is: 0.5049029588699341\n",
            "loss on batch 71 in epoch 75 for the simple nn is: 0.48471763730049133\n",
            "loss on batch 72 in epoch 75 for the simple nn is: 0.5054631233215332\n",
            "loss on batch 73 in epoch 75 for the simple nn is: 0.535417914390564\n",
            "loss on batch 74 in epoch 75 for the simple nn is: 0.5008034706115723\n",
            "loss on batch 75 in epoch 75 for the simple nn is: 0.48791685700416565\n",
            "loss on batch 76 in epoch 75 for the simple nn is: 0.500944972038269\n",
            "loss on batch 77 in epoch 75 for the simple nn is: 0.4433068633079529\n",
            "loss on batch 78 in epoch 75 for the simple nn is: 0.4819663166999817\n",
            "loss on batch 79 in epoch 75 for the simple nn is: 0.4175149202346802\n",
            "loss on batch 80 in epoch 75 for the simple nn is: 0.46885785460472107\n",
            "loss on batch 81 in epoch 75 for the simple nn is: 0.4166257381439209\n",
            "loss on batch 82 in epoch 75 for the simple nn is: 0.4690658450126648\n",
            "loss on batch 83 in epoch 75 for the simple nn is: 0.5000693202018738\n",
            "loss on batch 84 in epoch 75 for the simple nn is: 0.4846312701702118\n",
            "loss on batch 85 in epoch 75 for the simple nn is: 0.4900854527950287\n",
            "loss on batch 86 in epoch 75 for the simple nn is: 0.3913932740688324\n",
            "loss on batch 87 in epoch 75 for the simple nn is: 0.5287879109382629\n",
            "loss on batch 88 in epoch 75 for the simple nn is: 0.42324161529541016\n",
            "loss on batch 89 in epoch 75 for the simple nn is: 0.5005343556404114\n",
            "loss on batch 90 in epoch 75 for the simple nn is: 0.4556952714920044\n",
            "loss on batch 91 in epoch 75 for the simple nn is: 0.4287653863430023\n",
            "loss on batch 92 in epoch 75 for the simple nn is: 0.5504382848739624\n",
            "loss on batch 93 in epoch 75 for the simple nn is: 0.4444989860057831\n",
            "loss on batch 94 in epoch 75 for the simple nn is: 0.371993750333786\n",
            "loss on batch 95 in epoch 75 for the simple nn is: 0.4824652075767517\n",
            "loss on batch 96 in epoch 75 for the simple nn is: 0.42699065804481506\n",
            "loss on batch 97 in epoch 75 for the simple nn is: 0.48253199458122253\n",
            "loss on batch 98 in epoch 75 for the simple nn is: 0.42902520298957825\n",
            "loss on batch 99 in epoch 75 for the simple nn is: 0.42538779973983765\n",
            "loss on batch 100 in epoch 75 for the simple nn is: 0.49189403653144836\n",
            "loss on batch 101 in epoch 75 for the simple nn is: 0.395900696516037\n",
            "loss on batch 102 in epoch 75 for the simple nn is: 0.36261656880378723\n",
            "loss on batch 103 in epoch 75 for the simple nn is: 0.4872323274612427\n",
            "loss on batch 104 in epoch 75 for the simple nn is: 0.6135482788085938\n",
            "loss on batch 105 in epoch 75 for the simple nn is: 0.41414570808410645\n",
            "loss on batch 106 in epoch 75 for the simple nn is: 0.5046125054359436\n",
            "loss on batch 107 in epoch 75 for the simple nn is: 0.4495305120944977\n",
            "loss on batch 108 in epoch 75 for the simple nn is: 0.45706692337989807\n",
            "loss on batch 109 in epoch 75 for the simple nn is: 0.5073035955429077\n",
            "loss on batch 110 in epoch 75 for the simple nn is: 0.3318847119808197\n",
            "loss on batch 111 in epoch 75 for the simple nn is: 0.44384315609931946\n",
            "loss on batch 112 in epoch 75 for the simple nn is: 0.507902204990387\n",
            "loss on batch 113 in epoch 75 for the simple nn is: 0.4933536946773529\n",
            "loss on batch 114 in epoch 75 for the simple nn is: 0.575709342956543\n",
            "loss on batch 115 in epoch 75 for the simple nn is: 0.48001691699028015\n",
            "loss on batch 116 in epoch 75 for the simple nn is: 0.5718888640403748\n",
            "loss on batch 117 in epoch 75 for the simple nn is: 0.5341511368751526\n",
            "loss on batch 118 in epoch 75 for the simple nn is: 0.5553125143051147\n",
            "loss on batch 119 in epoch 75 for the simple nn is: 0.527498722076416\n",
            "loss on batch 120 in epoch 75 for the simple nn is: 0.4560004472732544\n",
            "loss on batch 0 in epoch 76 for the simple nn is: 0.468077152967453\n",
            "loss on batch 1 in epoch 76 for the simple nn is: 0.5933154225349426\n",
            "loss on batch 2 in epoch 76 for the simple nn is: 0.56995689868927\n",
            "loss on batch 3 in epoch 76 for the simple nn is: 0.559636652469635\n",
            "loss on batch 4 in epoch 76 for the simple nn is: 0.5390556454658508\n",
            "loss on batch 5 in epoch 76 for the simple nn is: 0.5511958003044128\n",
            "loss on batch 6 in epoch 76 for the simple nn is: 0.5339745283126831\n",
            "loss on batch 7 in epoch 76 for the simple nn is: 0.5160292983055115\n",
            "loss on batch 8 in epoch 76 for the simple nn is: 0.46714234352111816\n",
            "loss on batch 9 in epoch 76 for the simple nn is: 0.4742511510848999\n",
            "loss on batch 10 in epoch 76 for the simple nn is: 0.4252546429634094\n",
            "loss on batch 11 in epoch 76 for the simple nn is: 0.4919484555721283\n",
            "loss on batch 12 in epoch 76 for the simple nn is: 0.5259672999382019\n",
            "loss on batch 13 in epoch 76 for the simple nn is: 0.4997771978378296\n",
            "loss on batch 14 in epoch 76 for the simple nn is: 0.49156641960144043\n",
            "loss on batch 15 in epoch 76 for the simple nn is: 0.59272301197052\n",
            "loss on batch 16 in epoch 76 for the simple nn is: 0.4904429018497467\n",
            "loss on batch 17 in epoch 76 for the simple nn is: 0.4728359580039978\n",
            "loss on batch 18 in epoch 76 for the simple nn is: 0.47351130843162537\n",
            "loss on batch 19 in epoch 76 for the simple nn is: 0.4578120708465576\n",
            "loss on batch 20 in epoch 76 for the simple nn is: 0.49321600794792175\n",
            "loss on batch 21 in epoch 76 for the simple nn is: 0.5109121799468994\n",
            "loss on batch 22 in epoch 76 for the simple nn is: 0.46346020698547363\n",
            "loss on batch 23 in epoch 76 for the simple nn is: 0.41530561447143555\n",
            "loss on batch 24 in epoch 76 for the simple nn is: 0.40959057211875916\n",
            "loss on batch 25 in epoch 76 for the simple nn is: 0.4346843361854553\n",
            "loss on batch 26 in epoch 76 for the simple nn is: 0.5311492085456848\n",
            "loss on batch 27 in epoch 76 for the simple nn is: 0.4815460741519928\n",
            "loss on batch 28 in epoch 76 for the simple nn is: 0.482365220785141\n",
            "loss on batch 29 in epoch 76 for the simple nn is: 0.5615336894989014\n",
            "loss on batch 30 in epoch 76 for the simple nn is: 0.4839911460876465\n",
            "loss on batch 31 in epoch 76 for the simple nn is: 0.5492514371871948\n",
            "loss on batch 32 in epoch 76 for the simple nn is: 0.4399403929710388\n",
            "loss on batch 33 in epoch 76 for the simple nn is: 0.4626694917678833\n",
            "loss on batch 34 in epoch 76 for the simple nn is: 0.43449556827545166\n",
            "loss on batch 35 in epoch 76 for the simple nn is: 0.5796408653259277\n",
            "loss on batch 36 in epoch 76 for the simple nn is: 0.47091639041900635\n",
            "loss on batch 37 in epoch 76 for the simple nn is: 0.4271927773952484\n",
            "loss on batch 38 in epoch 76 for the simple nn is: 0.524815022945404\n",
            "loss on batch 39 in epoch 76 for the simple nn is: 0.4111080765724182\n",
            "loss on batch 40 in epoch 76 for the simple nn is: 0.5159941911697388\n",
            "loss on batch 41 in epoch 76 for the simple nn is: 0.4072987735271454\n",
            "loss on batch 42 in epoch 76 for the simple nn is: 0.47745704650878906\n",
            "loss on batch 43 in epoch 76 for the simple nn is: 0.4985479712486267\n",
            "loss on batch 44 in epoch 76 for the simple nn is: 0.5469721555709839\n",
            "loss on batch 45 in epoch 76 for the simple nn is: 0.48271825909614563\n",
            "loss on batch 46 in epoch 76 for the simple nn is: 0.42925509810447693\n",
            "loss on batch 47 in epoch 76 for the simple nn is: 0.4439536929130554\n",
            "loss on batch 48 in epoch 76 for the simple nn is: 0.4817770719528198\n",
            "loss on batch 49 in epoch 76 for the simple nn is: 0.5410630106925964\n",
            "loss on batch 50 in epoch 76 for the simple nn is: 0.4363333582878113\n",
            "loss on batch 51 in epoch 76 for the simple nn is: 0.5410822033882141\n",
            "loss on batch 52 in epoch 76 for the simple nn is: 0.376107394695282\n",
            "loss on batch 53 in epoch 76 for the simple nn is: 0.333959698677063\n",
            "loss on batch 54 in epoch 76 for the simple nn is: 0.5417319536209106\n",
            "loss on batch 55 in epoch 76 for the simple nn is: 0.4365657567977905\n",
            "loss on batch 56 in epoch 76 for the simple nn is: 0.47392773628234863\n",
            "loss on batch 57 in epoch 76 for the simple nn is: 0.4954279363155365\n",
            "loss on batch 58 in epoch 76 for the simple nn is: 0.45459723472595215\n",
            "loss on batch 59 in epoch 76 for the simple nn is: 0.47211724519729614\n",
            "loss on batch 60 in epoch 76 for the simple nn is: 0.4487156867980957\n",
            "loss on batch 61 in epoch 76 for the simple nn is: 0.4309007227420807\n",
            "loss on batch 62 in epoch 76 for the simple nn is: 0.48026242852211\n",
            "loss on batch 63 in epoch 76 for the simple nn is: 0.48085740208625793\n",
            "loss on batch 64 in epoch 76 for the simple nn is: 0.4045746326446533\n",
            "loss on batch 65 in epoch 76 for the simple nn is: 0.4077993333339691\n",
            "loss on batch 66 in epoch 76 for the simple nn is: 0.5920270085334778\n",
            "loss on batch 67 in epoch 76 for the simple nn is: 0.42378127574920654\n",
            "loss on batch 68 in epoch 76 for the simple nn is: 0.38688915967941284\n",
            "loss on batch 69 in epoch 76 for the simple nn is: 0.44882193207740784\n",
            "loss on batch 70 in epoch 76 for the simple nn is: 0.47095099091529846\n",
            "loss on batch 71 in epoch 76 for the simple nn is: 0.4784756898880005\n",
            "loss on batch 72 in epoch 76 for the simple nn is: 0.4657703638076782\n",
            "loss on batch 73 in epoch 76 for the simple nn is: 0.5148307681083679\n",
            "loss on batch 74 in epoch 76 for the simple nn is: 0.5621585249900818\n",
            "loss on batch 75 in epoch 76 for the simple nn is: 0.4716890752315521\n",
            "loss on batch 76 in epoch 76 for the simple nn is: 0.4612167477607727\n",
            "loss on batch 77 in epoch 76 for the simple nn is: 0.42734208703041077\n",
            "loss on batch 78 in epoch 76 for the simple nn is: 0.4835869073867798\n",
            "loss on batch 79 in epoch 76 for the simple nn is: 0.40991535782814026\n",
            "loss on batch 80 in epoch 76 for the simple nn is: 0.45557746291160583\n",
            "loss on batch 81 in epoch 76 for the simple nn is: 0.3899891972541809\n",
            "loss on batch 82 in epoch 76 for the simple nn is: 0.4821428656578064\n",
            "loss on batch 83 in epoch 76 for the simple nn is: 0.4690464437007904\n",
            "loss on batch 84 in epoch 76 for the simple nn is: 0.4703255295753479\n",
            "loss on batch 85 in epoch 76 for the simple nn is: 0.4977186918258667\n",
            "loss on batch 86 in epoch 76 for the simple nn is: 0.3923686742782593\n",
            "loss on batch 87 in epoch 76 for the simple nn is: 0.5092757940292358\n",
            "loss on batch 88 in epoch 76 for the simple nn is: 0.426569402217865\n",
            "loss on batch 89 in epoch 76 for the simple nn is: 0.4724191725254059\n",
            "loss on batch 90 in epoch 76 for the simple nn is: 0.4346454441547394\n",
            "loss on batch 91 in epoch 76 for the simple nn is: 0.43113207817077637\n",
            "loss on batch 92 in epoch 76 for the simple nn is: 0.5343115329742432\n",
            "loss on batch 93 in epoch 76 for the simple nn is: 0.438001811504364\n",
            "loss on batch 94 in epoch 76 for the simple nn is: 0.3780140280723572\n",
            "loss on batch 95 in epoch 76 for the simple nn is: 0.47704148292541504\n",
            "loss on batch 96 in epoch 76 for the simple nn is: 0.4299722909927368\n",
            "loss on batch 97 in epoch 76 for the simple nn is: 0.4951077401638031\n",
            "loss on batch 98 in epoch 76 for the simple nn is: 0.436145544052124\n",
            "loss on batch 99 in epoch 76 for the simple nn is: 0.4087129831314087\n",
            "loss on batch 100 in epoch 76 for the simple nn is: 0.5795782208442688\n",
            "loss on batch 101 in epoch 76 for the simple nn is: 0.398292601108551\n",
            "loss on batch 102 in epoch 76 for the simple nn is: 0.37920281291007996\n",
            "loss on batch 103 in epoch 76 for the simple nn is: 0.5025848150253296\n",
            "loss on batch 104 in epoch 76 for the simple nn is: 0.419851154088974\n",
            "loss on batch 105 in epoch 76 for the simple nn is: 0.43705758452415466\n",
            "loss on batch 106 in epoch 76 for the simple nn is: 0.454789936542511\n",
            "loss on batch 107 in epoch 76 for the simple nn is: 0.451465904712677\n",
            "loss on batch 108 in epoch 76 for the simple nn is: 0.45397311449050903\n",
            "loss on batch 109 in epoch 76 for the simple nn is: 0.5073379874229431\n",
            "loss on batch 110 in epoch 76 for the simple nn is: 0.3328372538089752\n",
            "loss on batch 111 in epoch 76 for the simple nn is: 0.443055659532547\n",
            "loss on batch 112 in epoch 76 for the simple nn is: 0.49590909481048584\n",
            "loss on batch 113 in epoch 76 for the simple nn is: 0.4898826777935028\n",
            "loss on batch 114 in epoch 76 for the simple nn is: 0.5097628831863403\n",
            "loss on batch 115 in epoch 76 for the simple nn is: 0.4357229471206665\n",
            "loss on batch 116 in epoch 76 for the simple nn is: 0.5795497894287109\n",
            "loss on batch 117 in epoch 76 for the simple nn is: 0.5227209329605103\n",
            "loss on batch 118 in epoch 76 for the simple nn is: 0.5477144122123718\n",
            "loss on batch 119 in epoch 76 for the simple nn is: 0.5249901413917542\n",
            "loss on batch 120 in epoch 76 for the simple nn is: 0.46489816904067993\n",
            "loss on batch 0 in epoch 77 for the simple nn is: 0.46692976355552673\n",
            "loss on batch 1 in epoch 77 for the simple nn is: 0.5948573350906372\n",
            "loss on batch 2 in epoch 77 for the simple nn is: 0.5576121211051941\n",
            "loss on batch 3 in epoch 77 for the simple nn is: 0.5401501059532166\n",
            "loss on batch 4 in epoch 77 for the simple nn is: 0.541194498538971\n",
            "loss on batch 5 in epoch 77 for the simple nn is: 0.5846418142318726\n",
            "loss on batch 6 in epoch 77 for the simple nn is: 0.5259751081466675\n",
            "loss on batch 7 in epoch 77 for the simple nn is: 0.5026789903640747\n",
            "loss on batch 8 in epoch 77 for the simple nn is: 0.45222964882850647\n",
            "loss on batch 9 in epoch 77 for the simple nn is: 0.47927218675613403\n",
            "loss on batch 10 in epoch 77 for the simple nn is: 0.5313400030136108\n",
            "loss on batch 11 in epoch 77 for the simple nn is: 0.4928358793258667\n",
            "loss on batch 12 in epoch 77 for the simple nn is: 0.5115570425987244\n",
            "loss on batch 13 in epoch 77 for the simple nn is: 0.4854919910430908\n",
            "loss on batch 14 in epoch 77 for the simple nn is: 0.47743409872055054\n",
            "loss on batch 15 in epoch 77 for the simple nn is: 0.4862484037876129\n",
            "loss on batch 16 in epoch 77 for the simple nn is: 0.4993450343608856\n",
            "loss on batch 17 in epoch 77 for the simple nn is: 0.46595999598503113\n",
            "loss on batch 18 in epoch 77 for the simple nn is: 0.46131736040115356\n",
            "loss on batch 19 in epoch 77 for the simple nn is: 0.46380478143692017\n",
            "loss on batch 20 in epoch 77 for the simple nn is: 0.9099625945091248\n",
            "loss on batch 21 in epoch 77 for the simple nn is: 0.5045626163482666\n",
            "loss on batch 22 in epoch 77 for the simple nn is: 0.4592680037021637\n",
            "loss on batch 23 in epoch 77 for the simple nn is: 0.44247010350227356\n",
            "loss on batch 24 in epoch 77 for the simple nn is: 0.3877842128276825\n",
            "loss on batch 25 in epoch 77 for the simple nn is: 0.5660415887832642\n",
            "loss on batch 26 in epoch 77 for the simple nn is: 0.5239659547805786\n",
            "loss on batch 27 in epoch 77 for the simple nn is: 0.4723615348339081\n",
            "loss on batch 28 in epoch 77 for the simple nn is: 0.47510236501693726\n",
            "loss on batch 29 in epoch 77 for the simple nn is: 0.5479723215103149\n",
            "loss on batch 30 in epoch 77 for the simple nn is: 0.4828093349933624\n",
            "loss on batch 31 in epoch 77 for the simple nn is: 0.5254240036010742\n",
            "loss on batch 32 in epoch 77 for the simple nn is: 0.43377920985221863\n",
            "loss on batch 33 in epoch 77 for the simple nn is: 0.5227665305137634\n",
            "loss on batch 34 in epoch 77 for the simple nn is: 0.4464159905910492\n",
            "loss on batch 35 in epoch 77 for the simple nn is: 0.7215359807014465\n",
            "loss on batch 36 in epoch 77 for the simple nn is: 0.46631962060928345\n",
            "loss on batch 37 in epoch 77 for the simple nn is: 0.4280987083911896\n",
            "loss on batch 38 in epoch 77 for the simple nn is: 0.49566566944122314\n",
            "loss on batch 39 in epoch 77 for the simple nn is: 0.4406922161579132\n",
            "loss on batch 40 in epoch 77 for the simple nn is: 0.5193030834197998\n",
            "loss on batch 41 in epoch 77 for the simple nn is: 0.3995029628276825\n",
            "loss on batch 42 in epoch 77 for the simple nn is: 0.46914419531822205\n",
            "loss on batch 43 in epoch 77 for the simple nn is: 0.4905780851840973\n",
            "loss on batch 44 in epoch 77 for the simple nn is: 0.4964800775051117\n",
            "loss on batch 45 in epoch 77 for the simple nn is: 0.4705066680908203\n",
            "loss on batch 46 in epoch 77 for the simple nn is: 0.42915233969688416\n",
            "loss on batch 47 in epoch 77 for the simple nn is: 0.4452996850013733\n",
            "loss on batch 48 in epoch 77 for the simple nn is: 0.47431352734565735\n",
            "loss on batch 49 in epoch 77 for the simple nn is: 0.5232024192810059\n",
            "loss on batch 50 in epoch 77 for the simple nn is: 0.4377022683620453\n",
            "loss on batch 51 in epoch 77 for the simple nn is: 0.4675164520740509\n",
            "loss on batch 52 in epoch 77 for the simple nn is: 0.37968048453330994\n",
            "loss on batch 53 in epoch 77 for the simple nn is: 0.36942020058631897\n",
            "loss on batch 54 in epoch 77 for the simple nn is: 0.5555033087730408\n",
            "loss on batch 55 in epoch 77 for the simple nn is: 0.4372900426387787\n",
            "loss on batch 56 in epoch 77 for the simple nn is: 0.4781399965286255\n",
            "loss on batch 57 in epoch 77 for the simple nn is: 0.4950703978538513\n",
            "loss on batch 58 in epoch 77 for the simple nn is: 0.44987669587135315\n",
            "loss on batch 59 in epoch 77 for the simple nn is: 0.48714423179626465\n",
            "loss on batch 60 in epoch 77 for the simple nn is: 0.44831281900405884\n",
            "loss on batch 61 in epoch 77 for the simple nn is: 0.44516217708587646\n",
            "loss on batch 62 in epoch 77 for the simple nn is: 0.44770365953445435\n",
            "loss on batch 63 in epoch 77 for the simple nn is: 0.45986053347587585\n",
            "loss on batch 64 in epoch 77 for the simple nn is: 0.4029093086719513\n",
            "loss on batch 65 in epoch 77 for the simple nn is: 0.4206404685974121\n",
            "loss on batch 66 in epoch 77 for the simple nn is: 0.4881325960159302\n",
            "loss on batch 67 in epoch 77 for the simple nn is: 0.4106783866882324\n",
            "loss on batch 68 in epoch 77 for the simple nn is: 0.40861213207244873\n",
            "loss on batch 69 in epoch 77 for the simple nn is: 0.44854554533958435\n",
            "loss on batch 70 in epoch 77 for the simple nn is: 0.5287081003189087\n",
            "loss on batch 71 in epoch 77 for the simple nn is: 0.46195971965789795\n",
            "loss on batch 72 in epoch 77 for the simple nn is: 0.4653575122356415\n",
            "loss on batch 73 in epoch 77 for the simple nn is: 0.5031105279922485\n",
            "loss on batch 74 in epoch 77 for the simple nn is: 0.47336867451667786\n",
            "loss on batch 75 in epoch 77 for the simple nn is: 0.47455185651779175\n",
            "loss on batch 76 in epoch 77 for the simple nn is: 0.47146928310394287\n",
            "loss on batch 77 in epoch 77 for the simple nn is: 0.4093223810195923\n",
            "loss on batch 78 in epoch 77 for the simple nn is: 0.47467154264450073\n",
            "loss on batch 79 in epoch 77 for the simple nn is: 0.3960956335067749\n",
            "loss on batch 80 in epoch 77 for the simple nn is: 0.5065388083457947\n",
            "loss on batch 81 in epoch 77 for the simple nn is: 0.3959611654281616\n",
            "loss on batch 82 in epoch 77 for the simple nn is: 0.44619008898735046\n",
            "loss on batch 83 in epoch 77 for the simple nn is: 0.46914929151535034\n",
            "loss on batch 84 in epoch 77 for the simple nn is: 0.49061205983161926\n",
            "loss on batch 85 in epoch 77 for the simple nn is: 0.48438897728919983\n",
            "loss on batch 86 in epoch 77 for the simple nn is: 0.3986313045024872\n",
            "loss on batch 87 in epoch 77 for the simple nn is: 0.46905577182769775\n",
            "loss on batch 88 in epoch 77 for the simple nn is: 0.3512194752693176\n",
            "loss on batch 89 in epoch 77 for the simple nn is: 0.48115572333335876\n",
            "loss on batch 90 in epoch 77 for the simple nn is: 0.46748456358909607\n",
            "loss on batch 91 in epoch 77 for the simple nn is: 0.4431256949901581\n",
            "loss on batch 92 in epoch 77 for the simple nn is: 0.5301700234413147\n",
            "loss on batch 93 in epoch 77 for the simple nn is: 0.44019338488578796\n",
            "loss on batch 94 in epoch 77 for the simple nn is: 0.3808504343032837\n",
            "loss on batch 95 in epoch 77 for the simple nn is: 0.47707197070121765\n",
            "loss on batch 96 in epoch 77 for the simple nn is: 0.402802437543869\n",
            "loss on batch 97 in epoch 77 for the simple nn is: 0.4714990258216858\n",
            "loss on batch 98 in epoch 77 for the simple nn is: 0.43067044019699097\n",
            "loss on batch 99 in epoch 77 for the simple nn is: 0.4015093147754669\n",
            "loss on batch 100 in epoch 77 for the simple nn is: 0.4697061777114868\n",
            "loss on batch 101 in epoch 77 for the simple nn is: 0.4216653108596802\n",
            "loss on batch 102 in epoch 77 for the simple nn is: 0.3811670243740082\n",
            "loss on batch 103 in epoch 77 for the simple nn is: 0.49121609330177307\n",
            "loss on batch 104 in epoch 77 for the simple nn is: 0.40846821665763855\n",
            "loss on batch 105 in epoch 77 for the simple nn is: 0.5581765174865723\n",
            "loss on batch 106 in epoch 77 for the simple nn is: 0.4368152320384979\n",
            "loss on batch 107 in epoch 77 for the simple nn is: 0.4360010623931885\n",
            "loss on batch 108 in epoch 77 for the simple nn is: 0.42975735664367676\n",
            "loss on batch 109 in epoch 77 for the simple nn is: 0.5243048071861267\n",
            "loss on batch 110 in epoch 77 for the simple nn is: 0.330630362033844\n",
            "loss on batch 111 in epoch 77 for the simple nn is: 0.43521466851234436\n",
            "loss on batch 112 in epoch 77 for the simple nn is: 0.5026804208755493\n",
            "loss on batch 113 in epoch 77 for the simple nn is: 0.482267826795578\n",
            "loss on batch 114 in epoch 77 for the simple nn is: 0.5546525716781616\n",
            "loss on batch 115 in epoch 77 for the simple nn is: 0.43225955963134766\n",
            "loss on batch 116 in epoch 77 for the simple nn is: 0.5899524092674255\n",
            "loss on batch 117 in epoch 77 for the simple nn is: 0.5382078886032104\n",
            "loss on batch 118 in epoch 77 for the simple nn is: 0.5462801456451416\n",
            "loss on batch 119 in epoch 77 for the simple nn is: 0.5199137330055237\n",
            "loss on batch 120 in epoch 77 for the simple nn is: 0.46555185317993164\n",
            "loss on batch 0 in epoch 78 for the simple nn is: 0.45891082286834717\n",
            "loss on batch 1 in epoch 78 for the simple nn is: 0.5927425622940063\n",
            "loss on batch 2 in epoch 78 for the simple nn is: 0.5561342239379883\n",
            "loss on batch 3 in epoch 78 for the simple nn is: 0.532739520072937\n",
            "loss on batch 4 in epoch 78 for the simple nn is: 0.5309200286865234\n",
            "loss on batch 5 in epoch 78 for the simple nn is: 0.5578631162643433\n",
            "loss on batch 6 in epoch 78 for the simple nn is: 0.5265030860900879\n",
            "loss on batch 7 in epoch 78 for the simple nn is: 0.4986141324043274\n",
            "loss on batch 8 in epoch 78 for the simple nn is: 0.44883492588996887\n",
            "loss on batch 9 in epoch 78 for the simple nn is: 0.4835606515407562\n",
            "loss on batch 10 in epoch 78 for the simple nn is: 0.41938069462776184\n",
            "loss on batch 11 in epoch 78 for the simple nn is: 0.48843199014663696\n",
            "loss on batch 12 in epoch 78 for the simple nn is: 0.5125134587287903\n",
            "loss on batch 13 in epoch 78 for the simple nn is: 0.5104290246963501\n",
            "loss on batch 14 in epoch 78 for the simple nn is: 0.48115912079811096\n",
            "loss on batch 15 in epoch 78 for the simple nn is: 0.47011855244636536\n",
            "loss on batch 16 in epoch 78 for the simple nn is: 0.48525869846343994\n",
            "loss on batch 17 in epoch 78 for the simple nn is: 0.4667767882347107\n",
            "loss on batch 18 in epoch 78 for the simple nn is: 0.4734523594379425\n",
            "loss on batch 19 in epoch 78 for the simple nn is: 0.47187539935112\n",
            "loss on batch 20 in epoch 78 for the simple nn is: 0.5118180513381958\n",
            "loss on batch 21 in epoch 78 for the simple nn is: 0.4999440312385559\n",
            "loss on batch 22 in epoch 78 for the simple nn is: 0.5488297343254089\n",
            "loss on batch 23 in epoch 78 for the simple nn is: 0.4218769371509552\n",
            "loss on batch 24 in epoch 78 for the simple nn is: 0.3780178725719452\n",
            "loss on batch 25 in epoch 78 for the simple nn is: 0.44329333305358887\n",
            "loss on batch 26 in epoch 78 for the simple nn is: 0.48660367727279663\n",
            "loss on batch 27 in epoch 78 for the simple nn is: 0.4668872058391571\n",
            "loss on batch 28 in epoch 78 for the simple nn is: 0.49841538071632385\n",
            "loss on batch 29 in epoch 78 for the simple nn is: 0.531641960144043\n",
            "loss on batch 30 in epoch 78 for the simple nn is: 0.605690598487854\n",
            "loss on batch 31 in epoch 78 for the simple nn is: 0.522882342338562\n",
            "loss on batch 32 in epoch 78 for the simple nn is: 0.5590717792510986\n",
            "loss on batch 33 in epoch 78 for the simple nn is: 0.46861568093299866\n",
            "loss on batch 34 in epoch 78 for the simple nn is: 0.4317716062068939\n",
            "loss on batch 35 in epoch 78 for the simple nn is: 0.5050471425056458\n",
            "loss on batch 36 in epoch 78 for the simple nn is: 0.4803623855113983\n",
            "loss on batch 37 in epoch 78 for the simple nn is: 0.4306637644767761\n",
            "loss on batch 38 in epoch 78 for the simple nn is: 0.5531144142150879\n",
            "loss on batch 39 in epoch 78 for the simple nn is: 0.4231431782245636\n",
            "loss on batch 40 in epoch 78 for the simple nn is: 0.5110912322998047\n",
            "loss on batch 41 in epoch 78 for the simple nn is: 0.47462740540504456\n",
            "loss on batch 42 in epoch 78 for the simple nn is: 0.48054060339927673\n",
            "loss on batch 43 in epoch 78 for the simple nn is: 0.4839763045310974\n",
            "loss on batch 44 in epoch 78 for the simple nn is: 0.5174803733825684\n",
            "loss on batch 45 in epoch 78 for the simple nn is: 0.46547484397888184\n",
            "loss on batch 46 in epoch 78 for the simple nn is: 0.43821820616722107\n",
            "loss on batch 47 in epoch 78 for the simple nn is: 0.6771666407585144\n",
            "loss on batch 48 in epoch 78 for the simple nn is: 0.47225818037986755\n",
            "loss on batch 49 in epoch 78 for the simple nn is: 0.5201126337051392\n",
            "loss on batch 50 in epoch 78 for the simple nn is: 0.44493380188941956\n",
            "loss on batch 51 in epoch 78 for the simple nn is: 0.5327671766281128\n",
            "loss on batch 52 in epoch 78 for the simple nn is: 0.37032902240753174\n",
            "loss on batch 53 in epoch 78 for the simple nn is: 0.33780437707901\n",
            "loss on batch 54 in epoch 78 for the simple nn is: 0.5602667331695557\n",
            "loss on batch 55 in epoch 78 for the simple nn is: 0.46795788407325745\n",
            "loss on batch 56 in epoch 78 for the simple nn is: 0.4657049775123596\n",
            "loss on batch 57 in epoch 78 for the simple nn is: 0.549411416053772\n",
            "loss on batch 58 in epoch 78 for the simple nn is: 0.448777437210083\n",
            "loss on batch 59 in epoch 78 for the simple nn is: 0.5177763104438782\n",
            "loss on batch 60 in epoch 78 for the simple nn is: 0.45876914262771606\n",
            "loss on batch 61 in epoch 78 for the simple nn is: 0.4466593861579895\n",
            "loss on batch 62 in epoch 78 for the simple nn is: 0.44800904393196106\n",
            "loss on batch 63 in epoch 78 for the simple nn is: 0.44624796509742737\n",
            "loss on batch 64 in epoch 78 for the simple nn is: 0.4165082573890686\n",
            "loss on batch 65 in epoch 78 for the simple nn is: 0.40960103273391724\n",
            "loss on batch 66 in epoch 78 for the simple nn is: 0.521838366985321\n",
            "loss on batch 67 in epoch 78 for the simple nn is: 0.4180513620376587\n",
            "loss on batch 68 in epoch 78 for the simple nn is: 0.460673063993454\n",
            "loss on batch 69 in epoch 78 for the simple nn is: 0.4343557357788086\n",
            "loss on batch 70 in epoch 78 for the simple nn is: 0.45721015334129333\n",
            "loss on batch 71 in epoch 78 for the simple nn is: 0.44872015714645386\n",
            "loss on batch 72 in epoch 78 for the simple nn is: 0.5718407034873962\n",
            "loss on batch 73 in epoch 78 for the simple nn is: 0.51598060131073\n",
            "loss on batch 74 in epoch 78 for the simple nn is: 0.4705640971660614\n",
            "loss on batch 75 in epoch 78 for the simple nn is: 0.4944751560688019\n",
            "loss on batch 76 in epoch 78 for the simple nn is: 0.4694160521030426\n",
            "loss on batch 77 in epoch 78 for the simple nn is: 0.4203881323337555\n",
            "loss on batch 78 in epoch 78 for the simple nn is: 0.47504326701164246\n",
            "loss on batch 79 in epoch 78 for the simple nn is: 0.4218812584877014\n",
            "loss on batch 80 in epoch 78 for the simple nn is: 0.461325466632843\n",
            "loss on batch 81 in epoch 78 for the simple nn is: 0.3991842269897461\n",
            "loss on batch 82 in epoch 78 for the simple nn is: 0.4291609823703766\n",
            "loss on batch 83 in epoch 78 for the simple nn is: 0.4710862636566162\n",
            "loss on batch 84 in epoch 78 for the simple nn is: 0.46162426471710205\n",
            "loss on batch 85 in epoch 78 for the simple nn is: 0.5411252975463867\n",
            "loss on batch 86 in epoch 78 for the simple nn is: 0.3956071734428406\n",
            "loss on batch 87 in epoch 78 for the simple nn is: 0.47046929597854614\n",
            "loss on batch 88 in epoch 78 for the simple nn is: 0.3460327386856079\n",
            "loss on batch 89 in epoch 78 for the simple nn is: 0.47008898854255676\n",
            "loss on batch 90 in epoch 78 for the simple nn is: 0.4394446909427643\n",
            "loss on batch 91 in epoch 78 for the simple nn is: 0.43505820631980896\n",
            "loss on batch 92 in epoch 78 for the simple nn is: 0.5670768618583679\n",
            "loss on batch 93 in epoch 78 for the simple nn is: 0.44478777050971985\n",
            "loss on batch 94 in epoch 78 for the simple nn is: 0.4076135456562042\n",
            "loss on batch 95 in epoch 78 for the simple nn is: 0.48492297530174255\n",
            "loss on batch 96 in epoch 78 for the simple nn is: 0.4141649603843689\n",
            "loss on batch 97 in epoch 78 for the simple nn is: 0.4772339463233948\n",
            "loss on batch 98 in epoch 78 for the simple nn is: 0.48435935378074646\n",
            "loss on batch 99 in epoch 78 for the simple nn is: 0.4011470377445221\n",
            "loss on batch 100 in epoch 78 for the simple nn is: 0.45755383372306824\n",
            "loss on batch 101 in epoch 78 for the simple nn is: 0.41198426485061646\n",
            "loss on batch 102 in epoch 78 for the simple nn is: 0.3524588644504547\n",
            "loss on batch 103 in epoch 78 for the simple nn is: 0.5076125264167786\n",
            "loss on batch 104 in epoch 78 for the simple nn is: 0.6953644752502441\n",
            "loss on batch 105 in epoch 78 for the simple nn is: 0.4310535788536072\n",
            "loss on batch 106 in epoch 78 for the simple nn is: 0.4419673979282379\n",
            "loss on batch 107 in epoch 78 for the simple nn is: 0.41009417176246643\n",
            "loss on batch 108 in epoch 78 for the simple nn is: 0.44067713618278503\n",
            "loss on batch 109 in epoch 78 for the simple nn is: 0.4982961416244507\n",
            "loss on batch 110 in epoch 78 for the simple nn is: 0.3515484631061554\n",
            "loss on batch 111 in epoch 78 for the simple nn is: 0.4348326027393341\n",
            "loss on batch 112 in epoch 78 for the simple nn is: 0.5238437056541443\n",
            "loss on batch 113 in epoch 78 for the simple nn is: 0.520989716053009\n",
            "loss on batch 114 in epoch 78 for the simple nn is: 0.5051866769790649\n",
            "loss on batch 115 in epoch 78 for the simple nn is: 0.42869386076927185\n",
            "loss on batch 116 in epoch 78 for the simple nn is: 0.5706208944320679\n",
            "loss on batch 117 in epoch 78 for the simple nn is: 0.5313318967819214\n",
            "loss on batch 118 in epoch 78 for the simple nn is: 0.5554795861244202\n",
            "loss on batch 119 in epoch 78 for the simple nn is: 0.5175883769989014\n",
            "loss on batch 120 in epoch 78 for the simple nn is: 0.4588254690170288\n",
            "loss on batch 0 in epoch 79 for the simple nn is: 0.4663475453853607\n",
            "loss on batch 1 in epoch 79 for the simple nn is: 0.6716599464416504\n",
            "loss on batch 2 in epoch 79 for the simple nn is: 0.603203535079956\n",
            "loss on batch 3 in epoch 79 for the simple nn is: 0.67696213722229\n",
            "loss on batch 4 in epoch 79 for the simple nn is: 0.533756673336029\n",
            "loss on batch 5 in epoch 79 for the simple nn is: 0.5577449202537537\n",
            "loss on batch 6 in epoch 79 for the simple nn is: 0.5213982462882996\n",
            "loss on batch 7 in epoch 79 for the simple nn is: 0.49771174788475037\n",
            "loss on batch 8 in epoch 79 for the simple nn is: 0.4779846966266632\n",
            "loss on batch 9 in epoch 79 for the simple nn is: 0.47495290637016296\n",
            "loss on batch 10 in epoch 79 for the simple nn is: 0.41265252232551575\n",
            "loss on batch 11 in epoch 79 for the simple nn is: 0.49029919505119324\n",
            "loss on batch 12 in epoch 79 for the simple nn is: 0.5285672545433044\n",
            "loss on batch 13 in epoch 79 for the simple nn is: 0.4902194142341614\n",
            "loss on batch 14 in epoch 79 for the simple nn is: 0.477418452501297\n",
            "loss on batch 15 in epoch 79 for the simple nn is: 0.4720229208469391\n",
            "loss on batch 16 in epoch 79 for the simple nn is: 0.5006436109542847\n",
            "loss on batch 17 in epoch 79 for the simple nn is: 0.47467055916786194\n",
            "loss on batch 18 in epoch 79 for the simple nn is: 0.5089640617370605\n",
            "loss on batch 19 in epoch 79 for the simple nn is: 0.45750969648361206\n",
            "loss on batch 20 in epoch 79 for the simple nn is: 0.4655088484287262\n",
            "loss on batch 21 in epoch 79 for the simple nn is: 0.5189719796180725\n",
            "loss on batch 22 in epoch 79 for the simple nn is: 0.4511721134185791\n",
            "loss on batch 23 in epoch 79 for the simple nn is: 0.5214171409606934\n",
            "loss on batch 24 in epoch 79 for the simple nn is: 0.39988604187965393\n",
            "loss on batch 25 in epoch 79 for the simple nn is: 0.4422749876976013\n",
            "loss on batch 26 in epoch 79 for the simple nn is: 0.5147676467895508\n",
            "loss on batch 27 in epoch 79 for the simple nn is: 0.4968660771846771\n",
            "loss on batch 28 in epoch 79 for the simple nn is: 0.45106643438339233\n",
            "loss on batch 29 in epoch 79 for the simple nn is: 0.5376014709472656\n",
            "loss on batch 30 in epoch 79 for the simple nn is: 0.4827398955821991\n",
            "loss on batch 31 in epoch 79 for the simple nn is: 0.5109145641326904\n",
            "loss on batch 32 in epoch 79 for the simple nn is: 0.43462714552879333\n",
            "loss on batch 33 in epoch 79 for the simple nn is: 0.49021363258361816\n",
            "loss on batch 34 in epoch 79 for the simple nn is: 0.44946545362472534\n",
            "loss on batch 35 in epoch 79 for the simple nn is: 0.46617013216018677\n",
            "loss on batch 36 in epoch 79 for the simple nn is: 0.4804016947746277\n",
            "loss on batch 37 in epoch 79 for the simple nn is: 0.4169759452342987\n",
            "loss on batch 38 in epoch 79 for the simple nn is: 0.5144045352935791\n",
            "loss on batch 39 in epoch 79 for the simple nn is: 0.41680097579956055\n",
            "loss on batch 40 in epoch 79 for the simple nn is: 0.49747297167778015\n",
            "loss on batch 41 in epoch 79 for the simple nn is: 0.4138410687446594\n",
            "loss on batch 42 in epoch 79 for the simple nn is: 0.5143910646438599\n",
            "loss on batch 43 in epoch 79 for the simple nn is: 0.48876699805259705\n",
            "loss on batch 44 in epoch 79 for the simple nn is: 0.496349036693573\n",
            "loss on batch 45 in epoch 79 for the simple nn is: 0.4669402837753296\n",
            "loss on batch 46 in epoch 79 for the simple nn is: 0.42923662066459656\n",
            "loss on batch 47 in epoch 79 for the simple nn is: 0.4595676064491272\n",
            "loss on batch 48 in epoch 79 for the simple nn is: 0.4756762385368347\n",
            "loss on batch 49 in epoch 79 for the simple nn is: 0.5201752185821533\n",
            "loss on batch 50 in epoch 79 for the simple nn is: 0.43954727053642273\n",
            "loss on batch 51 in epoch 79 for the simple nn is: 0.4583781957626343\n",
            "loss on batch 52 in epoch 79 for the simple nn is: 0.3794487714767456\n",
            "loss on batch 53 in epoch 79 for the simple nn is: 0.33895397186279297\n",
            "loss on batch 54 in epoch 79 for the simple nn is: 0.5325632095336914\n",
            "loss on batch 55 in epoch 79 for the simple nn is: 0.43792009353637695\n",
            "loss on batch 56 in epoch 79 for the simple nn is: 0.4714038670063019\n",
            "loss on batch 57 in epoch 79 for the simple nn is: 0.4945962429046631\n",
            "loss on batch 58 in epoch 79 for the simple nn is: 0.44968876242637634\n",
            "loss on batch 59 in epoch 79 for the simple nn is: 0.48257070779800415\n",
            "loss on batch 60 in epoch 79 for the simple nn is: 0.4428584575653076\n",
            "loss on batch 61 in epoch 79 for the simple nn is: 0.4339597523212433\n",
            "loss on batch 62 in epoch 79 for the simple nn is: 0.4526938498020172\n",
            "loss on batch 63 in epoch 79 for the simple nn is: 0.45805880427360535\n",
            "loss on batch 64 in epoch 79 for the simple nn is: 0.4062778055667877\n",
            "loss on batch 65 in epoch 79 for the simple nn is: 0.3976358473300934\n",
            "loss on batch 66 in epoch 79 for the simple nn is: 0.4640894830226898\n",
            "loss on batch 67 in epoch 79 for the simple nn is: 0.3916638493537903\n",
            "loss on batch 68 in epoch 79 for the simple nn is: 0.43731483817100525\n",
            "loss on batch 69 in epoch 79 for the simple nn is: 0.4443095326423645\n",
            "loss on batch 70 in epoch 79 for the simple nn is: 0.459346204996109\n",
            "loss on batch 71 in epoch 79 for the simple nn is: 0.4728319048881531\n",
            "loss on batch 72 in epoch 79 for the simple nn is: 0.4588353931903839\n",
            "loss on batch 73 in epoch 79 for the simple nn is: 0.495758593082428\n",
            "loss on batch 74 in epoch 79 for the simple nn is: 0.47532299160957336\n",
            "loss on batch 75 in epoch 79 for the simple nn is: 0.4678826928138733\n",
            "loss on batch 76 in epoch 79 for the simple nn is: 0.5221673846244812\n",
            "loss on batch 77 in epoch 79 for the simple nn is: 0.40272754430770874\n",
            "loss on batch 78 in epoch 79 for the simple nn is: 0.4660593271255493\n",
            "loss on batch 79 in epoch 79 for the simple nn is: 0.4031235873699188\n",
            "loss on batch 80 in epoch 79 for the simple nn is: 0.4533451497554779\n",
            "loss on batch 81 in epoch 79 for the simple nn is: 0.42702174186706543\n",
            "loss on batch 82 in epoch 79 for the simple nn is: 0.43738964200019836\n",
            "loss on batch 83 in epoch 79 for the simple nn is: 0.4662066102027893\n",
            "loss on batch 84 in epoch 79 for the simple nn is: 0.4752807915210724\n",
            "loss on batch 85 in epoch 79 for the simple nn is: 0.4747208058834076\n",
            "loss on batch 86 in epoch 79 for the simple nn is: 0.39073020219802856\n",
            "loss on batch 87 in epoch 79 for the simple nn is: 0.48360875248908997\n",
            "loss on batch 88 in epoch 79 for the simple nn is: 0.3459598124027252\n",
            "loss on batch 89 in epoch 79 for the simple nn is: 0.47348788380622864\n",
            "loss on batch 90 in epoch 79 for the simple nn is: 0.4384519159793854\n",
            "loss on batch 91 in epoch 79 for the simple nn is: 0.43711382150650024\n",
            "loss on batch 92 in epoch 79 for the simple nn is: 0.5300112366676331\n",
            "loss on batch 93 in epoch 79 for the simple nn is: 0.43969014286994934\n",
            "loss on batch 94 in epoch 79 for the simple nn is: 0.37585631012916565\n",
            "loss on batch 95 in epoch 79 for the simple nn is: 0.49070608615875244\n",
            "loss on batch 96 in epoch 79 for the simple nn is: 0.4158936142921448\n",
            "loss on batch 97 in epoch 79 for the simple nn is: 0.4897373616695404\n",
            "loss on batch 98 in epoch 79 for the simple nn is: 0.4804270565509796\n",
            "loss on batch 99 in epoch 79 for the simple nn is: 0.4146866202354431\n",
            "loss on batch 100 in epoch 79 for the simple nn is: 0.4577651023864746\n",
            "loss on batch 101 in epoch 79 for the simple nn is: 0.3992581367492676\n",
            "loss on batch 102 in epoch 79 for the simple nn is: 0.3513714075088501\n",
            "loss on batch 103 in epoch 79 for the simple nn is: 0.4820437431335449\n",
            "loss on batch 104 in epoch 79 for the simple nn is: 0.40967875719070435\n",
            "loss on batch 105 in epoch 79 for the simple nn is: 0.4152676463127136\n",
            "loss on batch 106 in epoch 79 for the simple nn is: 0.47681260108947754\n",
            "loss on batch 107 in epoch 79 for the simple nn is: 0.4153381586074829\n",
            "loss on batch 108 in epoch 79 for the simple nn is: 0.4268696904182434\n",
            "loss on batch 109 in epoch 79 for the simple nn is: 0.5158937573432922\n",
            "loss on batch 110 in epoch 79 for the simple nn is: 0.33010122179985046\n",
            "loss on batch 111 in epoch 79 for the simple nn is: 0.43377208709716797\n",
            "loss on batch 112 in epoch 79 for the simple nn is: 0.48729437589645386\n",
            "loss on batch 113 in epoch 79 for the simple nn is: 0.4799555540084839\n",
            "loss on batch 114 in epoch 79 for the simple nn is: 0.5050596594810486\n",
            "loss on batch 115 in epoch 79 for the simple nn is: 0.42338135838508606\n",
            "loss on batch 116 in epoch 79 for the simple nn is: 0.562483012676239\n",
            "loss on batch 117 in epoch 79 for the simple nn is: 0.5265234112739563\n",
            "loss on batch 118 in epoch 79 for the simple nn is: 0.5483536124229431\n",
            "loss on batch 119 in epoch 79 for the simple nn is: 0.5546475648880005\n",
            "loss on batch 120 in epoch 79 for the simple nn is: 0.47176244854927063\n",
            "loss on batch 0 in epoch 80 for the simple nn is: 0.489104300737381\n",
            "loss on batch 1 in epoch 80 for the simple nn is: 0.5936558842658997\n",
            "loss on batch 2 in epoch 80 for the simple nn is: 0.6097142100334167\n",
            "loss on batch 3 in epoch 80 for the simple nn is: 0.5414769053459167\n",
            "loss on batch 4 in epoch 80 for the simple nn is: 0.53416508436203\n",
            "loss on batch 5 in epoch 80 for the simple nn is: 0.550252377986908\n",
            "loss on batch 6 in epoch 80 for the simple nn is: 0.5187098383903503\n",
            "loss on batch 7 in epoch 80 for the simple nn is: 0.5047386288642883\n",
            "loss on batch 8 in epoch 80 for the simple nn is: 0.44899144768714905\n",
            "loss on batch 9 in epoch 80 for the simple nn is: 0.4699207544326782\n",
            "loss on batch 10 in epoch 80 for the simple nn is: 0.40418732166290283\n",
            "loss on batch 11 in epoch 80 for the simple nn is: 0.4861539602279663\n",
            "loss on batch 12 in epoch 80 for the simple nn is: 0.5200142860412598\n",
            "loss on batch 13 in epoch 80 for the simple nn is: 0.48452863097190857\n",
            "loss on batch 14 in epoch 80 for the simple nn is: 0.46775853633880615\n",
            "loss on batch 15 in epoch 80 for the simple nn is: 0.5805319547653198\n",
            "loss on batch 16 in epoch 80 for the simple nn is: 0.47448718547821045\n",
            "loss on batch 17 in epoch 80 for the simple nn is: 0.503318190574646\n",
            "loss on batch 18 in epoch 80 for the simple nn is: 0.44589418172836304\n",
            "loss on batch 19 in epoch 80 for the simple nn is: 0.4574102759361267\n",
            "loss on batch 20 in epoch 80 for the simple nn is: 0.4442387521266937\n",
            "loss on batch 21 in epoch 80 for the simple nn is: 0.5000168681144714\n",
            "loss on batch 22 in epoch 80 for the simple nn is: 0.5073191523551941\n",
            "loss on batch 23 in epoch 80 for the simple nn is: 0.5075650811195374\n",
            "loss on batch 24 in epoch 80 for the simple nn is: 0.48982810974121094\n",
            "loss on batch 25 in epoch 80 for the simple nn is: 0.42767974734306335\n",
            "loss on batch 26 in epoch 80 for the simple nn is: 0.4962046146392822\n",
            "loss on batch 27 in epoch 80 for the simple nn is: 0.46911337971687317\n",
            "loss on batch 28 in epoch 80 for the simple nn is: 0.46094849705696106\n",
            "loss on batch 29 in epoch 80 for the simple nn is: 0.5458839535713196\n",
            "loss on batch 30 in epoch 80 for the simple nn is: 0.524336576461792\n",
            "loss on batch 31 in epoch 80 for the simple nn is: 0.5090851783752441\n",
            "loss on batch 32 in epoch 80 for the simple nn is: 0.44024959206581116\n",
            "loss on batch 33 in epoch 80 for the simple nn is: 0.46132129430770874\n",
            "loss on batch 34 in epoch 80 for the simple nn is: 0.44588541984558105\n",
            "loss on batch 35 in epoch 80 for the simple nn is: 0.4882129430770874\n",
            "loss on batch 36 in epoch 80 for the simple nn is: 0.46679845452308655\n",
            "loss on batch 37 in epoch 80 for the simple nn is: 0.42567893862724304\n",
            "loss on batch 38 in epoch 80 for the simple nn is: 0.5183647871017456\n",
            "loss on batch 39 in epoch 80 for the simple nn is: 0.4063994586467743\n",
            "loss on batch 40 in epoch 80 for the simple nn is: 0.5307719111442566\n",
            "loss on batch 41 in epoch 80 for the simple nn is: 0.4372066259384155\n",
            "loss on batch 42 in epoch 80 for the simple nn is: 0.45841047167778015\n",
            "loss on batch 43 in epoch 80 for the simple nn is: 0.49864089488983154\n",
            "loss on batch 44 in epoch 80 for the simple nn is: 0.48415470123291016\n",
            "loss on batch 45 in epoch 80 for the simple nn is: 0.5557098388671875\n",
            "loss on batch 46 in epoch 80 for the simple nn is: 0.4613121747970581\n",
            "loss on batch 47 in epoch 80 for the simple nn is: 0.44661566615104675\n",
            "loss on batch 48 in epoch 80 for the simple nn is: 0.48125922679901123\n",
            "loss on batch 49 in epoch 80 for the simple nn is: 0.5246027708053589\n",
            "loss on batch 50 in epoch 80 for the simple nn is: 0.4590555727481842\n",
            "loss on batch 51 in epoch 80 for the simple nn is: 0.4707951545715332\n",
            "loss on batch 52 in epoch 80 for the simple nn is: 0.38368040323257446\n",
            "loss on batch 53 in epoch 80 for the simple nn is: 0.34851881861686707\n",
            "loss on batch 54 in epoch 80 for the simple nn is: 0.7131367325782776\n",
            "loss on batch 55 in epoch 80 for the simple nn is: 0.4414069950580597\n",
            "loss on batch 56 in epoch 80 for the simple nn is: 0.4704781174659729\n",
            "loss on batch 57 in epoch 80 for the simple nn is: 0.5104504823684692\n",
            "loss on batch 58 in epoch 80 for the simple nn is: 0.46702539920806885\n",
            "loss on batch 59 in epoch 80 for the simple nn is: 0.4927968382835388\n",
            "loss on batch 60 in epoch 80 for the simple nn is: 0.4549541771411896\n",
            "loss on batch 61 in epoch 80 for the simple nn is: 0.4531468451023102\n",
            "loss on batch 62 in epoch 80 for the simple nn is: 0.4645789563655853\n",
            "loss on batch 63 in epoch 80 for the simple nn is: 0.47666189074516296\n",
            "loss on batch 64 in epoch 80 for the simple nn is: 0.4391220808029175\n",
            "loss on batch 65 in epoch 80 for the simple nn is: 0.4213104248046875\n",
            "loss on batch 66 in epoch 80 for the simple nn is: 0.5010958909988403\n",
            "loss on batch 67 in epoch 80 for the simple nn is: 0.5214424729347229\n",
            "loss on batch 68 in epoch 80 for the simple nn is: 0.40140408277511597\n",
            "loss on batch 69 in epoch 80 for the simple nn is: 0.4706874489784241\n",
            "loss on batch 70 in epoch 80 for the simple nn is: 0.4988766610622406\n",
            "loss on batch 71 in epoch 80 for the simple nn is: 0.4758267402648926\n",
            "loss on batch 72 in epoch 80 for the simple nn is: 0.45706281065940857\n",
            "loss on batch 73 in epoch 80 for the simple nn is: 0.5120590925216675\n",
            "loss on batch 74 in epoch 80 for the simple nn is: 0.5147444009780884\n",
            "loss on batch 75 in epoch 80 for the simple nn is: 0.4689098298549652\n",
            "loss on batch 76 in epoch 80 for the simple nn is: 0.5026989579200745\n",
            "loss on batch 77 in epoch 80 for the simple nn is: 0.4160444140434265\n",
            "loss on batch 78 in epoch 80 for the simple nn is: 0.47529977560043335\n",
            "loss on batch 79 in epoch 80 for the simple nn is: 0.4419218599796295\n",
            "loss on batch 80 in epoch 80 for the simple nn is: 0.4512070417404175\n",
            "loss on batch 81 in epoch 80 for the simple nn is: 0.3870396018028259\n",
            "loss on batch 82 in epoch 80 for the simple nn is: 0.44559624791145325\n",
            "loss on batch 83 in epoch 80 for the simple nn is: 0.486783504486084\n",
            "loss on batch 84 in epoch 80 for the simple nn is: 0.4843940734863281\n",
            "loss on batch 85 in epoch 80 for the simple nn is: 0.48810192942619324\n",
            "loss on batch 86 in epoch 80 for the simple nn is: 0.40653735399246216\n",
            "loss on batch 87 in epoch 80 for the simple nn is: 0.4727935791015625\n",
            "loss on batch 88 in epoch 80 for the simple nn is: 0.37575143575668335\n",
            "loss on batch 89 in epoch 80 for the simple nn is: 0.47245004773139954\n",
            "loss on batch 90 in epoch 80 for the simple nn is: 0.45172086358070374\n",
            "loss on batch 91 in epoch 80 for the simple nn is: 0.44324225187301636\n",
            "loss on batch 92 in epoch 80 for the simple nn is: 0.5361137390136719\n",
            "loss on batch 93 in epoch 80 for the simple nn is: 0.4472652077674866\n",
            "loss on batch 94 in epoch 80 for the simple nn is: 0.39031022787094116\n",
            "loss on batch 95 in epoch 80 for the simple nn is: 0.4773026704788208\n",
            "loss on batch 96 in epoch 80 for the simple nn is: 0.39891549944877625\n",
            "loss on batch 97 in epoch 80 for the simple nn is: 0.4874386489391327\n",
            "loss on batch 98 in epoch 80 for the simple nn is: 0.43098175525665283\n",
            "loss on batch 99 in epoch 80 for the simple nn is: 0.4099242091178894\n",
            "loss on batch 100 in epoch 80 for the simple nn is: 0.4576530456542969\n",
            "loss on batch 101 in epoch 80 for the simple nn is: 0.3992689847946167\n",
            "loss on batch 102 in epoch 80 for the simple nn is: 0.3585016131401062\n",
            "loss on batch 103 in epoch 80 for the simple nn is: 0.5047040581703186\n",
            "loss on batch 104 in epoch 80 for the simple nn is: 0.39747297763824463\n",
            "loss on batch 105 in epoch 80 for the simple nn is: 0.4165858030319214\n",
            "loss on batch 106 in epoch 80 for the simple nn is: 0.4653046429157257\n",
            "loss on batch 107 in epoch 80 for the simple nn is: 0.4121415317058563\n",
            "loss on batch 108 in epoch 80 for the simple nn is: 0.4367228150367737\n",
            "loss on batch 109 in epoch 80 for the simple nn is: 0.5191987752914429\n",
            "loss on batch 110 in epoch 80 for the simple nn is: 0.3303663432598114\n",
            "loss on batch 111 in epoch 80 for the simple nn is: 0.44510841369628906\n",
            "loss on batch 112 in epoch 80 for the simple nn is: 0.4736476242542267\n",
            "loss on batch 113 in epoch 80 for the simple nn is: 0.4793824553489685\n",
            "loss on batch 114 in epoch 80 for the simple nn is: 0.5045955777168274\n",
            "loss on batch 115 in epoch 80 for the simple nn is: 0.4316765069961548\n",
            "loss on batch 116 in epoch 80 for the simple nn is: 0.5696970820426941\n",
            "loss on batch 117 in epoch 80 for the simple nn is: 0.523413360118866\n",
            "loss on batch 118 in epoch 80 for the simple nn is: 0.5478783845901489\n",
            "loss on batch 119 in epoch 80 for the simple nn is: 0.5167340636253357\n",
            "loss on batch 120 in epoch 80 for the simple nn is: 0.5483381748199463\n",
            "loss on batch 0 in epoch 81 for the simple nn is: 0.4588957726955414\n",
            "loss on batch 1 in epoch 81 for the simple nn is: 0.5925595164299011\n",
            "loss on batch 2 in epoch 81 for the simple nn is: 0.5793085694313049\n",
            "loss on batch 3 in epoch 81 for the simple nn is: 0.5528897643089294\n",
            "loss on batch 4 in epoch 81 for the simple nn is: 0.5312408208847046\n",
            "loss on batch 5 in epoch 81 for the simple nn is: 0.5418345928192139\n",
            "loss on batch 6 in epoch 81 for the simple nn is: 0.5347983837127686\n",
            "loss on batch 7 in epoch 81 for the simple nn is: 0.5138441920280457\n",
            "loss on batch 8 in epoch 81 for the simple nn is: 0.4422249495983124\n",
            "loss on batch 9 in epoch 81 for the simple nn is: 0.48147547245025635\n",
            "loss on batch 10 in epoch 81 for the simple nn is: 0.40437185764312744\n",
            "loss on batch 11 in epoch 81 for the simple nn is: 0.49692627787590027\n",
            "loss on batch 12 in epoch 81 for the simple nn is: 0.5476561784744263\n",
            "loss on batch 13 in epoch 81 for the simple nn is: 0.49378761649131775\n",
            "loss on batch 14 in epoch 81 for the simple nn is: 0.5725045800209045\n",
            "loss on batch 15 in epoch 81 for the simple nn is: 0.4493417739868164\n",
            "loss on batch 16 in epoch 81 for the simple nn is: 0.4907131493091583\n",
            "loss on batch 17 in epoch 81 for the simple nn is: 0.48926594853401184\n",
            "loss on batch 18 in epoch 81 for the simple nn is: 0.45380979776382446\n",
            "loss on batch 19 in epoch 81 for the simple nn is: 0.5528357028961182\n",
            "loss on batch 20 in epoch 81 for the simple nn is: 0.46170803904533386\n",
            "loss on batch 21 in epoch 81 for the simple nn is: 0.5010365843772888\n",
            "loss on batch 22 in epoch 81 for the simple nn is: 0.444491446018219\n",
            "loss on batch 23 in epoch 81 for the simple nn is: 0.4061165750026703\n",
            "loss on batch 24 in epoch 81 for the simple nn is: 0.4121263027191162\n",
            "loss on batch 25 in epoch 81 for the simple nn is: 0.4577000141143799\n",
            "loss on batch 26 in epoch 81 for the simple nn is: 0.4961489140987396\n",
            "loss on batch 27 in epoch 81 for the simple nn is: 0.6490140557289124\n",
            "loss on batch 28 in epoch 81 for the simple nn is: 0.5296763777732849\n",
            "loss on batch 29 in epoch 81 for the simple nn is: 0.5461000800132751\n",
            "loss on batch 30 in epoch 81 for the simple nn is: 0.49681296944618225\n",
            "loss on batch 31 in epoch 81 for the simple nn is: 0.5100992918014526\n",
            "loss on batch 32 in epoch 81 for the simple nn is: 0.4548671543598175\n",
            "loss on batch 33 in epoch 81 for the simple nn is: 0.4798477590084076\n",
            "loss on batch 34 in epoch 81 for the simple nn is: 0.4530250132083893\n",
            "loss on batch 35 in epoch 81 for the simple nn is: 0.6286172270774841\n",
            "loss on batch 36 in epoch 81 for the simple nn is: 0.5226055979728699\n",
            "loss on batch 37 in epoch 81 for the simple nn is: 0.4300687313079834\n",
            "loss on batch 38 in epoch 81 for the simple nn is: 0.5165863633155823\n",
            "loss on batch 39 in epoch 81 for the simple nn is: 0.4067482650279999\n",
            "loss on batch 40 in epoch 81 for the simple nn is: 0.48789355158805847\n",
            "loss on batch 41 in epoch 81 for the simple nn is: 0.436245858669281\n",
            "loss on batch 42 in epoch 81 for the simple nn is: 0.48840707540512085\n",
            "loss on batch 43 in epoch 81 for the simple nn is: 0.48879262804985046\n",
            "loss on batch 44 in epoch 81 for the simple nn is: 0.5190220475196838\n",
            "loss on batch 45 in epoch 81 for the simple nn is: 0.4968806207180023\n",
            "loss on batch 46 in epoch 81 for the simple nn is: 0.5397765636444092\n",
            "loss on batch 47 in epoch 81 for the simple nn is: 0.44696006178855896\n",
            "loss on batch 48 in epoch 81 for the simple nn is: 0.48882341384887695\n",
            "loss on batch 49 in epoch 81 for the simple nn is: 0.5782591104507446\n",
            "loss on batch 50 in epoch 81 for the simple nn is: 0.4508814215660095\n",
            "loss on batch 51 in epoch 81 for the simple nn is: 0.4659527540206909\n",
            "loss on batch 52 in epoch 81 for the simple nn is: 0.376941442489624\n",
            "loss on batch 53 in epoch 81 for the simple nn is: 0.3637348711490631\n",
            "loss on batch 54 in epoch 81 for the simple nn is: 0.5379521250724792\n",
            "loss on batch 55 in epoch 81 for the simple nn is: 0.45298561453819275\n",
            "loss on batch 56 in epoch 81 for the simple nn is: 0.5278176665306091\n",
            "loss on batch 57 in epoch 81 for the simple nn is: 0.5231203436851501\n",
            "loss on batch 58 in epoch 81 for the simple nn is: 0.4737468361854553\n",
            "loss on batch 59 in epoch 81 for the simple nn is: 0.47707509994506836\n",
            "loss on batch 60 in epoch 81 for the simple nn is: 0.44979071617126465\n",
            "loss on batch 61 in epoch 81 for the simple nn is: 0.4858986735343933\n",
            "loss on batch 62 in epoch 81 for the simple nn is: 0.5486659407615662\n",
            "loss on batch 63 in epoch 81 for the simple nn is: 0.4820100963115692\n",
            "loss on batch 64 in epoch 81 for the simple nn is: 0.44957226514816284\n",
            "loss on batch 65 in epoch 81 for the simple nn is: 0.4311355948448181\n",
            "loss on batch 66 in epoch 81 for the simple nn is: 0.48301467299461365\n",
            "loss on batch 67 in epoch 81 for the simple nn is: 0.39880502223968506\n",
            "loss on batch 68 in epoch 81 for the simple nn is: 0.43318408727645874\n",
            "loss on batch 69 in epoch 81 for the simple nn is: 0.49506500363349915\n",
            "loss on batch 70 in epoch 81 for the simple nn is: 0.48035070300102234\n",
            "loss on batch 71 in epoch 81 for the simple nn is: 0.4563571810722351\n",
            "loss on batch 72 in epoch 81 for the simple nn is: 0.45840442180633545\n",
            "loss on batch 73 in epoch 81 for the simple nn is: 0.5111731290817261\n",
            "loss on batch 74 in epoch 81 for the simple nn is: 0.4870356321334839\n",
            "loss on batch 75 in epoch 81 for the simple nn is: 0.5201451182365417\n",
            "loss on batch 76 in epoch 81 for the simple nn is: 0.5235905647277832\n",
            "loss on batch 77 in epoch 81 for the simple nn is: 0.42539727687835693\n",
            "loss on batch 78 in epoch 81 for the simple nn is: 0.49172651767730713\n",
            "loss on batch 79 in epoch 81 for the simple nn is: 0.4848380386829376\n",
            "loss on batch 80 in epoch 81 for the simple nn is: 0.45122671127319336\n",
            "loss on batch 81 in epoch 81 for the simple nn is: 0.3878161907196045\n",
            "loss on batch 82 in epoch 81 for the simple nn is: 0.4477241039276123\n",
            "loss on batch 83 in epoch 81 for the simple nn is: 0.4745085537433624\n",
            "loss on batch 84 in epoch 81 for the simple nn is: 0.53482985496521\n",
            "loss on batch 85 in epoch 81 for the simple nn is: 0.48903024196624756\n",
            "loss on batch 86 in epoch 81 for the simple nn is: 0.3837648332118988\n",
            "loss on batch 87 in epoch 81 for the simple nn is: 0.4763535261154175\n",
            "loss on batch 88 in epoch 81 for the simple nn is: 0.36996757984161377\n",
            "loss on batch 89 in epoch 81 for the simple nn is: 0.5180535912513733\n",
            "loss on batch 90 in epoch 81 for the simple nn is: 0.46076419949531555\n",
            "loss on batch 91 in epoch 81 for the simple nn is: 0.44335460662841797\n",
            "loss on batch 92 in epoch 81 for the simple nn is: 0.5271764397621155\n",
            "loss on batch 93 in epoch 81 for the simple nn is: 0.4449388384819031\n",
            "loss on batch 94 in epoch 81 for the simple nn is: 0.4135879874229431\n",
            "loss on batch 95 in epoch 81 for the simple nn is: 0.4793583154678345\n",
            "loss on batch 96 in epoch 81 for the simple nn is: 0.41906049847602844\n",
            "loss on batch 97 in epoch 81 for the simple nn is: 0.48973965644836426\n",
            "loss on batch 98 in epoch 81 for the simple nn is: 0.44213956594467163\n",
            "loss on batch 99 in epoch 81 for the simple nn is: 0.414469450712204\n",
            "loss on batch 100 in epoch 81 for the simple nn is: 0.46425294876098633\n",
            "loss on batch 101 in epoch 81 for the simple nn is: 0.4012652039527893\n",
            "loss on batch 102 in epoch 81 for the simple nn is: 0.37031814455986023\n",
            "loss on batch 103 in epoch 81 for the simple nn is: 0.48548078536987305\n",
            "loss on batch 104 in epoch 81 for the simple nn is: 0.43422338366508484\n",
            "loss on batch 105 in epoch 81 for the simple nn is: 0.43671905994415283\n",
            "loss on batch 106 in epoch 81 for the simple nn is: 0.4796557128429413\n",
            "loss on batch 107 in epoch 81 for the simple nn is: 0.4229840636253357\n",
            "loss on batch 108 in epoch 81 for the simple nn is: 0.45704782009124756\n",
            "loss on batch 109 in epoch 81 for the simple nn is: 0.5079815983772278\n",
            "loss on batch 110 in epoch 81 for the simple nn is: 0.3381122946739197\n",
            "loss on batch 111 in epoch 81 for the simple nn is: 0.471111536026001\n",
            "loss on batch 112 in epoch 81 for the simple nn is: 0.48180070519447327\n",
            "loss on batch 113 in epoch 81 for the simple nn is: 0.4921030104160309\n",
            "loss on batch 114 in epoch 81 for the simple nn is: 0.5034658908843994\n",
            "loss on batch 115 in epoch 81 for the simple nn is: 0.4718843400478363\n",
            "loss on batch 116 in epoch 81 for the simple nn is: 0.5629730820655823\n",
            "loss on batch 117 in epoch 81 for the simple nn is: 0.5210702419281006\n",
            "loss on batch 118 in epoch 81 for the simple nn is: 0.5486955046653748\n",
            "loss on batch 119 in epoch 81 for the simple nn is: 0.7713363766670227\n",
            "loss on batch 120 in epoch 81 for the simple nn is: 0.47214585542678833\n",
            "loss on batch 0 in epoch 82 for the simple nn is: 0.4684526324272156\n",
            "loss on batch 1 in epoch 82 for the simple nn is: 0.5925490260124207\n",
            "loss on batch 2 in epoch 82 for the simple nn is: 0.5564570426940918\n",
            "loss on batch 3 in epoch 82 for the simple nn is: 0.5323597192764282\n",
            "loss on batch 4 in epoch 82 for the simple nn is: 0.534157931804657\n",
            "loss on batch 5 in epoch 82 for the simple nn is: 0.5754219889640808\n",
            "loss on batch 6 in epoch 82 for the simple nn is: 0.5208715796470642\n",
            "loss on batch 7 in epoch 82 for the simple nn is: 0.4983300268650055\n",
            "loss on batch 8 in epoch 82 for the simple nn is: 0.4485919177532196\n",
            "loss on batch 9 in epoch 82 for the simple nn is: 0.4713459610939026\n",
            "loss on batch 10 in epoch 82 for the simple nn is: 0.4231162369251251\n",
            "loss on batch 11 in epoch 82 for the simple nn is: 0.49758341908454895\n",
            "loss on batch 12 in epoch 82 for the simple nn is: 0.540301501750946\n",
            "loss on batch 13 in epoch 82 for the simple nn is: 0.5017380714416504\n",
            "loss on batch 14 in epoch 82 for the simple nn is: 0.4865807592868805\n",
            "loss on batch 15 in epoch 82 for the simple nn is: 0.4544796645641327\n",
            "loss on batch 16 in epoch 82 for the simple nn is: 0.4945599436759949\n",
            "loss on batch 17 in epoch 82 for the simple nn is: 0.4772101044654846\n",
            "loss on batch 18 in epoch 82 for the simple nn is: 0.45590677857398987\n",
            "loss on batch 19 in epoch 82 for the simple nn is: 0.4587417542934418\n",
            "loss on batch 20 in epoch 82 for the simple nn is: 0.43477827310562134\n",
            "loss on batch 21 in epoch 82 for the simple nn is: 0.5023442506790161\n",
            "loss on batch 22 in epoch 82 for the simple nn is: 0.45093491673469543\n",
            "loss on batch 23 in epoch 82 for the simple nn is: 0.4342036843299866\n",
            "loss on batch 24 in epoch 82 for the simple nn is: 0.3767305910587311\n",
            "loss on batch 25 in epoch 82 for the simple nn is: 0.4284425973892212\n",
            "loss on batch 26 in epoch 82 for the simple nn is: 0.5090256929397583\n",
            "loss on batch 27 in epoch 82 for the simple nn is: 0.45872440934181213\n",
            "loss on batch 28 in epoch 82 for the simple nn is: 0.4491567015647888\n",
            "loss on batch 29 in epoch 82 for the simple nn is: 0.545775294303894\n",
            "loss on batch 30 in epoch 82 for the simple nn is: 0.4969578981399536\n",
            "loss on batch 31 in epoch 82 for the simple nn is: 0.5135906338691711\n",
            "loss on batch 32 in epoch 82 for the simple nn is: 0.44033023715019226\n",
            "loss on batch 33 in epoch 82 for the simple nn is: 0.5655786991119385\n",
            "loss on batch 34 in epoch 82 for the simple nn is: 0.4417736530303955\n",
            "loss on batch 35 in epoch 82 for the simple nn is: 0.49507617950439453\n",
            "loss on batch 36 in epoch 82 for the simple nn is: 0.4660979211330414\n",
            "loss on batch 37 in epoch 82 for the simple nn is: 0.4213045835494995\n",
            "loss on batch 38 in epoch 82 for the simple nn is: 0.5020703673362732\n",
            "loss on batch 39 in epoch 82 for the simple nn is: 0.39803165197372437\n",
            "loss on batch 40 in epoch 82 for the simple nn is: 0.511767566204071\n",
            "loss on batch 41 in epoch 82 for the simple nn is: 0.41522446274757385\n",
            "loss on batch 42 in epoch 82 for the simple nn is: 0.4690806269645691\n",
            "loss on batch 43 in epoch 82 for the simple nn is: 0.5647331476211548\n",
            "loss on batch 44 in epoch 82 for the simple nn is: 0.491880863904953\n",
            "loss on batch 45 in epoch 82 for the simple nn is: 0.4652956426143646\n",
            "loss on batch 46 in epoch 82 for the simple nn is: 0.41692352294921875\n",
            "loss on batch 47 in epoch 82 for the simple nn is: 0.44794130325317383\n",
            "loss on batch 48 in epoch 82 for the simple nn is: 0.49051403999328613\n",
            "loss on batch 49 in epoch 82 for the simple nn is: 0.5135930776596069\n",
            "loss on batch 50 in epoch 82 for the simple nn is: 0.43012735247612\n",
            "loss on batch 51 in epoch 82 for the simple nn is: 0.4863393008708954\n",
            "loss on batch 52 in epoch 82 for the simple nn is: 0.3713601529598236\n",
            "loss on batch 53 in epoch 82 for the simple nn is: 0.3466065526008606\n",
            "loss on batch 54 in epoch 82 for the simple nn is: 0.5388156175613403\n",
            "loss on batch 55 in epoch 82 for the simple nn is: 0.4520191550254822\n",
            "loss on batch 56 in epoch 82 for the simple nn is: 0.46958038210868835\n",
            "loss on batch 57 in epoch 82 for the simple nn is: 0.5280845165252686\n",
            "loss on batch 58 in epoch 82 for the simple nn is: 0.4514695405960083\n",
            "loss on batch 59 in epoch 82 for the simple nn is: 0.46861356496810913\n",
            "loss on batch 60 in epoch 82 for the simple nn is: 0.4399421215057373\n",
            "loss on batch 61 in epoch 82 for the simple nn is: 0.44617071747779846\n",
            "loss on batch 62 in epoch 82 for the simple nn is: 0.45287251472473145\n",
            "loss on batch 63 in epoch 82 for the simple nn is: 0.4528482258319855\n",
            "loss on batch 64 in epoch 82 for the simple nn is: 0.41624191403388977\n",
            "loss on batch 65 in epoch 82 for the simple nn is: 0.414551317691803\n",
            "loss on batch 66 in epoch 82 for the simple nn is: 0.45895758271217346\n",
            "loss on batch 67 in epoch 82 for the simple nn is: 0.3924929201602936\n",
            "loss on batch 68 in epoch 82 for the simple nn is: 0.5071198344230652\n",
            "loss on batch 69 in epoch 82 for the simple nn is: 0.46551841497421265\n",
            "loss on batch 70 in epoch 82 for the simple nn is: 0.46240565180778503\n",
            "loss on batch 71 in epoch 82 for the simple nn is: 0.4661877155303955\n",
            "loss on batch 72 in epoch 82 for the simple nn is: 0.47047773003578186\n",
            "loss on batch 73 in epoch 82 for the simple nn is: 0.4990292191505432\n",
            "loss on batch 74 in epoch 82 for the simple nn is: 0.4800459146499634\n",
            "loss on batch 75 in epoch 82 for the simple nn is: 0.49520695209503174\n",
            "loss on batch 76 in epoch 82 for the simple nn is: 0.49094244837760925\n",
            "loss on batch 77 in epoch 82 for the simple nn is: 0.4166784882545471\n",
            "loss on batch 78 in epoch 82 for the simple nn is: 0.4823269546031952\n",
            "loss on batch 79 in epoch 82 for the simple nn is: 0.39676979184150696\n",
            "loss on batch 80 in epoch 82 for the simple nn is: 0.44450125098228455\n",
            "loss on batch 81 in epoch 82 for the simple nn is: 0.39016100764274597\n",
            "loss on batch 82 in epoch 82 for the simple nn is: 0.44673123955726624\n",
            "loss on batch 83 in epoch 82 for the simple nn is: 0.4786328077316284\n",
            "loss on batch 84 in epoch 82 for the simple nn is: 0.4849125146865845\n",
            "loss on batch 85 in epoch 82 for the simple nn is: 0.48763492703437805\n",
            "loss on batch 86 in epoch 82 for the simple nn is: 0.3954552114009857\n",
            "loss on batch 87 in epoch 82 for the simple nn is: 0.48134803771972656\n",
            "loss on batch 88 in epoch 82 for the simple nn is: 0.3534199297428131\n",
            "loss on batch 89 in epoch 82 for the simple nn is: 0.4701736867427826\n",
            "loss on batch 90 in epoch 82 for the simple nn is: 0.42789098620414734\n",
            "loss on batch 91 in epoch 82 for the simple nn is: 0.4139624834060669\n",
            "loss on batch 92 in epoch 82 for the simple nn is: 0.5272186398506165\n",
            "loss on batch 93 in epoch 82 for the simple nn is: 0.44202789664268494\n",
            "loss on batch 94 in epoch 82 for the simple nn is: 0.37483301758766174\n",
            "loss on batch 95 in epoch 82 for the simple nn is: 0.4783265292644501\n",
            "loss on batch 96 in epoch 82 for the simple nn is: 0.43899789452552795\n",
            "loss on batch 97 in epoch 82 for the simple nn is: 0.4999784529209137\n",
            "loss on batch 98 in epoch 82 for the simple nn is: 0.43333861231803894\n",
            "loss on batch 99 in epoch 82 for the simple nn is: 0.40173542499542236\n",
            "loss on batch 100 in epoch 82 for the simple nn is: 0.4596714675426483\n",
            "loss on batch 101 in epoch 82 for the simple nn is: 0.4000503122806549\n",
            "loss on batch 102 in epoch 82 for the simple nn is: 0.3568038046360016\n",
            "loss on batch 103 in epoch 82 for the simple nn is: 0.47463148832321167\n",
            "loss on batch 104 in epoch 82 for the simple nn is: 0.3965817391872406\n",
            "loss on batch 105 in epoch 82 for the simple nn is: 0.505145251750946\n",
            "loss on batch 106 in epoch 82 for the simple nn is: 0.47329235076904297\n",
            "loss on batch 107 in epoch 82 for the simple nn is: 0.42402350902557373\n",
            "loss on batch 108 in epoch 82 for the simple nn is: 0.4076829254627228\n",
            "loss on batch 109 in epoch 82 for the simple nn is: 0.5053815245628357\n",
            "loss on batch 110 in epoch 82 for the simple nn is: 0.36810487508773804\n",
            "loss on batch 111 in epoch 82 for the simple nn is: 0.42591214179992676\n",
            "loss on batch 112 in epoch 82 for the simple nn is: 0.4703471064567566\n",
            "loss on batch 113 in epoch 82 for the simple nn is: 0.5835439562797546\n",
            "loss on batch 114 in epoch 82 for the simple nn is: 0.5029980540275574\n",
            "loss on batch 115 in epoch 82 for the simple nn is: 0.5304938554763794\n",
            "loss on batch 116 in epoch 82 for the simple nn is: 0.5549958348274231\n",
            "loss on batch 117 in epoch 82 for the simple nn is: 0.518143355846405\n",
            "loss on batch 118 in epoch 82 for the simple nn is: 0.5730980038642883\n",
            "loss on batch 119 in epoch 82 for the simple nn is: 0.5259510278701782\n",
            "loss on batch 120 in epoch 82 for the simple nn is: 0.4623304307460785\n",
            "loss on batch 0 in epoch 83 for the simple nn is: 0.4410361349582672\n",
            "loss on batch 1 in epoch 83 for the simple nn is: 0.5959821343421936\n",
            "loss on batch 2 in epoch 83 for the simple nn is: 0.5669800639152527\n",
            "loss on batch 3 in epoch 83 for the simple nn is: 0.5538074374198914\n",
            "loss on batch 4 in epoch 83 for the simple nn is: 0.5373209118843079\n",
            "loss on batch 5 in epoch 83 for the simple nn is: 0.6121089458465576\n",
            "loss on batch 6 in epoch 83 for the simple nn is: 0.5103655457496643\n",
            "loss on batch 7 in epoch 83 for the simple nn is: 0.4885140359401703\n",
            "loss on batch 8 in epoch 83 for the simple nn is: 0.47507598996162415\n",
            "loss on batch 9 in epoch 83 for the simple nn is: 0.47555211186408997\n",
            "loss on batch 10 in epoch 83 for the simple nn is: 0.4135110080242157\n",
            "loss on batch 11 in epoch 83 for the simple nn is: 0.49723175168037415\n",
            "loss on batch 12 in epoch 83 for the simple nn is: 0.6491369605064392\n",
            "loss on batch 13 in epoch 83 for the simple nn is: 0.5077028870582581\n",
            "loss on batch 14 in epoch 83 for the simple nn is: 0.4771072268486023\n",
            "loss on batch 15 in epoch 83 for the simple nn is: 0.44137927889823914\n",
            "loss on batch 16 in epoch 83 for the simple nn is: 0.4899119734764099\n",
            "loss on batch 17 in epoch 83 for the simple nn is: 0.5015272498130798\n",
            "loss on batch 18 in epoch 83 for the simple nn is: 0.46008220314979553\n",
            "loss on batch 19 in epoch 83 for the simple nn is: 0.4704929292201996\n",
            "loss on batch 20 in epoch 83 for the simple nn is: 0.4289090633392334\n",
            "loss on batch 21 in epoch 83 for the simple nn is: 0.5055221915245056\n",
            "loss on batch 22 in epoch 83 for the simple nn is: 0.4743206202983856\n",
            "loss on batch 23 in epoch 83 for the simple nn is: 0.4322037994861603\n",
            "loss on batch 24 in epoch 83 for the simple nn is: 0.39163488149642944\n",
            "loss on batch 25 in epoch 83 for the simple nn is: 0.5799170732498169\n",
            "loss on batch 26 in epoch 83 for the simple nn is: 0.5122395157814026\n",
            "loss on batch 27 in epoch 83 for the simple nn is: 0.4487673044204712\n",
            "loss on batch 28 in epoch 83 for the simple nn is: 0.4717535376548767\n",
            "loss on batch 29 in epoch 83 for the simple nn is: 0.5585578680038452\n",
            "loss on batch 30 in epoch 83 for the simple nn is: 0.5054277181625366\n",
            "loss on batch 31 in epoch 83 for the simple nn is: 0.5297121405601501\n",
            "loss on batch 32 in epoch 83 for the simple nn is: 0.6102923154830933\n",
            "loss on batch 33 in epoch 83 for the simple nn is: 0.47187936305999756\n",
            "loss on batch 34 in epoch 83 for the simple nn is: 0.44855940341949463\n",
            "loss on batch 35 in epoch 83 for the simple nn is: 0.4972696304321289\n",
            "loss on batch 36 in epoch 83 for the simple nn is: 0.469831258058548\n",
            "loss on batch 37 in epoch 83 for the simple nn is: 0.4764558672904968\n",
            "loss on batch 38 in epoch 83 for the simple nn is: 0.5145595073699951\n",
            "loss on batch 39 in epoch 83 for the simple nn is: 0.40906745195388794\n",
            "loss on batch 40 in epoch 83 for the simple nn is: 0.4944785535335541\n",
            "loss on batch 41 in epoch 83 for the simple nn is: 0.43750903010368347\n",
            "loss on batch 42 in epoch 83 for the simple nn is: 0.4795631468296051\n",
            "loss on batch 43 in epoch 83 for the simple nn is: 0.4860999286174774\n",
            "loss on batch 44 in epoch 83 for the simple nn is: 0.4958679974079132\n",
            "loss on batch 45 in epoch 83 for the simple nn is: 0.4730691611766815\n",
            "loss on batch 46 in epoch 83 for the simple nn is: 0.4408920407295227\n",
            "loss on batch 47 in epoch 83 for the simple nn is: 0.4552417993545532\n",
            "loss on batch 48 in epoch 83 for the simple nn is: 0.521712601184845\n",
            "loss on batch 49 in epoch 83 for the simple nn is: 0.5492215752601624\n",
            "loss on batch 50 in epoch 83 for the simple nn is: 0.47569355368614197\n",
            "loss on batch 51 in epoch 83 for the simple nn is: 0.4872567653656006\n",
            "loss on batch 52 in epoch 83 for the simple nn is: 0.3679574429988861\n",
            "loss on batch 53 in epoch 83 for the simple nn is: 0.40438663959503174\n",
            "loss on batch 54 in epoch 83 for the simple nn is: 0.5384613871574402\n",
            "loss on batch 55 in epoch 83 for the simple nn is: 0.5108567476272583\n",
            "loss on batch 56 in epoch 83 for the simple nn is: 0.4839717447757721\n",
            "loss on batch 57 in epoch 83 for the simple nn is: 0.49920493364334106\n",
            "loss on batch 58 in epoch 83 for the simple nn is: 0.49153417348861694\n",
            "loss on batch 59 in epoch 83 for the simple nn is: 0.5171166062355042\n",
            "loss on batch 60 in epoch 83 for the simple nn is: 0.4472671449184418\n",
            "loss on batch 61 in epoch 83 for the simple nn is: 0.46428051590919495\n",
            "loss on batch 62 in epoch 83 for the simple nn is: 0.45953136682510376\n",
            "loss on batch 63 in epoch 83 for the simple nn is: 0.4647778272628784\n",
            "loss on batch 64 in epoch 83 for the simple nn is: 0.432688444852829\n",
            "loss on batch 65 in epoch 83 for the simple nn is: 0.41424569487571716\n",
            "loss on batch 66 in epoch 83 for the simple nn is: 0.47037744522094727\n",
            "loss on batch 67 in epoch 83 for the simple nn is: 0.3968372046947479\n",
            "loss on batch 68 in epoch 83 for the simple nn is: 0.39139842987060547\n",
            "loss on batch 69 in epoch 83 for the simple nn is: 0.4422256350517273\n",
            "loss on batch 70 in epoch 83 for the simple nn is: 0.4645937979221344\n",
            "loss on batch 71 in epoch 83 for the simple nn is: 0.4966168701648712\n",
            "loss on batch 72 in epoch 83 for the simple nn is: 0.5030982494354248\n",
            "loss on batch 73 in epoch 83 for the simple nn is: 0.5139405131340027\n",
            "loss on batch 74 in epoch 83 for the simple nn is: 0.4672500491142273\n",
            "loss on batch 75 in epoch 83 for the simple nn is: 0.45922958850860596\n",
            "loss on batch 76 in epoch 83 for the simple nn is: 0.46458399295806885\n",
            "loss on batch 77 in epoch 83 for the simple nn is: 0.40773752331733704\n",
            "loss on batch 78 in epoch 83 for the simple nn is: 0.5153218507766724\n",
            "loss on batch 79 in epoch 83 for the simple nn is: 0.48395201563835144\n",
            "loss on batch 80 in epoch 83 for the simple nn is: 0.445132315158844\n",
            "loss on batch 81 in epoch 83 for the simple nn is: 0.37369242310523987\n",
            "loss on batch 82 in epoch 83 for the simple nn is: 0.4446585178375244\n",
            "loss on batch 83 in epoch 83 for the simple nn is: 0.4649488925933838\n",
            "loss on batch 84 in epoch 83 for the simple nn is: 0.4829719066619873\n",
            "loss on batch 85 in epoch 83 for the simple nn is: 0.47679629921913147\n",
            "loss on batch 86 in epoch 83 for the simple nn is: 0.37690868973731995\n",
            "loss on batch 87 in epoch 83 for the simple nn is: 0.46322503685951233\n",
            "loss on batch 88 in epoch 83 for the simple nn is: 0.35051798820495605\n",
            "loss on batch 89 in epoch 83 for the simple nn is: 0.4709247052669525\n",
            "loss on batch 90 in epoch 83 for the simple nn is: 0.42209458351135254\n",
            "loss on batch 91 in epoch 83 for the simple nn is: 0.44445788860321045\n",
            "loss on batch 92 in epoch 83 for the simple nn is: 0.5195621252059937\n",
            "loss on batch 93 in epoch 83 for the simple nn is: 0.4384954571723938\n",
            "loss on batch 94 in epoch 83 for the simple nn is: 0.3549945652484894\n",
            "loss on batch 95 in epoch 83 for the simple nn is: 0.4768889844417572\n",
            "loss on batch 96 in epoch 83 for the simple nn is: 0.39164814352989197\n",
            "loss on batch 97 in epoch 83 for the simple nn is: 0.4961439073085785\n",
            "loss on batch 98 in epoch 83 for the simple nn is: 0.6332586407661438\n",
            "loss on batch 99 in epoch 83 for the simple nn is: 0.4099319279193878\n",
            "loss on batch 100 in epoch 83 for the simple nn is: 0.4584115445613861\n",
            "loss on batch 101 in epoch 83 for the simple nn is: 0.39912548661231995\n",
            "loss on batch 102 in epoch 83 for the simple nn is: 0.3605186641216278\n",
            "loss on batch 103 in epoch 83 for the simple nn is: 0.4844985604286194\n",
            "loss on batch 104 in epoch 83 for the simple nn is: 0.4492555558681488\n",
            "loss on batch 105 in epoch 83 for the simple nn is: 0.40126776695251465\n",
            "loss on batch 106 in epoch 83 for the simple nn is: 0.5779538154602051\n",
            "loss on batch 107 in epoch 83 for the simple nn is: 0.5756006240844727\n",
            "loss on batch 108 in epoch 83 for the simple nn is: 0.43571001291275024\n",
            "loss on batch 109 in epoch 83 for the simple nn is: 0.5067670941352844\n",
            "loss on batch 110 in epoch 83 for the simple nn is: 0.3528886139392853\n",
            "loss on batch 111 in epoch 83 for the simple nn is: 0.42525410652160645\n",
            "loss on batch 112 in epoch 83 for the simple nn is: 0.48467519879341125\n",
            "loss on batch 113 in epoch 83 for the simple nn is: 0.4870113730430603\n",
            "loss on batch 114 in epoch 83 for the simple nn is: 0.5035791993141174\n",
            "loss on batch 115 in epoch 83 for the simple nn is: 0.4545869529247284\n",
            "loss on batch 116 in epoch 83 for the simple nn is: 0.5808537602424622\n",
            "loss on batch 117 in epoch 83 for the simple nn is: 0.5219837427139282\n",
            "loss on batch 118 in epoch 83 for the simple nn is: 0.6118316054344177\n",
            "loss on batch 119 in epoch 83 for the simple nn is: 0.5187321305274963\n",
            "loss on batch 120 in epoch 83 for the simple nn is: 0.5575760006904602\n",
            "loss on batch 0 in epoch 84 for the simple nn is: 0.4754050076007843\n",
            "loss on batch 1 in epoch 84 for the simple nn is: 0.592964231967926\n",
            "loss on batch 2 in epoch 84 for the simple nn is: 0.5652402639389038\n",
            "loss on batch 3 in epoch 84 for the simple nn is: 0.5383583903312683\n",
            "loss on batch 4 in epoch 84 for the simple nn is: 0.5233426690101624\n",
            "loss on batch 5 in epoch 84 for the simple nn is: 0.5879030227661133\n",
            "loss on batch 6 in epoch 84 for the simple nn is: 0.5172672271728516\n",
            "loss on batch 7 in epoch 84 for the simple nn is: 0.49637994170188904\n",
            "loss on batch 8 in epoch 84 for the simple nn is: 0.5505015254020691\n",
            "loss on batch 9 in epoch 84 for the simple nn is: 0.46607357263565063\n",
            "loss on batch 10 in epoch 84 for the simple nn is: 0.39728254079818726\n",
            "loss on batch 11 in epoch 84 for the simple nn is: 0.5007620453834534\n",
            "loss on batch 12 in epoch 84 for the simple nn is: 0.5124801397323608\n",
            "loss on batch 13 in epoch 84 for the simple nn is: 0.5045962333679199\n",
            "loss on batch 14 in epoch 84 for the simple nn is: 0.4735428988933563\n",
            "loss on batch 15 in epoch 84 for the simple nn is: 0.47815316915512085\n",
            "loss on batch 16 in epoch 84 for the simple nn is: 0.5462267994880676\n",
            "loss on batch 17 in epoch 84 for the simple nn is: 0.5722584128379822\n",
            "loss on batch 18 in epoch 84 for the simple nn is: 0.4976343810558319\n",
            "loss on batch 19 in epoch 84 for the simple nn is: 0.48393160104751587\n",
            "loss on batch 20 in epoch 84 for the simple nn is: 0.45963209867477417\n",
            "loss on batch 21 in epoch 84 for the simple nn is: 0.52112877368927\n",
            "loss on batch 22 in epoch 84 for the simple nn is: 0.4575478732585907\n",
            "loss on batch 23 in epoch 84 for the simple nn is: 0.45534807443618774\n",
            "loss on batch 24 in epoch 84 for the simple nn is: 0.3811597526073456\n",
            "loss on batch 25 in epoch 84 for the simple nn is: 0.4397200345993042\n",
            "loss on batch 26 in epoch 84 for the simple nn is: 0.4949539303779602\n",
            "loss on batch 27 in epoch 84 for the simple nn is: 0.44737523794174194\n",
            "loss on batch 28 in epoch 84 for the simple nn is: 0.4422263205051422\n",
            "loss on batch 29 in epoch 84 for the simple nn is: 0.5578267574310303\n",
            "loss on batch 30 in epoch 84 for the simple nn is: 0.48405227065086365\n",
            "loss on batch 31 in epoch 84 for the simple nn is: 0.5126305818557739\n",
            "loss on batch 32 in epoch 84 for the simple nn is: 0.43111151456832886\n",
            "loss on batch 33 in epoch 84 for the simple nn is: 0.45577722787857056\n",
            "loss on batch 34 in epoch 84 for the simple nn is: 0.4384256601333618\n",
            "loss on batch 35 in epoch 84 for the simple nn is: 0.49344974756240845\n",
            "loss on batch 36 in epoch 84 for the simple nn is: 0.4644121527671814\n",
            "loss on batch 37 in epoch 84 for the simple nn is: 0.40797024965286255\n",
            "loss on batch 38 in epoch 84 for the simple nn is: 0.5363598465919495\n",
            "loss on batch 39 in epoch 84 for the simple nn is: 0.39239710569381714\n",
            "loss on batch 40 in epoch 84 for the simple nn is: 0.4697491526603699\n",
            "loss on batch 41 in epoch 84 for the simple nn is: 0.4297931492328644\n",
            "loss on batch 42 in epoch 84 for the simple nn is: 0.45568811893463135\n",
            "loss on batch 43 in epoch 84 for the simple nn is: 0.4805452227592468\n",
            "loss on batch 44 in epoch 84 for the simple nn is: 0.4920709431171417\n",
            "loss on batch 45 in epoch 84 for the simple nn is: 0.45444455742836\n",
            "loss on batch 46 in epoch 84 for the simple nn is: 0.4217119812965393\n",
            "loss on batch 47 in epoch 84 for the simple nn is: 0.44643911719322205\n",
            "loss on batch 48 in epoch 84 for the simple nn is: 0.4671618342399597\n",
            "loss on batch 49 in epoch 84 for the simple nn is: 0.5262910723686218\n",
            "loss on batch 50 in epoch 84 for the simple nn is: 0.44478723406791687\n",
            "loss on batch 51 in epoch 84 for the simple nn is: 0.47410088777542114\n",
            "loss on batch 52 in epoch 84 for the simple nn is: 0.36224228143692017\n",
            "loss on batch 53 in epoch 84 for the simple nn is: 0.3379466235637665\n",
            "loss on batch 54 in epoch 84 for the simple nn is: 0.5373808741569519\n",
            "loss on batch 55 in epoch 84 for the simple nn is: 0.4537947475910187\n",
            "loss on batch 56 in epoch 84 for the simple nn is: 0.46650224924087524\n",
            "loss on batch 57 in epoch 84 for the simple nn is: 0.49363499879837036\n",
            "loss on batch 58 in epoch 84 for the simple nn is: 0.43501797318458557\n",
            "loss on batch 59 in epoch 84 for the simple nn is: 0.5083174705505371\n",
            "loss on batch 60 in epoch 84 for the simple nn is: 0.4398333430290222\n",
            "loss on batch 61 in epoch 84 for the simple nn is: 0.4348319172859192\n",
            "loss on batch 62 in epoch 84 for the simple nn is: 0.46164369583129883\n",
            "loss on batch 63 in epoch 84 for the simple nn is: 0.45676389336586\n",
            "loss on batch 64 in epoch 84 for the simple nn is: 0.4116864800453186\n",
            "loss on batch 65 in epoch 84 for the simple nn is: 0.3827817440032959\n",
            "loss on batch 66 in epoch 84 for the simple nn is: 0.45211684703826904\n",
            "loss on batch 67 in epoch 84 for the simple nn is: 0.4005295932292938\n",
            "loss on batch 68 in epoch 84 for the simple nn is: 0.4013363718986511\n",
            "loss on batch 69 in epoch 84 for the simple nn is: 0.4398479461669922\n",
            "loss on batch 70 in epoch 84 for the simple nn is: 0.4497200846672058\n",
            "loss on batch 71 in epoch 84 for the simple nn is: 0.45590394735336304\n",
            "loss on batch 72 in epoch 84 for the simple nn is: 0.456928014755249\n",
            "loss on batch 73 in epoch 84 for the simple nn is: 0.5276220440864563\n",
            "loss on batch 74 in epoch 84 for the simple nn is: 0.4714467227458954\n",
            "loss on batch 75 in epoch 84 for the simple nn is: 0.46005478501319885\n",
            "loss on batch 76 in epoch 84 for the simple nn is: 0.45438694953918457\n",
            "loss on batch 77 in epoch 84 for the simple nn is: 0.4173581600189209\n",
            "loss on batch 78 in epoch 84 for the simple nn is: 0.4503289461135864\n",
            "loss on batch 79 in epoch 84 for the simple nn is: 0.36635690927505493\n",
            "loss on batch 80 in epoch 84 for the simple nn is: 0.4482885003089905\n",
            "loss on batch 81 in epoch 84 for the simple nn is: 0.3847757875919342\n",
            "loss on batch 82 in epoch 84 for the simple nn is: 0.44139599800109863\n",
            "loss on batch 83 in epoch 84 for the simple nn is: 0.4615199565887451\n",
            "loss on batch 84 in epoch 84 for the simple nn is: 0.5285966992378235\n",
            "loss on batch 85 in epoch 84 for the simple nn is: 0.494145929813385\n",
            "loss on batch 86 in epoch 84 for the simple nn is: 0.36545851826667786\n",
            "loss on batch 87 in epoch 84 for the simple nn is: 0.4602123498916626\n",
            "loss on batch 88 in epoch 84 for the simple nn is: 0.34278857707977295\n",
            "loss on batch 89 in epoch 84 for the simple nn is: 0.48335182666778564\n",
            "loss on batch 90 in epoch 84 for the simple nn is: 0.4145992696285248\n",
            "loss on batch 91 in epoch 84 for the simple nn is: 0.4534219205379486\n",
            "loss on batch 92 in epoch 84 for the simple nn is: 0.5270758867263794\n",
            "loss on batch 93 in epoch 84 for the simple nn is: 0.44107088446617126\n",
            "loss on batch 94 in epoch 84 for the simple nn is: 0.4410833418369293\n",
            "loss on batch 95 in epoch 84 for the simple nn is: 0.4924943149089813\n",
            "loss on batch 96 in epoch 84 for the simple nn is: 0.4431587755680084\n",
            "loss on batch 97 in epoch 84 for the simple nn is: 0.4896402657032013\n",
            "loss on batch 98 in epoch 84 for the simple nn is: 0.43321746587753296\n",
            "loss on batch 99 in epoch 84 for the simple nn is: 0.4166252017021179\n",
            "loss on batch 100 in epoch 84 for the simple nn is: 0.45493507385253906\n",
            "loss on batch 101 in epoch 84 for the simple nn is: 0.3977324068546295\n",
            "loss on batch 102 in epoch 84 for the simple nn is: 0.37814220786094666\n",
            "loss on batch 103 in epoch 84 for the simple nn is: 0.4731542468070984\n",
            "loss on batch 104 in epoch 84 for the simple nn is: 0.42325422167778015\n",
            "loss on batch 105 in epoch 84 for the simple nn is: 0.4154895842075348\n",
            "loss on batch 106 in epoch 84 for the simple nn is: 0.48783963918685913\n",
            "loss on batch 107 in epoch 84 for the simple nn is: 0.4077163636684418\n",
            "loss on batch 108 in epoch 84 for the simple nn is: 0.4916535019874573\n",
            "loss on batch 109 in epoch 84 for the simple nn is: 0.521302342414856\n",
            "loss on batch 110 in epoch 84 for the simple nn is: 0.3809220492839813\n",
            "loss on batch 111 in epoch 84 for the simple nn is: 0.44220834970474243\n",
            "loss on batch 112 in epoch 84 for the simple nn is: 0.4709833562374115\n",
            "loss on batch 113 in epoch 84 for the simple nn is: 0.481912761926651\n",
            "loss on batch 114 in epoch 84 for the simple nn is: 0.5071239471435547\n",
            "loss on batch 115 in epoch 84 for the simple nn is: 0.4329235553741455\n",
            "loss on batch 116 in epoch 84 for the simple nn is: 0.566058874130249\n",
            "loss on batch 117 in epoch 84 for the simple nn is: 0.5382687449455261\n",
            "loss on batch 118 in epoch 84 for the simple nn is: 0.6343712210655212\n",
            "loss on batch 119 in epoch 84 for the simple nn is: 0.5388047695159912\n",
            "loss on batch 120 in epoch 84 for the simple nn is: 0.4709324538707733\n",
            "loss on batch 0 in epoch 85 for the simple nn is: 0.49639904499053955\n",
            "loss on batch 1 in epoch 85 for the simple nn is: 0.5932585597038269\n",
            "loss on batch 2 in epoch 85 for the simple nn is: 0.5585811138153076\n",
            "loss on batch 3 in epoch 85 for the simple nn is: 0.5343899130821228\n",
            "loss on batch 4 in epoch 85 for the simple nn is: 0.5526214241981506\n",
            "loss on batch 5 in epoch 85 for the simple nn is: 0.5594092607498169\n",
            "loss on batch 6 in epoch 85 for the simple nn is: 0.5020363330841064\n",
            "loss on batch 7 in epoch 85 for the simple nn is: 0.4929078221321106\n",
            "loss on batch 8 in epoch 85 for the simple nn is: 0.47552528977394104\n",
            "loss on batch 9 in epoch 85 for the simple nn is: 0.4698163568973541\n",
            "loss on batch 10 in epoch 85 for the simple nn is: 0.39828765392303467\n",
            "loss on batch 11 in epoch 85 for the simple nn is: 0.49609631299972534\n",
            "loss on batch 12 in epoch 85 for the simple nn is: 0.5138427019119263\n",
            "loss on batch 13 in epoch 85 for the simple nn is: 0.48540714383125305\n",
            "loss on batch 14 in epoch 85 for the simple nn is: 0.46221476793289185\n",
            "loss on batch 15 in epoch 85 for the simple nn is: 0.4484711289405823\n",
            "loss on batch 16 in epoch 85 for the simple nn is: 0.5373432636260986\n",
            "loss on batch 17 in epoch 85 for the simple nn is: 0.44003114104270935\n",
            "loss on batch 18 in epoch 85 for the simple nn is: 0.45635947585105896\n",
            "loss on batch 19 in epoch 85 for the simple nn is: 0.45641180872917175\n",
            "loss on batch 20 in epoch 85 for the simple nn is: 0.4410286843776703\n",
            "loss on batch 21 in epoch 85 for the simple nn is: 0.5405293107032776\n",
            "loss on batch 22 in epoch 85 for the simple nn is: 0.4669957160949707\n",
            "loss on batch 23 in epoch 85 for the simple nn is: 0.446562796831131\n",
            "loss on batch 24 in epoch 85 for the simple nn is: 0.36915674805641174\n",
            "loss on batch 25 in epoch 85 for the simple nn is: 0.4201124906539917\n",
            "loss on batch 26 in epoch 85 for the simple nn is: 0.4863831102848053\n",
            "loss on batch 27 in epoch 85 for the simple nn is: 0.44765767455101013\n",
            "loss on batch 28 in epoch 85 for the simple nn is: 0.45806893706321716\n",
            "loss on batch 29 in epoch 85 for the simple nn is: 0.5665650367736816\n",
            "loss on batch 30 in epoch 85 for the simple nn is: 0.506445050239563\n",
            "loss on batch 31 in epoch 85 for the simple nn is: 0.5088272094726562\n",
            "loss on batch 32 in epoch 85 for the simple nn is: 0.43622615933418274\n",
            "loss on batch 33 in epoch 85 for the simple nn is: 0.457863450050354\n",
            "loss on batch 34 in epoch 85 for the simple nn is: 0.45062559843063354\n",
            "loss on batch 35 in epoch 85 for the simple nn is: 0.5048651695251465\n",
            "loss on batch 36 in epoch 85 for the simple nn is: 0.5319488644599915\n",
            "loss on batch 37 in epoch 85 for the simple nn is: 0.42172062397003174\n",
            "loss on batch 38 in epoch 85 for the simple nn is: 0.49909234046936035\n",
            "loss on batch 39 in epoch 85 for the simple nn is: 0.3772413432598114\n",
            "loss on batch 40 in epoch 85 for the simple nn is: 0.5134221315383911\n",
            "loss on batch 41 in epoch 85 for the simple nn is: 0.5372143983840942\n",
            "loss on batch 42 in epoch 85 for the simple nn is: 0.4542192816734314\n",
            "loss on batch 43 in epoch 85 for the simple nn is: 0.4732755422592163\n",
            "loss on batch 44 in epoch 85 for the simple nn is: 0.4936016798019409\n",
            "loss on batch 45 in epoch 85 for the simple nn is: 0.47105398774147034\n",
            "loss on batch 46 in epoch 85 for the simple nn is: 0.422325998544693\n",
            "loss on batch 47 in epoch 85 for the simple nn is: 0.5195170044898987\n",
            "loss on batch 48 in epoch 85 for the simple nn is: 0.4670671820640564\n",
            "loss on batch 49 in epoch 85 for the simple nn is: 0.5243011713027954\n",
            "loss on batch 50 in epoch 85 for the simple nn is: 0.42969444394111633\n",
            "loss on batch 51 in epoch 85 for the simple nn is: 0.47494184970855713\n",
            "loss on batch 52 in epoch 85 for the simple nn is: 0.39490029215812683\n",
            "loss on batch 53 in epoch 85 for the simple nn is: 0.340410441160202\n",
            "loss on batch 54 in epoch 85 for the simple nn is: 0.5338518023490906\n",
            "loss on batch 55 in epoch 85 for the simple nn is: 0.5079777240753174\n",
            "loss on batch 56 in epoch 85 for the simple nn is: 0.4664047062397003\n",
            "loss on batch 57 in epoch 85 for the simple nn is: 0.49372559785842896\n",
            "loss on batch 58 in epoch 85 for the simple nn is: 0.4601861536502838\n",
            "loss on batch 59 in epoch 85 for the simple nn is: 0.4590972363948822\n",
            "loss on batch 60 in epoch 85 for the simple nn is: 0.45745423436164856\n",
            "loss on batch 61 in epoch 85 for the simple nn is: 0.4518830478191376\n",
            "loss on batch 62 in epoch 85 for the simple nn is: 0.46373263001441956\n",
            "loss on batch 63 in epoch 85 for the simple nn is: 0.47011449933052063\n",
            "loss on batch 64 in epoch 85 for the simple nn is: 0.40545299649238586\n",
            "loss on batch 65 in epoch 85 for the simple nn is: 0.39843007922172546\n",
            "loss on batch 66 in epoch 85 for the simple nn is: 0.45031896233558655\n",
            "loss on batch 67 in epoch 85 for the simple nn is: 0.38483673334121704\n",
            "loss on batch 68 in epoch 85 for the simple nn is: 0.3895774185657501\n",
            "loss on batch 69 in epoch 85 for the simple nn is: 0.454621285200119\n",
            "loss on batch 70 in epoch 85 for the simple nn is: 0.5284822583198547\n",
            "loss on batch 71 in epoch 85 for the simple nn is: 0.4478151798248291\n",
            "loss on batch 72 in epoch 85 for the simple nn is: 0.4792628288269043\n",
            "loss on batch 73 in epoch 85 for the simple nn is: 0.48036691546440125\n",
            "loss on batch 74 in epoch 85 for the simple nn is: 0.48856258392333984\n",
            "loss on batch 75 in epoch 85 for the simple nn is: 0.4561215937137604\n",
            "loss on batch 76 in epoch 85 for the simple nn is: 0.5272879004478455\n",
            "loss on batch 77 in epoch 85 for the simple nn is: 0.40950629115104675\n",
            "loss on batch 78 in epoch 85 for the simple nn is: 0.4534376859664917\n",
            "loss on batch 79 in epoch 85 for the simple nn is: 0.3656357228755951\n",
            "loss on batch 80 in epoch 85 for the simple nn is: 0.4535212516784668\n",
            "loss on batch 81 in epoch 85 for the simple nn is: 0.37883231043815613\n",
            "loss on batch 82 in epoch 85 for the simple nn is: 0.4540849030017853\n",
            "loss on batch 83 in epoch 85 for the simple nn is: 0.4765363037586212\n",
            "loss on batch 84 in epoch 85 for the simple nn is: 0.470567911863327\n",
            "loss on batch 85 in epoch 85 for the simple nn is: 0.5067996382713318\n",
            "loss on batch 86 in epoch 85 for the simple nn is: 0.49916040897369385\n",
            "loss on batch 87 in epoch 85 for the simple nn is: 0.45297324657440186\n",
            "loss on batch 88 in epoch 85 for the simple nn is: 0.34376001358032227\n",
            "loss on batch 89 in epoch 85 for the simple nn is: 0.4865438640117645\n",
            "loss on batch 90 in epoch 85 for the simple nn is: 0.4558042287826538\n",
            "loss on batch 91 in epoch 85 for the simple nn is: 0.4473434388637543\n",
            "loss on batch 92 in epoch 85 for the simple nn is: 0.5957449674606323\n",
            "loss on batch 93 in epoch 85 for the simple nn is: 0.44653230905532837\n",
            "loss on batch 94 in epoch 85 for the simple nn is: 0.3868202865123749\n",
            "loss on batch 95 in epoch 85 for the simple nn is: 0.4995018541812897\n",
            "loss on batch 96 in epoch 85 for the simple nn is: 0.414551705121994\n",
            "loss on batch 97 in epoch 85 for the simple nn is: 0.48590001463890076\n",
            "loss on batch 98 in epoch 85 for the simple nn is: 0.43542835116386414\n",
            "loss on batch 99 in epoch 85 for the simple nn is: 0.40203943848609924\n",
            "loss on batch 100 in epoch 85 for the simple nn is: 0.4598361849784851\n",
            "loss on batch 101 in epoch 85 for the simple nn is: 0.4367828071117401\n",
            "loss on batch 102 in epoch 85 for the simple nn is: 0.3804425001144409\n",
            "loss on batch 103 in epoch 85 for the simple nn is: 0.47312140464782715\n",
            "loss on batch 104 in epoch 85 for the simple nn is: 0.4030097723007202\n",
            "loss on batch 105 in epoch 85 for the simple nn is: 0.41579321026802063\n",
            "loss on batch 106 in epoch 85 for the simple nn is: 0.4735400676727295\n",
            "loss on batch 107 in epoch 85 for the simple nn is: 0.6419561505317688\n",
            "loss on batch 108 in epoch 85 for the simple nn is: 0.42112478613853455\n",
            "loss on batch 109 in epoch 85 for the simple nn is: 0.5069730281829834\n",
            "loss on batch 110 in epoch 85 for the simple nn is: 0.43557775020599365\n",
            "loss on batch 111 in epoch 85 for the simple nn is: 0.41750213503837585\n",
            "loss on batch 112 in epoch 85 for the simple nn is: 0.46861323714256287\n",
            "loss on batch 113 in epoch 85 for the simple nn is: 0.474751353263855\n",
            "loss on batch 114 in epoch 85 for the simple nn is: 0.4990631639957428\n",
            "loss on batch 115 in epoch 85 for the simple nn is: 0.44544482231140137\n",
            "loss on batch 116 in epoch 85 for the simple nn is: 0.5480905175209045\n",
            "loss on batch 117 in epoch 85 for the simple nn is: 0.5250058174133301\n",
            "loss on batch 118 in epoch 85 for the simple nn is: 0.5505937337875366\n",
            "loss on batch 119 in epoch 85 for the simple nn is: 0.5165207386016846\n",
            "loss on batch 120 in epoch 85 for the simple nn is: 0.4414573311805725\n",
            "loss on batch 0 in epoch 86 for the simple nn is: 0.4686379134654999\n",
            "loss on batch 1 in epoch 86 for the simple nn is: 0.8400385975837708\n",
            "loss on batch 2 in epoch 86 for the simple nn is: 0.5674750804901123\n",
            "loss on batch 3 in epoch 86 for the simple nn is: 0.5491151809692383\n",
            "loss on batch 4 in epoch 86 for the simple nn is: 0.5283907055854797\n",
            "loss on batch 5 in epoch 86 for the simple nn is: 0.5271981358528137\n",
            "loss on batch 6 in epoch 86 for the simple nn is: 0.5014777183532715\n",
            "loss on batch 7 in epoch 86 for the simple nn is: 0.4819384813308716\n",
            "loss on batch 8 in epoch 86 for the simple nn is: 0.4810947775840759\n",
            "loss on batch 9 in epoch 86 for the simple nn is: 0.5692198872566223\n",
            "loss on batch 10 in epoch 86 for the simple nn is: 0.4761495292186737\n",
            "loss on batch 11 in epoch 86 for the simple nn is: 0.47799479961395264\n",
            "loss on batch 12 in epoch 86 for the simple nn is: 0.5160301327705383\n",
            "loss on batch 13 in epoch 86 for the simple nn is: 0.49839627742767334\n",
            "loss on batch 14 in epoch 86 for the simple nn is: 0.5129479169845581\n",
            "loss on batch 15 in epoch 86 for the simple nn is: 0.4467206597328186\n",
            "loss on batch 16 in epoch 86 for the simple nn is: 0.4835730493068695\n",
            "loss on batch 17 in epoch 86 for the simple nn is: 0.894843339920044\n",
            "loss on batch 18 in epoch 86 for the simple nn is: 0.5148693919181824\n",
            "loss on batch 19 in epoch 86 for the simple nn is: 0.5709400773048401\n",
            "loss on batch 20 in epoch 86 for the simple nn is: 0.48013070225715637\n",
            "loss on batch 21 in epoch 86 for the simple nn is: 0.5038638710975647\n",
            "loss on batch 22 in epoch 86 for the simple nn is: 0.4500337839126587\n",
            "loss on batch 23 in epoch 86 for the simple nn is: 0.44500258564949036\n",
            "loss on batch 24 in epoch 86 for the simple nn is: 0.3952801823616028\n",
            "loss on batch 25 in epoch 86 for the simple nn is: 0.45172110199928284\n",
            "loss on batch 26 in epoch 86 for the simple nn is: 0.5227296948432922\n",
            "loss on batch 27 in epoch 86 for the simple nn is: 0.5584776997566223\n",
            "loss on batch 28 in epoch 86 for the simple nn is: 0.4871693253517151\n",
            "loss on batch 29 in epoch 86 for the simple nn is: 0.5582714676856995\n",
            "loss on batch 30 in epoch 86 for the simple nn is: 0.5015535950660706\n",
            "loss on batch 31 in epoch 86 for the simple nn is: 0.5279713273048401\n",
            "loss on batch 32 in epoch 86 for the simple nn is: 0.5036003589630127\n",
            "loss on batch 33 in epoch 86 for the simple nn is: 0.4729987382888794\n",
            "loss on batch 34 in epoch 86 for the simple nn is: 0.4423984885215759\n",
            "loss on batch 35 in epoch 86 for the simple nn is: 0.47175562381744385\n",
            "loss on batch 36 in epoch 86 for the simple nn is: 0.47212347388267517\n",
            "loss on batch 37 in epoch 86 for the simple nn is: 0.4677532911300659\n",
            "loss on batch 38 in epoch 86 for the simple nn is: 0.5840625762939453\n",
            "loss on batch 39 in epoch 86 for the simple nn is: 0.37645119428634644\n",
            "loss on batch 40 in epoch 86 for the simple nn is: 0.4866514801979065\n",
            "loss on batch 41 in epoch 86 for the simple nn is: 0.41353994607925415\n",
            "loss on batch 42 in epoch 86 for the simple nn is: 0.5409868955612183\n",
            "loss on batch 43 in epoch 86 for the simple nn is: 0.5205745100975037\n",
            "loss on batch 44 in epoch 86 for the simple nn is: 0.5617288947105408\n",
            "loss on batch 45 in epoch 86 for the simple nn is: 0.47858667373657227\n",
            "loss on batch 46 in epoch 86 for the simple nn is: 0.43008190393447876\n",
            "loss on batch 47 in epoch 86 for the simple nn is: 0.4322032034397125\n",
            "loss on batch 48 in epoch 86 for the simple nn is: 0.7752504944801331\n",
            "loss on batch 49 in epoch 86 for the simple nn is: 0.5384642481803894\n",
            "loss on batch 50 in epoch 86 for the simple nn is: 0.4581182897090912\n",
            "loss on batch 51 in epoch 86 for the simple nn is: 0.49669763445854187\n",
            "loss on batch 52 in epoch 86 for the simple nn is: 0.38940784335136414\n",
            "loss on batch 53 in epoch 86 for the simple nn is: 0.4169537425041199\n",
            "loss on batch 54 in epoch 86 for the simple nn is: 0.5308200120925903\n",
            "loss on batch 55 in epoch 86 for the simple nn is: 0.4501701891422272\n",
            "loss on batch 56 in epoch 86 for the simple nn is: 0.46532881259918213\n",
            "loss on batch 57 in epoch 86 for the simple nn is: 0.5070219039916992\n",
            "loss on batch 58 in epoch 86 for the simple nn is: 0.45591840147972107\n",
            "loss on batch 59 in epoch 86 for the simple nn is: 0.47189846634864807\n",
            "loss on batch 60 in epoch 86 for the simple nn is: 0.4534332752227783\n",
            "loss on batch 61 in epoch 86 for the simple nn is: 0.43354636430740356\n",
            "loss on batch 62 in epoch 86 for the simple nn is: 0.4755370020866394\n",
            "loss on batch 63 in epoch 86 for the simple nn is: 0.4764711260795593\n",
            "loss on batch 64 in epoch 86 for the simple nn is: 0.4790022671222687\n",
            "loss on batch 65 in epoch 86 for the simple nn is: 0.42058631777763367\n",
            "loss on batch 66 in epoch 86 for the simple nn is: 0.4586540162563324\n",
            "loss on batch 67 in epoch 86 for the simple nn is: 0.39735618233680725\n",
            "loss on batch 68 in epoch 86 for the simple nn is: 0.4233696460723877\n",
            "loss on batch 69 in epoch 86 for the simple nn is: 0.46357202529907227\n",
            "loss on batch 70 in epoch 86 for the simple nn is: 0.4852404296398163\n",
            "loss on batch 71 in epoch 86 for the simple nn is: 0.481558620929718\n",
            "loss on batch 72 in epoch 86 for the simple nn is: 0.4930078089237213\n",
            "loss on batch 73 in epoch 86 for the simple nn is: 0.505077064037323\n",
            "loss on batch 74 in epoch 86 for the simple nn is: 0.48908287286758423\n",
            "loss on batch 75 in epoch 86 for the simple nn is: 0.5419347286224365\n",
            "loss on batch 76 in epoch 86 for the simple nn is: 0.47843295335769653\n",
            "loss on batch 77 in epoch 86 for the simple nn is: 0.47662651538848877\n",
            "loss on batch 78 in epoch 86 for the simple nn is: 0.4484911262989044\n",
            "loss on batch 79 in epoch 86 for the simple nn is: 0.3903452157974243\n",
            "loss on batch 80 in epoch 86 for the simple nn is: 0.44370636343955994\n",
            "loss on batch 81 in epoch 86 for the simple nn is: 0.3997095227241516\n",
            "loss on batch 82 in epoch 86 for the simple nn is: 0.458700031042099\n",
            "loss on batch 83 in epoch 86 for the simple nn is: 0.559195339679718\n",
            "loss on batch 84 in epoch 86 for the simple nn is: 0.46801504492759705\n",
            "loss on batch 85 in epoch 86 for the simple nn is: 0.47975432872772217\n",
            "loss on batch 86 in epoch 86 for the simple nn is: 0.37509071826934814\n",
            "loss on batch 87 in epoch 86 for the simple nn is: 0.4555339813232422\n",
            "loss on batch 88 in epoch 86 for the simple nn is: 0.3792140483856201\n",
            "loss on batch 89 in epoch 86 for the simple nn is: 0.48394203186035156\n",
            "loss on batch 90 in epoch 86 for the simple nn is: 0.4533255994319916\n",
            "loss on batch 91 in epoch 86 for the simple nn is: 0.44364747405052185\n",
            "loss on batch 92 in epoch 86 for the simple nn is: 0.5215355157852173\n",
            "loss on batch 93 in epoch 86 for the simple nn is: 0.4422141909599304\n",
            "loss on batch 94 in epoch 86 for the simple nn is: 0.37584659457206726\n",
            "loss on batch 95 in epoch 86 for the simple nn is: 0.6850948333740234\n",
            "loss on batch 96 in epoch 86 for the simple nn is: 0.45722725987434387\n",
            "loss on batch 97 in epoch 86 for the simple nn is: 0.4752485752105713\n",
            "loss on batch 98 in epoch 86 for the simple nn is: 0.4354773759841919\n",
            "loss on batch 99 in epoch 86 for the simple nn is: 0.4245270788669586\n",
            "loss on batch 100 in epoch 86 for the simple nn is: 0.45375052094459534\n",
            "loss on batch 101 in epoch 86 for the simple nn is: 0.39904069900512695\n",
            "loss on batch 102 in epoch 86 for the simple nn is: 0.4889983832836151\n",
            "loss on batch 103 in epoch 86 for the simple nn is: 0.4798571467399597\n",
            "loss on batch 104 in epoch 86 for the simple nn is: 0.3857017457485199\n",
            "loss on batch 105 in epoch 86 for the simple nn is: 0.434752881526947\n",
            "loss on batch 106 in epoch 86 for the simple nn is: 0.5071529150009155\n",
            "loss on batch 107 in epoch 86 for the simple nn is: 0.42284253239631653\n",
            "loss on batch 108 in epoch 86 for the simple nn is: 0.669647216796875\n",
            "loss on batch 109 in epoch 86 for the simple nn is: 0.5266867876052856\n",
            "loss on batch 110 in epoch 86 for the simple nn is: 0.36585280299186707\n",
            "loss on batch 111 in epoch 86 for the simple nn is: 0.5321919322013855\n",
            "loss on batch 112 in epoch 86 for the simple nn is: 0.5042622089385986\n",
            "loss on batch 113 in epoch 86 for the simple nn is: 0.49938541650772095\n",
            "loss on batch 114 in epoch 86 for the simple nn is: 0.5202571749687195\n",
            "loss on batch 115 in epoch 86 for the simple nn is: 0.5072054266929626\n",
            "loss on batch 116 in epoch 86 for the simple nn is: 0.5452242493629456\n",
            "loss on batch 117 in epoch 86 for the simple nn is: 0.540656328201294\n",
            "loss on batch 118 in epoch 86 for the simple nn is: 0.5758579969406128\n",
            "loss on batch 119 in epoch 86 for the simple nn is: 0.5585277676582336\n",
            "loss on batch 120 in epoch 86 for the simple nn is: 0.5338227152824402\n",
            "loss on batch 0 in epoch 87 for the simple nn is: 0.49278706312179565\n",
            "loss on batch 1 in epoch 87 for the simple nn is: 0.6219685077667236\n",
            "loss on batch 2 in epoch 87 for the simple nn is: 0.6613113284111023\n",
            "loss on batch 3 in epoch 87 for the simple nn is: 0.5453794598579407\n",
            "loss on batch 4 in epoch 87 for the simple nn is: 0.527145504951477\n",
            "loss on batch 5 in epoch 87 for the simple nn is: 0.5185368657112122\n",
            "loss on batch 6 in epoch 87 for the simple nn is: 0.5032142400741577\n",
            "loss on batch 7 in epoch 87 for the simple nn is: 0.4921487867832184\n",
            "loss on batch 8 in epoch 87 for the simple nn is: 0.4860856533050537\n",
            "loss on batch 9 in epoch 87 for the simple nn is: 0.4987601637840271\n",
            "loss on batch 10 in epoch 87 for the simple nn is: 0.4218473732471466\n",
            "loss on batch 11 in epoch 87 for the simple nn is: 0.6112935543060303\n",
            "loss on batch 12 in epoch 87 for the simple nn is: 0.5420984625816345\n",
            "loss on batch 13 in epoch 87 for the simple nn is: 0.4930131435394287\n",
            "loss on batch 14 in epoch 87 for the simple nn is: 0.45384877920150757\n",
            "loss on batch 15 in epoch 87 for the simple nn is: 0.45891737937927246\n",
            "loss on batch 16 in epoch 87 for the simple nn is: 0.48338115215301514\n",
            "loss on batch 17 in epoch 87 for the simple nn is: 0.448933869600296\n",
            "loss on batch 18 in epoch 87 for the simple nn is: 0.49495694041252136\n",
            "loss on batch 19 in epoch 87 for the simple nn is: 0.47070324420928955\n",
            "loss on batch 20 in epoch 87 for the simple nn is: 0.45912331342697144\n",
            "loss on batch 21 in epoch 87 for the simple nn is: 0.47925132513046265\n",
            "loss on batch 22 in epoch 87 for the simple nn is: 0.4223382771015167\n",
            "loss on batch 23 in epoch 87 for the simple nn is: 0.4234176278114319\n",
            "loss on batch 24 in epoch 87 for the simple nn is: 0.38323739171028137\n",
            "loss on batch 25 in epoch 87 for the simple nn is: 0.4177263081073761\n",
            "loss on batch 26 in epoch 87 for the simple nn is: 0.49769386649131775\n",
            "loss on batch 27 in epoch 87 for the simple nn is: 0.4732205867767334\n",
            "loss on batch 28 in epoch 87 for the simple nn is: 0.44781604409217834\n",
            "loss on batch 29 in epoch 87 for the simple nn is: 0.5387693047523499\n",
            "loss on batch 30 in epoch 87 for the simple nn is: 0.4826176166534424\n",
            "loss on batch 31 in epoch 87 for the simple nn is: 0.48880302906036377\n",
            "loss on batch 32 in epoch 87 for the simple nn is: 0.4840107262134552\n",
            "loss on batch 33 in epoch 87 for the simple nn is: 0.45079830288887024\n",
            "loss on batch 34 in epoch 87 for the simple nn is: 0.42921388149261475\n",
            "loss on batch 35 in epoch 87 for the simple nn is: 0.4756219685077667\n",
            "loss on batch 36 in epoch 87 for the simple nn is: 0.4901653230190277\n",
            "loss on batch 37 in epoch 87 for the simple nn is: 0.4332415461540222\n",
            "loss on batch 38 in epoch 87 for the simple nn is: 0.6141030192375183\n",
            "loss on batch 39 in epoch 87 for the simple nn is: 0.40414413809776306\n",
            "loss on batch 40 in epoch 87 for the simple nn is: 0.4591786861419678\n",
            "loss on batch 41 in epoch 87 for the simple nn is: 0.42065614461898804\n",
            "loss on batch 42 in epoch 87 for the simple nn is: 0.4540598392486572\n",
            "loss on batch 43 in epoch 87 for the simple nn is: 0.5061420798301697\n",
            "loss on batch 44 in epoch 87 for the simple nn is: 0.4907209873199463\n",
            "loss on batch 45 in epoch 87 for the simple nn is: 0.44842007756233215\n",
            "loss on batch 46 in epoch 87 for the simple nn is: 0.45361408591270447\n",
            "loss on batch 47 in epoch 87 for the simple nn is: 0.5073028206825256\n",
            "loss on batch 48 in epoch 87 for the simple nn is: 0.4638668894767761\n",
            "loss on batch 49 in epoch 87 for the simple nn is: 0.5166729688644409\n",
            "loss on batch 50 in epoch 87 for the simple nn is: 0.4348130226135254\n",
            "loss on batch 51 in epoch 87 for the simple nn is: 0.47384852170944214\n",
            "loss on batch 52 in epoch 87 for the simple nn is: 0.3890925943851471\n",
            "loss on batch 53 in epoch 87 for the simple nn is: 0.34359192848205566\n",
            "loss on batch 54 in epoch 87 for the simple nn is: 0.5199845433235168\n",
            "loss on batch 55 in epoch 87 for the simple nn is: 0.45369723439216614\n",
            "loss on batch 56 in epoch 87 for the simple nn is: 0.46678614616394043\n",
            "loss on batch 57 in epoch 87 for the simple nn is: 0.4962713420391083\n",
            "loss on batch 58 in epoch 87 for the simple nn is: 0.4804520010948181\n",
            "loss on batch 59 in epoch 87 for the simple nn is: 0.4532734155654907\n",
            "loss on batch 60 in epoch 87 for the simple nn is: 0.4875011444091797\n",
            "loss on batch 61 in epoch 87 for the simple nn is: 0.4358968734741211\n",
            "loss on batch 62 in epoch 87 for the simple nn is: 0.45361223816871643\n",
            "loss on batch 63 in epoch 87 for the simple nn is: 0.47027847170829773\n",
            "loss on batch 64 in epoch 87 for the simple nn is: 0.4045056402683258\n",
            "loss on batch 65 in epoch 87 for the simple nn is: 0.3938082754611969\n",
            "loss on batch 66 in epoch 87 for the simple nn is: 0.45015597343444824\n",
            "loss on batch 67 in epoch 87 for the simple nn is: 0.3876011073589325\n",
            "loss on batch 68 in epoch 87 for the simple nn is: 0.381985604763031\n",
            "loss on batch 69 in epoch 87 for the simple nn is: 0.450967937707901\n",
            "loss on batch 70 in epoch 87 for the simple nn is: 0.44533872604370117\n",
            "loss on batch 71 in epoch 87 for the simple nn is: 0.46419757604599\n",
            "loss on batch 72 in epoch 87 for the simple nn is: 0.448984295129776\n",
            "loss on batch 73 in epoch 87 for the simple nn is: 0.49670684337615967\n",
            "loss on batch 74 in epoch 87 for the simple nn is: 0.4696663022041321\n",
            "loss on batch 75 in epoch 87 for the simple nn is: 0.46864068508148193\n",
            "loss on batch 76 in epoch 87 for the simple nn is: 0.6324196457862854\n",
            "loss on batch 77 in epoch 87 for the simple nn is: 0.4333694577217102\n",
            "loss on batch 78 in epoch 87 for the simple nn is: 0.47133567929267883\n",
            "loss on batch 79 in epoch 87 for the simple nn is: 0.3877317011356354\n",
            "loss on batch 80 in epoch 87 for the simple nn is: 0.43587228655815125\n",
            "loss on batch 81 in epoch 87 for the simple nn is: 0.39601826667785645\n",
            "loss on batch 82 in epoch 87 for the simple nn is: 0.5223162174224854\n",
            "loss on batch 83 in epoch 87 for the simple nn is: 0.5030297636985779\n",
            "loss on batch 84 in epoch 87 for the simple nn is: 0.47000566124916077\n",
            "loss on batch 85 in epoch 87 for the simple nn is: 0.491111695766449\n",
            "loss on batch 86 in epoch 87 for the simple nn is: 0.37455055117607117\n",
            "loss on batch 87 in epoch 87 for the simple nn is: 0.4849112629890442\n",
            "loss on batch 88 in epoch 87 for the simple nn is: 0.3715020716190338\n",
            "loss on batch 89 in epoch 87 for the simple nn is: 0.48506230115890503\n",
            "loss on batch 90 in epoch 87 for the simple nn is: 0.42478564381599426\n",
            "loss on batch 91 in epoch 87 for the simple nn is: 0.4737285375595093\n",
            "loss on batch 92 in epoch 87 for the simple nn is: 0.5123688578605652\n",
            "loss on batch 93 in epoch 87 for the simple nn is: 0.4527243971824646\n",
            "loss on batch 94 in epoch 87 for the simple nn is: 0.38141995668411255\n",
            "loss on batch 95 in epoch 87 for the simple nn is: 0.5733813047409058\n",
            "loss on batch 96 in epoch 87 for the simple nn is: 0.41288986802101135\n",
            "loss on batch 97 in epoch 87 for the simple nn is: 0.47162488102912903\n",
            "loss on batch 98 in epoch 87 for the simple nn is: 0.46562591195106506\n",
            "loss on batch 99 in epoch 87 for the simple nn is: 0.45392146706581116\n",
            "loss on batch 100 in epoch 87 for the simple nn is: 0.44779449701309204\n",
            "loss on batch 101 in epoch 87 for the simple nn is: 0.5042282342910767\n",
            "loss on batch 102 in epoch 87 for the simple nn is: 0.3778974115848541\n",
            "loss on batch 103 in epoch 87 for the simple nn is: 0.47269636392593384\n",
            "loss on batch 104 in epoch 87 for the simple nn is: 0.42411836981773376\n",
            "loss on batch 105 in epoch 87 for the simple nn is: 0.41106441617012024\n",
            "loss on batch 106 in epoch 87 for the simple nn is: 0.5861218571662903\n",
            "loss on batch 107 in epoch 87 for the simple nn is: 0.42305436730384827\n",
            "loss on batch 108 in epoch 87 for the simple nn is: 0.40984052419662476\n",
            "loss on batch 109 in epoch 87 for the simple nn is: 0.6006247401237488\n",
            "loss on batch 110 in epoch 87 for the simple nn is: 0.35479122400283813\n",
            "loss on batch 111 in epoch 87 for the simple nn is: 0.4905906021595001\n",
            "loss on batch 112 in epoch 87 for the simple nn is: 0.4605192542076111\n",
            "loss on batch 113 in epoch 87 for the simple nn is: 0.49089494347572327\n",
            "loss on batch 114 in epoch 87 for the simple nn is: 0.5948989391326904\n",
            "loss on batch 115 in epoch 87 for the simple nn is: 0.4359436333179474\n",
            "loss on batch 116 in epoch 87 for the simple nn is: 0.5520467758178711\n",
            "loss on batch 117 in epoch 87 for the simple nn is: 0.5368370413780212\n",
            "loss on batch 118 in epoch 87 for the simple nn is: 0.5591811537742615\n",
            "loss on batch 119 in epoch 87 for the simple nn is: 0.505963146686554\n",
            "loss on batch 120 in epoch 87 for the simple nn is: 0.49168580770492554\n",
            "loss on batch 0 in epoch 88 for the simple nn is: 0.4966953992843628\n",
            "loss on batch 1 in epoch 88 for the simple nn is: 0.6062168478965759\n",
            "loss on batch 2 in epoch 88 for the simple nn is: 0.5705552697181702\n",
            "loss on batch 3 in epoch 88 for the simple nn is: 0.5385399460792542\n",
            "loss on batch 4 in epoch 88 for the simple nn is: 0.5365681648254395\n",
            "loss on batch 5 in epoch 88 for the simple nn is: 0.5164353251457214\n",
            "loss on batch 6 in epoch 88 for the simple nn is: 0.558387041091919\n",
            "loss on batch 7 in epoch 88 for the simple nn is: 0.46966639161109924\n",
            "loss on batch 8 in epoch 88 for the simple nn is: 0.4820428192615509\n",
            "loss on batch 9 in epoch 88 for the simple nn is: 0.5000998377799988\n",
            "loss on batch 10 in epoch 88 for the simple nn is: 0.6090378761291504\n",
            "loss on batch 11 in epoch 88 for the simple nn is: 0.5140681266784668\n",
            "loss on batch 12 in epoch 88 for the simple nn is: 0.6274743676185608\n",
            "loss on batch 13 in epoch 88 for the simple nn is: 0.51084965467453\n",
            "loss on batch 14 in epoch 88 for the simple nn is: 0.4742276966571808\n",
            "loss on batch 15 in epoch 88 for the simple nn is: 0.4770995080471039\n",
            "loss on batch 16 in epoch 88 for the simple nn is: 0.6038440465927124\n",
            "loss on batch 17 in epoch 88 for the simple nn is: 0.48916202783584595\n",
            "loss on batch 18 in epoch 88 for the simple nn is: 0.46962711215019226\n",
            "loss on batch 19 in epoch 88 for the simple nn is: 0.48164206743240356\n",
            "loss on batch 20 in epoch 88 for the simple nn is: 0.4783480167388916\n",
            "loss on batch 21 in epoch 88 for the simple nn is: 0.5138382315635681\n",
            "loss on batch 22 in epoch 88 for the simple nn is: 0.44651055335998535\n",
            "loss on batch 23 in epoch 88 for the simple nn is: 0.4399644136428833\n",
            "loss on batch 24 in epoch 88 for the simple nn is: 0.3910229504108429\n",
            "loss on batch 25 in epoch 88 for the simple nn is: 0.45036014914512634\n",
            "loss on batch 26 in epoch 88 for the simple nn is: 0.48533937335014343\n",
            "loss on batch 27 in epoch 88 for the simple nn is: 0.5123595595359802\n",
            "loss on batch 28 in epoch 88 for the simple nn is: 0.487011581659317\n",
            "loss on batch 29 in epoch 88 for the simple nn is: 0.5478535294532776\n",
            "loss on batch 30 in epoch 88 for the simple nn is: 0.474638432264328\n",
            "loss on batch 31 in epoch 88 for the simple nn is: 0.5191285014152527\n",
            "loss on batch 32 in epoch 88 for the simple nn is: 0.5249301195144653\n",
            "loss on batch 33 in epoch 88 for the simple nn is: 0.4580056965351105\n",
            "loss on batch 34 in epoch 88 for the simple nn is: 0.4567238390445709\n",
            "loss on batch 35 in epoch 88 for the simple nn is: 0.498885840177536\n",
            "loss on batch 36 in epoch 88 for the simple nn is: 0.4839165210723877\n",
            "loss on batch 37 in epoch 88 for the simple nn is: 0.4700184166431427\n",
            "loss on batch 38 in epoch 88 for the simple nn is: 0.5152074694633484\n",
            "loss on batch 39 in epoch 88 for the simple nn is: 0.5469048619270325\n",
            "loss on batch 40 in epoch 88 for the simple nn is: 0.46069541573524475\n",
            "loss on batch 41 in epoch 88 for the simple nn is: 0.43363264203071594\n",
            "loss on batch 42 in epoch 88 for the simple nn is: 0.471998393535614\n",
            "loss on batch 43 in epoch 88 for the simple nn is: 0.4997551143169403\n",
            "loss on batch 44 in epoch 88 for the simple nn is: 0.5054125189781189\n",
            "loss on batch 45 in epoch 88 for the simple nn is: 0.44816598296165466\n",
            "loss on batch 46 in epoch 88 for the simple nn is: 0.4289796054363251\n",
            "loss on batch 47 in epoch 88 for the simple nn is: 0.43027427792549133\n",
            "loss on batch 48 in epoch 88 for the simple nn is: 0.6621556878089905\n",
            "loss on batch 49 in epoch 88 for the simple nn is: 0.5272176861763\n",
            "loss on batch 50 in epoch 88 for the simple nn is: 0.4765859842300415\n",
            "loss on batch 51 in epoch 88 for the simple nn is: 0.4827420115470886\n",
            "loss on batch 52 in epoch 88 for the simple nn is: 0.4159896969795227\n",
            "loss on batch 53 in epoch 88 for the simple nn is: 0.4469956159591675\n",
            "loss on batch 54 in epoch 88 for the simple nn is: 0.5349591374397278\n",
            "loss on batch 55 in epoch 88 for the simple nn is: 0.4983595013618469\n",
            "loss on batch 56 in epoch 88 for the simple nn is: 0.490985631942749\n",
            "loss on batch 57 in epoch 88 for the simple nn is: 0.520327627658844\n",
            "loss on batch 58 in epoch 88 for the simple nn is: 0.4453218877315521\n",
            "loss on batch 59 in epoch 88 for the simple nn is: 0.47217991948127747\n",
            "loss on batch 60 in epoch 88 for the simple nn is: 0.4492458403110504\n",
            "loss on batch 61 in epoch 88 for the simple nn is: 0.44877180457115173\n",
            "loss on batch 62 in epoch 88 for the simple nn is: 0.45581209659576416\n",
            "loss on batch 63 in epoch 88 for the simple nn is: 0.47366148233413696\n",
            "loss on batch 64 in epoch 88 for the simple nn is: 0.4900766611099243\n",
            "loss on batch 65 in epoch 88 for the simple nn is: 0.4018188416957855\n",
            "loss on batch 66 in epoch 88 for the simple nn is: 0.45464321970939636\n",
            "loss on batch 67 in epoch 88 for the simple nn is: 0.389262318611145\n",
            "loss on batch 68 in epoch 88 for the simple nn is: 0.6760770082473755\n",
            "loss on batch 69 in epoch 88 for the simple nn is: 0.45207419991493225\n",
            "loss on batch 70 in epoch 88 for the simple nn is: 0.5718873143196106\n",
            "loss on batch 71 in epoch 88 for the simple nn is: 0.4875771403312683\n",
            "loss on batch 72 in epoch 88 for the simple nn is: 0.4541877508163452\n",
            "loss on batch 73 in epoch 88 for the simple nn is: 0.5960404872894287\n",
            "loss on batch 74 in epoch 88 for the simple nn is: 0.47424793243408203\n",
            "loss on batch 75 in epoch 88 for the simple nn is: 0.4789256155490875\n",
            "loss on batch 76 in epoch 88 for the simple nn is: 0.4817884564399719\n",
            "loss on batch 77 in epoch 88 for the simple nn is: 0.45992225408554077\n",
            "loss on batch 78 in epoch 88 for the simple nn is: 0.5090286135673523\n",
            "loss on batch 79 in epoch 88 for the simple nn is: 0.40976494550704956\n",
            "loss on batch 80 in epoch 88 for the simple nn is: 0.46408119797706604\n",
            "loss on batch 81 in epoch 88 for the simple nn is: 0.47233396768569946\n",
            "loss on batch 82 in epoch 88 for the simple nn is: 0.4472165107727051\n",
            "loss on batch 83 in epoch 88 for the simple nn is: 0.481998085975647\n",
            "loss on batch 84 in epoch 88 for the simple nn is: 0.4835934340953827\n",
            "loss on batch 85 in epoch 88 for the simple nn is: 0.5047823786735535\n",
            "loss on batch 86 in epoch 88 for the simple nn is: 0.5136149525642395\n",
            "loss on batch 87 in epoch 88 for the simple nn is: 0.5818666219711304\n",
            "loss on batch 88 in epoch 88 for the simple nn is: 0.3698606789112091\n",
            "loss on batch 89 in epoch 88 for the simple nn is: 0.6508146524429321\n",
            "loss on batch 90 in epoch 88 for the simple nn is: 0.48424234986305237\n",
            "loss on batch 91 in epoch 88 for the simple nn is: 0.44526201486587524\n",
            "loss on batch 92 in epoch 88 for the simple nn is: 0.6330540180206299\n",
            "loss on batch 93 in epoch 88 for the simple nn is: 0.45055440068244934\n",
            "loss on batch 94 in epoch 88 for the simple nn is: 0.4009651839733124\n",
            "loss on batch 95 in epoch 88 for the simple nn is: 0.4972037672996521\n",
            "loss on batch 96 in epoch 88 for the simple nn is: 0.45748430490493774\n",
            "loss on batch 97 in epoch 88 for the simple nn is: 0.4712662398815155\n",
            "loss on batch 98 in epoch 88 for the simple nn is: 0.45278623700141907\n",
            "loss on batch 99 in epoch 88 for the simple nn is: 0.4470134377479553\n",
            "loss on batch 100 in epoch 88 for the simple nn is: 0.49849677085876465\n",
            "loss on batch 101 in epoch 88 for the simple nn is: 0.41191229224205017\n",
            "loss on batch 102 in epoch 88 for the simple nn is: 0.5662998557090759\n",
            "loss on batch 103 in epoch 88 for the simple nn is: 0.5265809893608093\n",
            "loss on batch 104 in epoch 88 for the simple nn is: 0.45374396443367004\n",
            "loss on batch 105 in epoch 88 for the simple nn is: 0.4282894432544708\n",
            "loss on batch 106 in epoch 88 for the simple nn is: 0.5199146270751953\n",
            "loss on batch 107 in epoch 88 for the simple nn is: 0.4385673701763153\n",
            "loss on batch 108 in epoch 88 for the simple nn is: 0.48057040572166443\n",
            "loss on batch 109 in epoch 88 for the simple nn is: 0.5186346173286438\n",
            "loss on batch 110 in epoch 88 for the simple nn is: 0.3574373722076416\n",
            "loss on batch 111 in epoch 88 for the simple nn is: 0.429110586643219\n",
            "loss on batch 112 in epoch 88 for the simple nn is: 0.551656186580658\n",
            "loss on batch 113 in epoch 88 for the simple nn is: 0.5232033133506775\n",
            "loss on batch 114 in epoch 88 for the simple nn is: 0.5233705043792725\n",
            "loss on batch 115 in epoch 88 for the simple nn is: 0.44551631808280945\n",
            "loss on batch 116 in epoch 88 for the simple nn is: 0.5435126423835754\n",
            "loss on batch 117 in epoch 88 for the simple nn is: 0.5170401930809021\n",
            "loss on batch 118 in epoch 88 for the simple nn is: 0.5466006398200989\n",
            "loss on batch 119 in epoch 88 for the simple nn is: 0.5223316550254822\n",
            "loss on batch 120 in epoch 88 for the simple nn is: 0.5072072148323059\n",
            "loss on batch 0 in epoch 89 for the simple nn is: 0.4880586862564087\n",
            "loss on batch 1 in epoch 89 for the simple nn is: 0.5942733883857727\n",
            "loss on batch 2 in epoch 89 for the simple nn is: 0.5492966771125793\n",
            "loss on batch 3 in epoch 89 for the simple nn is: 0.5449187755584717\n",
            "loss on batch 4 in epoch 89 for the simple nn is: 0.5464037656784058\n",
            "loss on batch 5 in epoch 89 for the simple nn is: 0.5290501713752747\n",
            "loss on batch 6 in epoch 89 for the simple nn is: 0.5508182644844055\n",
            "loss on batch 7 in epoch 89 for the simple nn is: 0.4895663559436798\n",
            "loss on batch 8 in epoch 89 for the simple nn is: 0.47751522064208984\n",
            "loss on batch 9 in epoch 89 for the simple nn is: 0.48017576336860657\n",
            "loss on batch 10 in epoch 89 for the simple nn is: 0.44499069452285767\n",
            "loss on batch 11 in epoch 89 for the simple nn is: 0.4922274947166443\n",
            "loss on batch 12 in epoch 89 for the simple nn is: 0.5273983478546143\n",
            "loss on batch 13 in epoch 89 for the simple nn is: 0.4789399206638336\n",
            "loss on batch 14 in epoch 89 for the simple nn is: 0.4834710359573364\n",
            "loss on batch 15 in epoch 89 for the simple nn is: 0.4325186312198639\n",
            "loss on batch 16 in epoch 89 for the simple nn is: 0.4844600558280945\n",
            "loss on batch 17 in epoch 89 for the simple nn is: 0.4722695052623749\n",
            "loss on batch 18 in epoch 89 for the simple nn is: 0.46434009075164795\n",
            "loss on batch 19 in epoch 89 for the simple nn is: 0.48026788234710693\n",
            "loss on batch 20 in epoch 89 for the simple nn is: 0.43029311299324036\n",
            "loss on batch 21 in epoch 89 for the simple nn is: 0.5050023794174194\n",
            "loss on batch 22 in epoch 89 for the simple nn is: 0.4441443979740143\n",
            "loss on batch 23 in epoch 89 for the simple nn is: 0.42653995752334595\n",
            "loss on batch 24 in epoch 89 for the simple nn is: 0.3657466769218445\n",
            "loss on batch 25 in epoch 89 for the simple nn is: 0.5381156206130981\n",
            "loss on batch 26 in epoch 89 for the simple nn is: 0.4918145537376404\n",
            "loss on batch 27 in epoch 89 for the simple nn is: 0.4463740885257721\n",
            "loss on batch 28 in epoch 89 for the simple nn is: 0.4559479355812073\n",
            "loss on batch 29 in epoch 89 for the simple nn is: 0.5488660931587219\n",
            "loss on batch 30 in epoch 89 for the simple nn is: 0.4611150622367859\n",
            "loss on batch 31 in epoch 89 for the simple nn is: 0.5066972970962524\n",
            "loss on batch 32 in epoch 89 for the simple nn is: 0.42998844385147095\n",
            "loss on batch 33 in epoch 89 for the simple nn is: 0.4486871659755707\n",
            "loss on batch 34 in epoch 89 for the simple nn is: 0.5253769159317017\n",
            "loss on batch 35 in epoch 89 for the simple nn is: 0.474651962518692\n",
            "loss on batch 36 in epoch 89 for the simple nn is: 0.4617726802825928\n",
            "loss on batch 37 in epoch 89 for the simple nn is: 0.4399172067642212\n",
            "loss on batch 38 in epoch 89 for the simple nn is: 0.4992300570011139\n",
            "loss on batch 39 in epoch 89 for the simple nn is: 0.4627950191497803\n",
            "loss on batch 40 in epoch 89 for the simple nn is: 0.45830804109573364\n",
            "loss on batch 41 in epoch 89 for the simple nn is: 0.4276287257671356\n",
            "loss on batch 42 in epoch 89 for the simple nn is: 0.4579065442085266\n",
            "loss on batch 43 in epoch 89 for the simple nn is: 0.4718891382217407\n",
            "loss on batch 44 in epoch 89 for the simple nn is: 0.5268615484237671\n",
            "loss on batch 45 in epoch 89 for the simple nn is: 0.4451565444469452\n",
            "loss on batch 46 in epoch 89 for the simple nn is: 0.43103349208831787\n",
            "loss on batch 47 in epoch 89 for the simple nn is: 0.41190433502197266\n",
            "loss on batch 48 in epoch 89 for the simple nn is: 0.46531468629837036\n",
            "loss on batch 49 in epoch 89 for the simple nn is: 0.5271356105804443\n",
            "loss on batch 50 in epoch 89 for the simple nn is: 0.4558803141117096\n",
            "loss on batch 51 in epoch 89 for the simple nn is: 0.4733726382255554\n",
            "loss on batch 52 in epoch 89 for the simple nn is: 0.36707571148872375\n",
            "loss on batch 53 in epoch 89 for the simple nn is: 0.5778826475143433\n",
            "loss on batch 54 in epoch 89 for the simple nn is: 0.5129495859146118\n",
            "loss on batch 55 in epoch 89 for the simple nn is: 0.45362377166748047\n",
            "loss on batch 56 in epoch 89 for the simple nn is: 0.46363896131515503\n",
            "loss on batch 57 in epoch 89 for the simple nn is: 0.5307884812355042\n",
            "loss on batch 58 in epoch 89 for the simple nn is: 0.43186303973197937\n",
            "loss on batch 59 in epoch 89 for the simple nn is: 0.47066640853881836\n",
            "loss on batch 60 in epoch 89 for the simple nn is: 0.4380899965763092\n",
            "loss on batch 61 in epoch 89 for the simple nn is: 0.44699281454086304\n",
            "loss on batch 62 in epoch 89 for the simple nn is: 0.4514677822589874\n",
            "loss on batch 63 in epoch 89 for the simple nn is: 0.45464521646499634\n",
            "loss on batch 64 in epoch 89 for the simple nn is: 0.4070233404636383\n",
            "loss on batch 65 in epoch 89 for the simple nn is: 0.37986207008361816\n",
            "loss on batch 66 in epoch 89 for the simple nn is: 0.45155441761016846\n",
            "loss on batch 67 in epoch 89 for the simple nn is: 0.38685542345046997\n",
            "loss on batch 68 in epoch 89 for the simple nn is: 0.38794419169425964\n",
            "loss on batch 69 in epoch 89 for the simple nn is: 0.43993112444877625\n",
            "loss on batch 70 in epoch 89 for the simple nn is: 0.4595222473144531\n",
            "loss on batch 71 in epoch 89 for the simple nn is: 0.5572988986968994\n",
            "loss on batch 72 in epoch 89 for the simple nn is: 0.45307329297065735\n",
            "loss on batch 73 in epoch 89 for the simple nn is: 0.5060097575187683\n",
            "loss on batch 74 in epoch 89 for the simple nn is: 0.45238035917282104\n",
            "loss on batch 75 in epoch 89 for the simple nn is: 0.4642268717288971\n",
            "loss on batch 76 in epoch 89 for the simple nn is: 0.44469842314720154\n",
            "loss on batch 77 in epoch 89 for the simple nn is: 0.42755091190338135\n",
            "loss on batch 78 in epoch 89 for the simple nn is: 0.44993361830711365\n",
            "loss on batch 79 in epoch 89 for the simple nn is: 0.38076281547546387\n",
            "loss on batch 80 in epoch 89 for the simple nn is: 0.43505337834358215\n",
            "loss on batch 81 in epoch 89 for the simple nn is: 0.37874820828437805\n",
            "loss on batch 82 in epoch 89 for the simple nn is: 0.4961741268634796\n",
            "loss on batch 83 in epoch 89 for the simple nn is: 0.46818214654922485\n",
            "loss on batch 84 in epoch 89 for the simple nn is: 0.526618242263794\n",
            "loss on batch 85 in epoch 89 for the simple nn is: 0.4797562062740326\n",
            "loss on batch 86 in epoch 89 for the simple nn is: 0.3879657983779907\n",
            "loss on batch 87 in epoch 89 for the simple nn is: 0.4426695704460144\n",
            "loss on batch 88 in epoch 89 for the simple nn is: 0.3427160680294037\n",
            "loss on batch 89 in epoch 89 for the simple nn is: 0.4723307192325592\n",
            "loss on batch 90 in epoch 89 for the simple nn is: 0.4233274459838867\n",
            "loss on batch 91 in epoch 89 for the simple nn is: 0.4700303077697754\n",
            "loss on batch 92 in epoch 89 for the simple nn is: 0.5049864649772644\n",
            "loss on batch 93 in epoch 89 for the simple nn is: 0.4296662509441376\n",
            "loss on batch 94 in epoch 89 for the simple nn is: 0.3711456060409546\n",
            "loss on batch 95 in epoch 89 for the simple nn is: 0.4956294000148773\n",
            "loss on batch 96 in epoch 89 for the simple nn is: 0.4056590795516968\n",
            "loss on batch 97 in epoch 89 for the simple nn is: 0.47501423954963684\n",
            "loss on batch 98 in epoch 89 for the simple nn is: 0.4351764917373657\n",
            "loss on batch 99 in epoch 89 for the simple nn is: 0.4105377495288849\n",
            "loss on batch 100 in epoch 89 for the simple nn is: 0.4527711272239685\n",
            "loss on batch 101 in epoch 89 for the simple nn is: 0.4016314148902893\n",
            "loss on batch 102 in epoch 89 for the simple nn is: 0.35068008303642273\n",
            "loss on batch 103 in epoch 89 for the simple nn is: 0.4734627604484558\n",
            "loss on batch 104 in epoch 89 for the simple nn is: 0.3971790075302124\n",
            "loss on batch 105 in epoch 89 for the simple nn is: 0.4326425790786743\n",
            "loss on batch 106 in epoch 89 for the simple nn is: 0.47930750250816345\n",
            "loss on batch 107 in epoch 89 for the simple nn is: 0.5867090225219727\n",
            "loss on batch 108 in epoch 89 for the simple nn is: 0.4299278259277344\n",
            "loss on batch 109 in epoch 89 for the simple nn is: 0.5117395520210266\n",
            "loss on batch 110 in epoch 89 for the simple nn is: 0.34597986936569214\n",
            "loss on batch 111 in epoch 89 for the simple nn is: 0.4253048002719879\n",
            "loss on batch 112 in epoch 89 for the simple nn is: 0.467649906873703\n",
            "loss on batch 113 in epoch 89 for the simple nn is: 0.4986061751842499\n",
            "loss on batch 114 in epoch 89 for the simple nn is: 0.5107894539833069\n",
            "loss on batch 115 in epoch 89 for the simple nn is: 0.4937216639518738\n",
            "loss on batch 116 in epoch 89 for the simple nn is: 0.5537441968917847\n",
            "loss on batch 117 in epoch 89 for the simple nn is: 0.5236361622810364\n",
            "loss on batch 118 in epoch 89 for the simple nn is: 0.5437489748001099\n",
            "loss on batch 119 in epoch 89 for the simple nn is: 0.5185356736183167\n",
            "loss on batch 120 in epoch 89 for the simple nn is: 0.48280760645866394\n",
            "loss on batch 0 in epoch 90 for the simple nn is: 0.5753482580184937\n",
            "loss on batch 1 in epoch 90 for the simple nn is: 0.5929886698722839\n",
            "loss on batch 2 in epoch 90 for the simple nn is: 0.5476921200752258\n",
            "loss on batch 3 in epoch 90 for the simple nn is: 0.5731900930404663\n",
            "loss on batch 4 in epoch 90 for the simple nn is: 0.5758676528930664\n",
            "loss on batch 5 in epoch 90 for the simple nn is: 0.6255382895469666\n",
            "loss on batch 6 in epoch 90 for the simple nn is: 0.5329709053039551\n",
            "loss on batch 7 in epoch 90 for the simple nn is: 0.48186221718788147\n",
            "loss on batch 8 in epoch 90 for the simple nn is: 0.4492018520832062\n",
            "loss on batch 9 in epoch 90 for the simple nn is: 0.4784986078739166\n",
            "loss on batch 10 in epoch 90 for the simple nn is: 0.4219416081905365\n",
            "loss on batch 11 in epoch 90 for the simple nn is: 0.5230866074562073\n",
            "loss on batch 12 in epoch 90 for the simple nn is: 0.520993173122406\n",
            "loss on batch 13 in epoch 90 for the simple nn is: 0.4981851279735565\n",
            "loss on batch 14 in epoch 90 for the simple nn is: 0.4829249083995819\n",
            "loss on batch 15 in epoch 90 for the simple nn is: 0.4579857289791107\n",
            "loss on batch 16 in epoch 90 for the simple nn is: 0.5931779742240906\n",
            "loss on batch 17 in epoch 90 for the simple nn is: 0.4764454960823059\n",
            "loss on batch 18 in epoch 90 for the simple nn is: 0.45314735174179077\n",
            "loss on batch 19 in epoch 90 for the simple nn is: 0.47302547097206116\n",
            "loss on batch 20 in epoch 90 for the simple nn is: 0.43678417801856995\n",
            "loss on batch 21 in epoch 90 for the simple nn is: 0.5248441696166992\n",
            "loss on batch 22 in epoch 90 for the simple nn is: 0.49269160628318787\n",
            "loss on batch 23 in epoch 90 for the simple nn is: 0.4519813358783722\n",
            "loss on batch 24 in epoch 90 for the simple nn is: 0.3915289044380188\n",
            "loss on batch 25 in epoch 90 for the simple nn is: 0.43181201815605164\n",
            "loss on batch 26 in epoch 90 for the simple nn is: 0.5110254287719727\n",
            "loss on batch 27 in epoch 90 for the simple nn is: 0.4677261412143707\n",
            "loss on batch 28 in epoch 90 for the simple nn is: 0.48533645272254944\n",
            "loss on batch 29 in epoch 90 for the simple nn is: 0.5962943434715271\n",
            "loss on batch 30 in epoch 90 for the simple nn is: 0.5678846836090088\n",
            "loss on batch 31 in epoch 90 for the simple nn is: 0.5580831170082092\n",
            "loss on batch 32 in epoch 90 for the simple nn is: 0.48460808396339417\n",
            "loss on batch 33 in epoch 90 for the simple nn is: 0.5212512612342834\n",
            "loss on batch 34 in epoch 90 for the simple nn is: 0.4688226282596588\n",
            "loss on batch 35 in epoch 90 for the simple nn is: 0.4625152349472046\n",
            "loss on batch 36 in epoch 90 for the simple nn is: 0.6217933297157288\n",
            "loss on batch 37 in epoch 90 for the simple nn is: 0.4549665153026581\n",
            "loss on batch 38 in epoch 90 for the simple nn is: 0.5109964609146118\n",
            "loss on batch 39 in epoch 90 for the simple nn is: 0.37880825996398926\n",
            "loss on batch 40 in epoch 90 for the simple nn is: 0.46300187706947327\n",
            "loss on batch 41 in epoch 90 for the simple nn is: 0.44838494062423706\n",
            "loss on batch 42 in epoch 90 for the simple nn is: 0.4937751591205597\n",
            "loss on batch 43 in epoch 90 for the simple nn is: 0.4886312186717987\n",
            "loss on batch 44 in epoch 90 for the simple nn is: 0.5201737880706787\n",
            "loss on batch 45 in epoch 90 for the simple nn is: 0.4474177956581116\n",
            "loss on batch 46 in epoch 90 for the simple nn is: 0.43793296813964844\n",
            "loss on batch 47 in epoch 90 for the simple nn is: 0.5126884579658508\n",
            "loss on batch 48 in epoch 90 for the simple nn is: 0.5426796078681946\n",
            "loss on batch 49 in epoch 90 for the simple nn is: 0.5830271244049072\n",
            "loss on batch 50 in epoch 90 for the simple nn is: 0.44088488817214966\n",
            "loss on batch 51 in epoch 90 for the simple nn is: 0.45225775241851807\n",
            "loss on batch 52 in epoch 90 for the simple nn is: 0.381161093711853\n",
            "loss on batch 53 in epoch 90 for the simple nn is: 0.3425620496273041\n",
            "loss on batch 54 in epoch 90 for the simple nn is: 0.5134597420692444\n",
            "loss on batch 55 in epoch 90 for the simple nn is: 0.4701854884624481\n",
            "loss on batch 56 in epoch 90 for the simple nn is: 0.4847296476364136\n",
            "loss on batch 57 in epoch 90 for the simple nn is: 0.5036916136741638\n",
            "loss on batch 58 in epoch 90 for the simple nn is: 0.4318954050540924\n",
            "loss on batch 59 in epoch 90 for the simple nn is: 0.4938139319419861\n",
            "loss on batch 60 in epoch 90 for the simple nn is: 0.5238518118858337\n",
            "loss on batch 61 in epoch 90 for the simple nn is: 0.46320876479148865\n",
            "loss on batch 62 in epoch 90 for the simple nn is: 0.48801639676094055\n",
            "loss on batch 63 in epoch 90 for the simple nn is: 0.5170741677284241\n",
            "loss on batch 64 in epoch 90 for the simple nn is: 0.4558562934398651\n",
            "loss on batch 65 in epoch 90 for the simple nn is: 0.610553503036499\n",
            "loss on batch 66 in epoch 90 for the simple nn is: 0.46848955750465393\n",
            "loss on batch 67 in epoch 90 for the simple nn is: 0.39975103735923767\n",
            "loss on batch 68 in epoch 90 for the simple nn is: 0.418923020362854\n",
            "loss on batch 69 in epoch 90 for the simple nn is: 0.4414943754673004\n",
            "loss on batch 70 in epoch 90 for the simple nn is: 0.4436762034893036\n",
            "loss on batch 71 in epoch 90 for the simple nn is: 0.4909244477748871\n",
            "loss on batch 72 in epoch 90 for the simple nn is: 0.4629916548728943\n",
            "loss on batch 73 in epoch 90 for the simple nn is: 0.5136849880218506\n",
            "loss on batch 74 in epoch 90 for the simple nn is: 0.459771066904068\n",
            "loss on batch 75 in epoch 90 for the simple nn is: 0.6186789870262146\n",
            "loss on batch 76 in epoch 90 for the simple nn is: 0.4528234302997589\n",
            "loss on batch 77 in epoch 90 for the simple nn is: 0.40705084800720215\n",
            "loss on batch 78 in epoch 90 for the simple nn is: 0.5118900537490845\n",
            "loss on batch 79 in epoch 90 for the simple nn is: 0.4279499351978302\n",
            "loss on batch 80 in epoch 90 for the simple nn is: 0.4485584795475006\n",
            "loss on batch 81 in epoch 90 for the simple nn is: 0.3850742280483246\n",
            "loss on batch 82 in epoch 90 for the simple nn is: 0.4590628147125244\n",
            "loss on batch 83 in epoch 90 for the simple nn is: 0.5060392022132874\n",
            "loss on batch 84 in epoch 90 for the simple nn is: 0.45488306879997253\n",
            "loss on batch 85 in epoch 90 for the simple nn is: 0.4860747456550598\n",
            "loss on batch 86 in epoch 90 for the simple nn is: 0.48479413986206055\n",
            "loss on batch 87 in epoch 90 for the simple nn is: 0.4621516168117523\n",
            "loss on batch 88 in epoch 90 for the simple nn is: 0.3491843044757843\n",
            "loss on batch 89 in epoch 90 for the simple nn is: 0.49536508321762085\n",
            "loss on batch 90 in epoch 90 for the simple nn is: 0.42925336956977844\n",
            "loss on batch 91 in epoch 90 for the simple nn is: 0.43088993430137634\n",
            "loss on batch 92 in epoch 90 for the simple nn is: 0.5270165205001831\n",
            "loss on batch 93 in epoch 90 for the simple nn is: 0.4586423635482788\n",
            "loss on batch 94 in epoch 90 for the simple nn is: 0.46871235966682434\n",
            "loss on batch 95 in epoch 90 for the simple nn is: 0.5215317606925964\n",
            "loss on batch 96 in epoch 90 for the simple nn is: 0.43320995569229126\n",
            "loss on batch 97 in epoch 90 for the simple nn is: 0.49512267112731934\n",
            "loss on batch 98 in epoch 90 for the simple nn is: 0.4405173063278198\n",
            "loss on batch 99 in epoch 90 for the simple nn is: 0.412028044462204\n",
            "loss on batch 100 in epoch 90 for the simple nn is: 0.4358675181865692\n",
            "loss on batch 101 in epoch 90 for the simple nn is: 0.492571622133255\n",
            "loss on batch 102 in epoch 90 for the simple nn is: 0.3702974319458008\n",
            "loss on batch 103 in epoch 90 for the simple nn is: 0.6634579300880432\n",
            "loss on batch 104 in epoch 90 for the simple nn is: 0.39830338954925537\n",
            "loss on batch 105 in epoch 90 for the simple nn is: 0.4420054852962494\n",
            "loss on batch 106 in epoch 90 for the simple nn is: 0.4907918870449066\n",
            "loss on batch 107 in epoch 90 for the simple nn is: 0.47522681951522827\n",
            "loss on batch 108 in epoch 90 for the simple nn is: 0.4325026273727417\n",
            "loss on batch 109 in epoch 90 for the simple nn is: 0.5350273251533508\n",
            "loss on batch 110 in epoch 90 for the simple nn is: 0.3741406798362732\n",
            "loss on batch 111 in epoch 90 for the simple nn is: 0.4646376669406891\n",
            "loss on batch 112 in epoch 90 for the simple nn is: 0.47568753361701965\n",
            "loss on batch 113 in epoch 90 for the simple nn is: 0.4942969083786011\n",
            "loss on batch 114 in epoch 90 for the simple nn is: 0.7338455319404602\n",
            "loss on batch 115 in epoch 90 for the simple nn is: 0.43171000480651855\n",
            "loss on batch 116 in epoch 90 for the simple nn is: 0.6198511719703674\n",
            "loss on batch 117 in epoch 90 for the simple nn is: 0.5315446257591248\n",
            "loss on batch 118 in epoch 90 for the simple nn is: 0.5528011918067932\n",
            "loss on batch 119 in epoch 90 for the simple nn is: 0.5246275663375854\n",
            "loss on batch 120 in epoch 90 for the simple nn is: 0.525560200214386\n",
            "loss on batch 0 in epoch 91 for the simple nn is: 0.502511203289032\n",
            "loss on batch 1 in epoch 91 for the simple nn is: 0.5970249176025391\n",
            "loss on batch 2 in epoch 91 for the simple nn is: 0.5744805335998535\n",
            "loss on batch 3 in epoch 91 for the simple nn is: 0.5597280263900757\n",
            "loss on batch 4 in epoch 91 for the simple nn is: 0.5740038156509399\n",
            "loss on batch 5 in epoch 91 for the simple nn is: 0.5184827446937561\n",
            "loss on batch 6 in epoch 91 for the simple nn is: 0.5568467378616333\n",
            "loss on batch 7 in epoch 91 for the simple nn is: 0.509555995464325\n",
            "loss on batch 8 in epoch 91 for the simple nn is: 0.4685145616531372\n",
            "loss on batch 9 in epoch 91 for the simple nn is: 0.4798488914966583\n",
            "loss on batch 10 in epoch 91 for the simple nn is: 0.4362112283706665\n",
            "loss on batch 11 in epoch 91 for the simple nn is: 0.4953037202358246\n",
            "loss on batch 12 in epoch 91 for the simple nn is: 0.520564615726471\n",
            "loss on batch 13 in epoch 91 for the simple nn is: 0.5106103420257568\n",
            "loss on batch 14 in epoch 91 for the simple nn is: 0.4725456237792969\n",
            "loss on batch 15 in epoch 91 for the simple nn is: 0.44161614775657654\n",
            "loss on batch 16 in epoch 91 for the simple nn is: 0.49961549043655396\n",
            "loss on batch 17 in epoch 91 for the simple nn is: 0.48741644620895386\n",
            "loss on batch 18 in epoch 91 for the simple nn is: 0.4437123239040375\n",
            "loss on batch 19 in epoch 91 for the simple nn is: 0.5277328491210938\n",
            "loss on batch 20 in epoch 91 for the simple nn is: 0.4017239212989807\n",
            "loss on batch 21 in epoch 91 for the simple nn is: 0.49159783124923706\n",
            "loss on batch 22 in epoch 91 for the simple nn is: 0.43006569147109985\n",
            "loss on batch 23 in epoch 91 for the simple nn is: 0.42201969027519226\n",
            "loss on batch 24 in epoch 91 for the simple nn is: 0.379178911447525\n",
            "loss on batch 25 in epoch 91 for the simple nn is: 0.4493601620197296\n",
            "loss on batch 26 in epoch 91 for the simple nn is: 0.502204179763794\n",
            "loss on batch 27 in epoch 91 for the simple nn is: 0.45977216958999634\n",
            "loss on batch 28 in epoch 91 for the simple nn is: 0.476472407579422\n",
            "loss on batch 29 in epoch 91 for the simple nn is: 0.5800188779830933\n",
            "loss on batch 30 in epoch 91 for the simple nn is: 0.4699975252151489\n",
            "loss on batch 31 in epoch 91 for the simple nn is: 0.5109125971794128\n",
            "loss on batch 32 in epoch 91 for the simple nn is: 0.4338546097278595\n",
            "loss on batch 33 in epoch 91 for the simple nn is: 0.4586591422557831\n",
            "loss on batch 34 in epoch 91 for the simple nn is: 0.4587806463241577\n",
            "loss on batch 35 in epoch 91 for the simple nn is: 0.4607591927051544\n",
            "loss on batch 36 in epoch 91 for the simple nn is: 0.45125600695610046\n",
            "loss on batch 37 in epoch 91 for the simple nn is: 0.5102202296257019\n",
            "loss on batch 38 in epoch 91 for the simple nn is: 0.5009989738464355\n",
            "loss on batch 39 in epoch 91 for the simple nn is: 0.3958107531070709\n",
            "loss on batch 40 in epoch 91 for the simple nn is: 0.45922064781188965\n",
            "loss on batch 41 in epoch 91 for the simple nn is: 0.4557434916496277\n",
            "loss on batch 42 in epoch 91 for the simple nn is: 0.4633996784687042\n",
            "loss on batch 43 in epoch 91 for the simple nn is: 0.47250398993492126\n",
            "loss on batch 44 in epoch 91 for the simple nn is: 0.497293621301651\n",
            "loss on batch 45 in epoch 91 for the simple nn is: 0.45282483100891113\n",
            "loss on batch 46 in epoch 91 for the simple nn is: 0.4575370252132416\n",
            "loss on batch 47 in epoch 91 for the simple nn is: 0.42920050024986267\n",
            "loss on batch 48 in epoch 91 for the simple nn is: 0.4783455729484558\n",
            "loss on batch 49 in epoch 91 for the simple nn is: 0.5066893696784973\n",
            "loss on batch 50 in epoch 91 for the simple nn is: 0.4284002184867859\n",
            "loss on batch 51 in epoch 91 for the simple nn is: 0.4554111063480377\n",
            "loss on batch 52 in epoch 91 for the simple nn is: 0.3685276508331299\n",
            "loss on batch 53 in epoch 91 for the simple nn is: 0.36557239294052124\n",
            "loss on batch 54 in epoch 91 for the simple nn is: 0.49702414870262146\n",
            "loss on batch 55 in epoch 91 for the simple nn is: 0.4657423496246338\n",
            "loss on batch 56 in epoch 91 for the simple nn is: 0.47212791442871094\n",
            "loss on batch 57 in epoch 91 for the simple nn is: 0.4889943599700928\n",
            "loss on batch 58 in epoch 91 for the simple nn is: 0.4404296278953552\n",
            "loss on batch 59 in epoch 91 for the simple nn is: 0.4818187355995178\n",
            "loss on batch 60 in epoch 91 for the simple nn is: 0.45270222425460815\n",
            "loss on batch 61 in epoch 91 for the simple nn is: 0.46356332302093506\n",
            "loss on batch 62 in epoch 91 for the simple nn is: 0.4657805860042572\n",
            "loss on batch 63 in epoch 91 for the simple nn is: 0.45765671133995056\n",
            "loss on batch 64 in epoch 91 for the simple nn is: 0.3957770764827728\n",
            "loss on batch 65 in epoch 91 for the simple nn is: 0.39214783906936646\n",
            "loss on batch 66 in epoch 91 for the simple nn is: 0.4503962993621826\n",
            "loss on batch 67 in epoch 91 for the simple nn is: 0.3874107897281647\n",
            "loss on batch 68 in epoch 91 for the simple nn is: 0.38727816939353943\n",
            "loss on batch 69 in epoch 91 for the simple nn is: 0.43931493163108826\n",
            "loss on batch 70 in epoch 91 for the simple nn is: 0.44453442096710205\n",
            "loss on batch 71 in epoch 91 for the simple nn is: 0.4524104595184326\n",
            "loss on batch 72 in epoch 91 for the simple nn is: 0.4395495057106018\n",
            "loss on batch 73 in epoch 91 for the simple nn is: 0.4992847144603729\n",
            "loss on batch 74 in epoch 91 for the simple nn is: 0.4538043439388275\n",
            "loss on batch 75 in epoch 91 for the simple nn is: 0.6503745913505554\n",
            "loss on batch 76 in epoch 91 for the simple nn is: 0.4429115951061249\n",
            "loss on batch 77 in epoch 91 for the simple nn is: 0.40223804116249084\n",
            "loss on batch 78 in epoch 91 for the simple nn is: 0.4477527141571045\n",
            "loss on batch 79 in epoch 91 for the simple nn is: 0.3785974085330963\n",
            "loss on batch 80 in epoch 91 for the simple nn is: 0.466159850358963\n",
            "loss on batch 81 in epoch 91 for the simple nn is: 0.3703555166721344\n",
            "loss on batch 82 in epoch 91 for the simple nn is: 0.4314507246017456\n",
            "loss on batch 83 in epoch 91 for the simple nn is: 0.4632273018360138\n",
            "loss on batch 84 in epoch 91 for the simple nn is: 0.45237186551094055\n",
            "loss on batch 85 in epoch 91 for the simple nn is: 0.4668865501880646\n",
            "loss on batch 86 in epoch 91 for the simple nn is: 0.35969430208206177\n",
            "loss on batch 87 in epoch 91 for the simple nn is: 0.5223736763000488\n",
            "loss on batch 88 in epoch 91 for the simple nn is: 0.33096346259117126\n",
            "loss on batch 89 in epoch 91 for the simple nn is: 0.48153233528137207\n",
            "loss on batch 90 in epoch 91 for the simple nn is: 0.4343448281288147\n",
            "loss on batch 91 in epoch 91 for the simple nn is: 0.44386419653892517\n",
            "loss on batch 92 in epoch 91 for the simple nn is: 0.5177240967750549\n",
            "loss on batch 93 in epoch 91 for the simple nn is: 0.4425746500492096\n",
            "loss on batch 94 in epoch 91 for the simple nn is: 0.3712266683578491\n",
            "loss on batch 95 in epoch 91 for the simple nn is: 0.47754421830177307\n",
            "loss on batch 96 in epoch 91 for the simple nn is: 0.5064325332641602\n",
            "loss on batch 97 in epoch 91 for the simple nn is: 0.4716934561729431\n",
            "loss on batch 98 in epoch 91 for the simple nn is: 0.43521106243133545\n",
            "loss on batch 99 in epoch 91 for the simple nn is: 0.396300733089447\n",
            "loss on batch 100 in epoch 91 for the simple nn is: 0.5246289968490601\n",
            "loss on batch 101 in epoch 91 for the simple nn is: 0.4051308035850525\n",
            "loss on batch 102 in epoch 91 for the simple nn is: 0.3598853051662445\n",
            "loss on batch 103 in epoch 91 for the simple nn is: 0.46692460775375366\n",
            "loss on batch 104 in epoch 91 for the simple nn is: 0.39509978890419006\n",
            "loss on batch 105 in epoch 91 for the simple nn is: 0.40737640857696533\n",
            "loss on batch 106 in epoch 91 for the simple nn is: 0.5406471490859985\n",
            "loss on batch 107 in epoch 91 for the simple nn is: 0.4142203629016876\n",
            "loss on batch 108 in epoch 91 for the simple nn is: 0.4233461320400238\n",
            "loss on batch 109 in epoch 91 for the simple nn is: 0.5706449151039124\n",
            "loss on batch 110 in epoch 91 for the simple nn is: 0.33302199840545654\n",
            "loss on batch 111 in epoch 91 for the simple nn is: 0.41621023416519165\n",
            "loss on batch 112 in epoch 91 for the simple nn is: 0.45196181535720825\n",
            "loss on batch 113 in epoch 91 for the simple nn is: 0.48052355647087097\n",
            "loss on batch 114 in epoch 91 for the simple nn is: 0.524872899055481\n",
            "loss on batch 115 in epoch 91 for the simple nn is: 0.4207846224308014\n",
            "loss on batch 116 in epoch 91 for the simple nn is: 0.5517443418502808\n",
            "loss on batch 117 in epoch 91 for the simple nn is: 0.5210480093955994\n",
            "loss on batch 118 in epoch 91 for the simple nn is: 0.5406214594841003\n",
            "loss on batch 119 in epoch 91 for the simple nn is: 0.5123222470283508\n",
            "loss on batch 120 in epoch 91 for the simple nn is: 0.4685550034046173\n",
            "loss on batch 0 in epoch 92 for the simple nn is: 0.4974623918533325\n",
            "loss on batch 1 in epoch 92 for the simple nn is: 0.6426917910575867\n",
            "loss on batch 2 in epoch 92 for the simple nn is: 0.5499603152275085\n",
            "loss on batch 3 in epoch 92 for the simple nn is: 0.5340297222137451\n",
            "loss on batch 4 in epoch 92 for the simple nn is: 0.5502470135688782\n",
            "loss on batch 5 in epoch 92 for the simple nn is: 0.5122168660163879\n",
            "loss on batch 6 in epoch 92 for the simple nn is: 0.54973965883255\n",
            "loss on batch 7 in epoch 92 for the simple nn is: 0.47402462363243103\n",
            "loss on batch 8 in epoch 92 for the simple nn is: 0.4663154184818268\n",
            "loss on batch 9 in epoch 92 for the simple nn is: 0.472312867641449\n",
            "loss on batch 10 in epoch 92 for the simple nn is: 0.39485591650009155\n",
            "loss on batch 11 in epoch 92 for the simple nn is: 0.512852668762207\n",
            "loss on batch 12 in epoch 92 for the simple nn is: 0.5099617838859558\n",
            "loss on batch 13 in epoch 92 for the simple nn is: 0.5544245839118958\n",
            "loss on batch 14 in epoch 92 for the simple nn is: 0.46096906065940857\n",
            "loss on batch 15 in epoch 92 for the simple nn is: 0.42415183782577515\n",
            "loss on batch 16 in epoch 92 for the simple nn is: 0.46990060806274414\n",
            "loss on batch 17 in epoch 92 for the simple nn is: 0.45711904764175415\n",
            "loss on batch 18 in epoch 92 for the simple nn is: 0.4469405710697174\n",
            "loss on batch 19 in epoch 92 for the simple nn is: 0.4540122449398041\n",
            "loss on batch 20 in epoch 92 for the simple nn is: 0.4009367525577545\n",
            "loss on batch 21 in epoch 92 for the simple nn is: 0.4873356819152832\n",
            "loss on batch 22 in epoch 92 for the simple nn is: 0.4473586976528168\n",
            "loss on batch 23 in epoch 92 for the simple nn is: 0.41506627202033997\n",
            "loss on batch 24 in epoch 92 for the simple nn is: 0.36256784200668335\n",
            "loss on batch 25 in epoch 92 for the simple nn is: 0.42963707447052\n",
            "loss on batch 26 in epoch 92 for the simple nn is: 0.4846021831035614\n",
            "loss on batch 27 in epoch 92 for the simple nn is: 0.45902198553085327\n",
            "loss on batch 28 in epoch 92 for the simple nn is: 0.45508459210395813\n",
            "loss on batch 29 in epoch 92 for the simple nn is: 0.5482499003410339\n",
            "loss on batch 30 in epoch 92 for the simple nn is: 0.46816638112068176\n",
            "loss on batch 31 in epoch 92 for the simple nn is: 0.5819042325019836\n",
            "loss on batch 32 in epoch 92 for the simple nn is: 0.42626678943634033\n",
            "loss on batch 33 in epoch 92 for the simple nn is: 0.577467679977417\n",
            "loss on batch 34 in epoch 92 for the simple nn is: 0.4440326690673828\n",
            "loss on batch 35 in epoch 92 for the simple nn is: 0.46890243887901306\n",
            "loss on batch 36 in epoch 92 for the simple nn is: 0.4570717513561249\n",
            "loss on batch 37 in epoch 92 for the simple nn is: 0.4550701975822449\n",
            "loss on batch 38 in epoch 92 for the simple nn is: 0.5123047232627869\n",
            "loss on batch 39 in epoch 92 for the simple nn is: 0.3691631853580475\n",
            "loss on batch 40 in epoch 92 for the simple nn is: 0.47197669744491577\n",
            "loss on batch 41 in epoch 92 for the simple nn is: 0.43166452646255493\n",
            "loss on batch 42 in epoch 92 for the simple nn is: 0.4847775995731354\n",
            "loss on batch 43 in epoch 92 for the simple nn is: 0.48553144931793213\n",
            "loss on batch 44 in epoch 92 for the simple nn is: 0.5200408101081848\n",
            "loss on batch 45 in epoch 92 for the simple nn is: 0.44496795535087585\n",
            "loss on batch 46 in epoch 92 for the simple nn is: 0.4460800588130951\n",
            "loss on batch 47 in epoch 92 for the simple nn is: 0.41041308641433716\n",
            "loss on batch 48 in epoch 92 for the simple nn is: 0.47845980525016785\n",
            "loss on batch 49 in epoch 92 for the simple nn is: 0.5017076134681702\n",
            "loss on batch 50 in epoch 92 for the simple nn is: 0.45774319767951965\n",
            "loss on batch 51 in epoch 92 for the simple nn is: 0.4749665856361389\n",
            "loss on batch 52 in epoch 92 for the simple nn is: 0.3997988700866699\n",
            "loss on batch 53 in epoch 92 for the simple nn is: 0.35593315958976746\n",
            "loss on batch 54 in epoch 92 for the simple nn is: 0.5143725872039795\n",
            "loss on batch 55 in epoch 92 for the simple nn is: 0.4622083902359009\n",
            "loss on batch 56 in epoch 92 for the simple nn is: 0.467990517616272\n",
            "loss on batch 57 in epoch 92 for the simple nn is: 0.4842599630355835\n",
            "loss on batch 58 in epoch 92 for the simple nn is: 0.4234668016433716\n",
            "loss on batch 59 in epoch 92 for the simple nn is: 0.46098774671554565\n",
            "loss on batch 60 in epoch 92 for the simple nn is: 0.435538113117218\n",
            "loss on batch 61 in epoch 92 for the simple nn is: 0.4563235938549042\n",
            "loss on batch 62 in epoch 92 for the simple nn is: 0.4664192199707031\n",
            "loss on batch 63 in epoch 92 for the simple nn is: 0.5477180480957031\n",
            "loss on batch 64 in epoch 92 for the simple nn is: 0.4103292226791382\n",
            "loss on batch 65 in epoch 92 for the simple nn is: 0.4136599004268646\n",
            "loss on batch 66 in epoch 92 for the simple nn is: 0.4510347843170166\n",
            "loss on batch 67 in epoch 92 for the simple nn is: 0.3975788354873657\n",
            "loss on batch 68 in epoch 92 for the simple nn is: 0.4407316744327545\n",
            "loss on batch 69 in epoch 92 for the simple nn is: 0.4422411322593689\n",
            "loss on batch 70 in epoch 92 for the simple nn is: 0.44396013021469116\n",
            "loss on batch 71 in epoch 92 for the simple nn is: 0.48556551337242126\n",
            "loss on batch 72 in epoch 92 for the simple nn is: 0.4392971694469452\n",
            "loss on batch 73 in epoch 92 for the simple nn is: 0.5435918569564819\n",
            "loss on batch 74 in epoch 92 for the simple nn is: 0.5193880796432495\n",
            "loss on batch 75 in epoch 92 for the simple nn is: 0.4902642071247101\n",
            "loss on batch 76 in epoch 92 for the simple nn is: 0.47806331515312195\n",
            "loss on batch 77 in epoch 92 for the simple nn is: 0.40395063161849976\n",
            "loss on batch 78 in epoch 92 for the simple nn is: 0.6131330728530884\n",
            "loss on batch 79 in epoch 92 for the simple nn is: 0.3925916850566864\n",
            "loss on batch 80 in epoch 92 for the simple nn is: 0.45712387561798096\n",
            "loss on batch 81 in epoch 92 for the simple nn is: 0.3892713785171509\n",
            "loss on batch 82 in epoch 92 for the simple nn is: 0.4441404342651367\n",
            "loss on batch 83 in epoch 92 for the simple nn is: 0.49004775285720825\n",
            "loss on batch 84 in epoch 92 for the simple nn is: 0.49393129348754883\n",
            "loss on batch 85 in epoch 92 for the simple nn is: 0.4728355407714844\n",
            "loss on batch 86 in epoch 92 for the simple nn is: 0.3774489164352417\n",
            "loss on batch 87 in epoch 92 for the simple nn is: 0.4399715065956116\n",
            "loss on batch 88 in epoch 92 for the simple nn is: 0.3431284725666046\n",
            "loss on batch 89 in epoch 92 for the simple nn is: 0.4798477590084076\n",
            "loss on batch 90 in epoch 92 for the simple nn is: 0.4412636458873749\n",
            "loss on batch 91 in epoch 92 for the simple nn is: 0.40719425678253174\n",
            "loss on batch 92 in epoch 92 for the simple nn is: 0.5050061345100403\n",
            "loss on batch 93 in epoch 92 for the simple nn is: 0.44638174772262573\n",
            "loss on batch 94 in epoch 92 for the simple nn is: 0.3712380528450012\n",
            "loss on batch 95 in epoch 92 for the simple nn is: 0.5064584016799927\n",
            "loss on batch 96 in epoch 92 for the simple nn is: 0.4991755187511444\n",
            "loss on batch 97 in epoch 92 for the simple nn is: 0.5421303510665894\n",
            "loss on batch 98 in epoch 92 for the simple nn is: 0.43954652547836304\n",
            "loss on batch 99 in epoch 92 for the simple nn is: 0.4427308738231659\n",
            "loss on batch 100 in epoch 92 for the simple nn is: 0.5260850787162781\n",
            "loss on batch 101 in epoch 92 for the simple nn is: 0.40492433309555054\n",
            "loss on batch 102 in epoch 92 for the simple nn is: 0.36057281494140625\n",
            "loss on batch 103 in epoch 92 for the simple nn is: 0.46533364057540894\n",
            "loss on batch 104 in epoch 92 for the simple nn is: 0.392032265663147\n",
            "loss on batch 105 in epoch 92 for the simple nn is: 0.41033437848091125\n",
            "loss on batch 106 in epoch 92 for the simple nn is: 0.47068673372268677\n",
            "loss on batch 107 in epoch 92 for the simple nn is: 0.4202393591403961\n",
            "loss on batch 108 in epoch 92 for the simple nn is: 0.4055001139640808\n",
            "loss on batch 109 in epoch 92 for the simple nn is: 0.5208947658538818\n",
            "loss on batch 110 in epoch 92 for the simple nn is: 0.34630119800567627\n",
            "loss on batch 111 in epoch 92 for the simple nn is: 0.5058013796806335\n",
            "loss on batch 112 in epoch 92 for the simple nn is: 0.47469690442085266\n",
            "loss on batch 113 in epoch 92 for the simple nn is: 0.47019949555397034\n",
            "loss on batch 114 in epoch 92 for the simple nn is: 0.6920403838157654\n",
            "loss on batch 115 in epoch 92 for the simple nn is: 0.5976967215538025\n",
            "loss on batch 116 in epoch 92 for the simple nn is: 0.5374928116798401\n",
            "loss on batch 117 in epoch 92 for the simple nn is: 0.5183784365653992\n",
            "loss on batch 118 in epoch 92 for the simple nn is: 0.5884916186332703\n",
            "loss on batch 119 in epoch 92 for the simple nn is: 0.506583571434021\n",
            "loss on batch 120 in epoch 92 for the simple nn is: 0.46124267578125\n",
            "loss on batch 0 in epoch 93 for the simple nn is: 0.48863422870635986\n",
            "loss on batch 1 in epoch 93 for the simple nn is: 0.606259286403656\n",
            "loss on batch 2 in epoch 93 for the simple nn is: 0.5515123605728149\n",
            "loss on batch 3 in epoch 93 for the simple nn is: 0.5794909000396729\n",
            "loss on batch 4 in epoch 93 for the simple nn is: 0.5517844557762146\n",
            "loss on batch 5 in epoch 93 for the simple nn is: 0.5467718243598938\n",
            "loss on batch 6 in epoch 93 for the simple nn is: 0.5689855813980103\n",
            "loss on batch 7 in epoch 93 for the simple nn is: 0.47192636132240295\n",
            "loss on batch 8 in epoch 93 for the simple nn is: 0.5848336219787598\n",
            "loss on batch 9 in epoch 93 for the simple nn is: 0.48054444789886475\n",
            "loss on batch 10 in epoch 93 for the simple nn is: 0.4301493763923645\n",
            "loss on batch 11 in epoch 93 for the simple nn is: 0.486587256193161\n",
            "loss on batch 12 in epoch 93 for the simple nn is: 0.5074344873428345\n",
            "loss on batch 13 in epoch 93 for the simple nn is: 0.4964158236980438\n",
            "loss on batch 14 in epoch 93 for the simple nn is: 0.5106838941574097\n",
            "loss on batch 15 in epoch 93 for the simple nn is: 0.47218483686447144\n",
            "loss on batch 16 in epoch 93 for the simple nn is: 0.48043590784072876\n",
            "loss on batch 17 in epoch 93 for the simple nn is: 0.47962456941604614\n",
            "loss on batch 18 in epoch 93 for the simple nn is: 0.46764591336250305\n",
            "loss on batch 19 in epoch 93 for the simple nn is: 0.5224876999855042\n",
            "loss on batch 20 in epoch 93 for the simple nn is: 0.43011003732681274\n",
            "loss on batch 21 in epoch 93 for the simple nn is: 0.5300047397613525\n",
            "loss on batch 22 in epoch 93 for the simple nn is: 0.4451808035373688\n",
            "loss on batch 23 in epoch 93 for the simple nn is: 0.4462619423866272\n",
            "loss on batch 24 in epoch 93 for the simple nn is: 0.3921193480491638\n",
            "loss on batch 25 in epoch 93 for the simple nn is: 0.44030216336250305\n",
            "loss on batch 26 in epoch 93 for the simple nn is: 0.48710253834724426\n",
            "loss on batch 27 in epoch 93 for the simple nn is: 0.6024818420410156\n",
            "loss on batch 28 in epoch 93 for the simple nn is: 0.4741747975349426\n",
            "loss on batch 29 in epoch 93 for the simple nn is: 0.5808500647544861\n",
            "loss on batch 30 in epoch 93 for the simple nn is: 0.4723730683326721\n",
            "loss on batch 31 in epoch 93 for the simple nn is: 0.5309842228889465\n",
            "loss on batch 32 in epoch 93 for the simple nn is: 0.43268996477127075\n",
            "loss on batch 33 in epoch 93 for the simple nn is: 0.4577101469039917\n",
            "loss on batch 34 in epoch 93 for the simple nn is: 0.5182633399963379\n",
            "loss on batch 35 in epoch 93 for the simple nn is: 0.45514774322509766\n",
            "loss on batch 36 in epoch 93 for the simple nn is: 0.46221232414245605\n",
            "loss on batch 37 in epoch 93 for the simple nn is: 0.42184093594551086\n",
            "loss on batch 38 in epoch 93 for the simple nn is: 0.5021733045578003\n",
            "loss on batch 39 in epoch 93 for the simple nn is: 0.3958621621131897\n",
            "loss on batch 40 in epoch 93 for the simple nn is: 0.46879592537879944\n",
            "loss on batch 41 in epoch 93 for the simple nn is: 0.4462709426879883\n",
            "loss on batch 42 in epoch 93 for the simple nn is: 0.48540884256362915\n",
            "loss on batch 43 in epoch 93 for the simple nn is: 0.4721910357475281\n",
            "loss on batch 44 in epoch 93 for the simple nn is: 0.5208354592323303\n",
            "loss on batch 45 in epoch 93 for the simple nn is: 0.45418161153793335\n",
            "loss on batch 46 in epoch 93 for the simple nn is: 0.44710397720336914\n",
            "loss on batch 47 in epoch 93 for the simple nn is: 0.5645151734352112\n",
            "loss on batch 48 in epoch 93 for the simple nn is: 0.5556670427322388\n",
            "loss on batch 49 in epoch 93 for the simple nn is: 0.5147532820701599\n",
            "loss on batch 50 in epoch 93 for the simple nn is: 0.4579189717769623\n",
            "loss on batch 51 in epoch 93 for the simple nn is: 0.47395145893096924\n",
            "loss on batch 52 in epoch 93 for the simple nn is: 0.4797109365463257\n",
            "loss on batch 53 in epoch 93 for the simple nn is: 0.47968703508377075\n",
            "loss on batch 54 in epoch 93 for the simple nn is: 0.5180718898773193\n",
            "loss on batch 55 in epoch 93 for the simple nn is: 0.46875977516174316\n",
            "loss on batch 56 in epoch 93 for the simple nn is: 0.5305189490318298\n",
            "loss on batch 57 in epoch 93 for the simple nn is: 0.49415475130081177\n",
            "loss on batch 58 in epoch 93 for the simple nn is: 0.42962488532066345\n",
            "loss on batch 59 in epoch 93 for the simple nn is: 0.49159157276153564\n",
            "loss on batch 60 in epoch 93 for the simple nn is: 0.4644203186035156\n",
            "loss on batch 61 in epoch 93 for the simple nn is: 0.45926937460899353\n",
            "loss on batch 62 in epoch 93 for the simple nn is: 0.46708956360816956\n",
            "loss on batch 63 in epoch 93 for the simple nn is: 0.4720028340816498\n",
            "loss on batch 64 in epoch 93 for the simple nn is: 0.4352883994579315\n",
            "loss on batch 65 in epoch 93 for the simple nn is: 0.548326313495636\n",
            "loss on batch 66 in epoch 93 for the simple nn is: 0.4608861804008484\n",
            "loss on batch 67 in epoch 93 for the simple nn is: 0.3946264684200287\n",
            "loss on batch 68 in epoch 93 for the simple nn is: 0.40393272042274475\n",
            "loss on batch 69 in epoch 93 for the simple nn is: 0.43991950154304504\n",
            "loss on batch 70 in epoch 93 for the simple nn is: 0.46049293875694275\n",
            "loss on batch 71 in epoch 93 for the simple nn is: 0.4879356324672699\n",
            "loss on batch 72 in epoch 93 for the simple nn is: 0.5092470049858093\n",
            "loss on batch 73 in epoch 93 for the simple nn is: 0.5516444444656372\n",
            "loss on batch 74 in epoch 93 for the simple nn is: 0.4729236960411072\n",
            "loss on batch 75 in epoch 93 for the simple nn is: 0.5169317126274109\n",
            "loss on batch 76 in epoch 93 for the simple nn is: 0.4674716591835022\n",
            "loss on batch 77 in epoch 93 for the simple nn is: 0.431315541267395\n",
            "loss on batch 78 in epoch 93 for the simple nn is: 0.4849962294101715\n",
            "loss on batch 79 in epoch 93 for the simple nn is: 0.4493279755115509\n",
            "loss on batch 80 in epoch 93 for the simple nn is: 0.4465237259864807\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-285-f42a6d6c01b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_epochs_simple_nn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mtrain_combo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimple_nn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_for_simple_nn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_train_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-277-009dc33bb0c7>\u001b[0m in \u001b[0;36mtrain_combo\u001b[0;34m(rnn, simple_nn, optimizer_for_simple_nn, rec_train_features, word_count_model, train_labels, batch_size, epoch)\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0moptimizer_for_simple_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'loss on batch {batch_idx} in epoch {epoch} for the simple nn is: {loss}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LFRUk3UXOQI"
      },
      "source": [
        "torch.save(simple_nn.state_dict(), basepath+'simple_nn_epoch3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cg3bXuO0421",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "43f83203-6fd5-44d4-adff-d3c39631a0ce"
      },
      "source": [
        "test_combo(network, simple_nn, rec_val_features, cv, validation_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 1, 0,  ..., 0, 0, 0])\n",
            "tensor([0, 0, 1,  ..., 0, 1, 0])\n",
            "test loss for the simple nn is: 0.47891566157341003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.47891566157341003"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 286
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ-RWalj6W9y"
      },
      "source": [
        "#MISC: A pre-made model: sklearn SVC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0hGQeohP_y8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1af4eebd-2780-47d6-cbf2-4240f3e49051"
      },
      "source": [
        "import sklearn\n",
        "premodel=sklearn.svm.SVC(degree=2)\n",
        "premodel.fit(word_count_train, train_labels)\n",
        "print(premodel.score(word_count_train, train_labels))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9984530101843496\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7fn_AWidd_p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9bd62d1e-07e7-46c4-d4eb-f0d57c8f49a3"
      },
      "source": [
        "print(premodel.score(word_count_validation, validation_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6995481927710844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMg_6XxbnSmz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3a28b4df-68f2-4810-cdc6-80728b843f21"
      },
      "source": [
        "import sklearn\n",
        "premodel_w2v=sklearn.svm.SVC()\n",
        "premodel_w2v.fit(train_features[:1000], train_labels[:1000])\n",
        "print(premodel_w2v.score(train_features, train_labels))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5645223668944179\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6rTsxpunUC2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "da5c2ff5-2ba8-4082-d394-ed48bf2debc5"
      },
      "source": [
        "print(premodel_w2v.score(validation_features, validation_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.545933734939759\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ker_onr3lv_n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "69fca186-ce9a-4852-e961-10c7c8f98456"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "#fit prior is false because classes are uniformly distributed both on train and on validation\n",
        "premodel_multinomial_NB=MultinomialNB(alpha=0.01, fit_prior=False)\n",
        "premodel_multinomial_NB.fit(word_count_train, train_labels)\n",
        "print(premodel_multinomial_NB.score(word_count_train, train_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9756349104035065\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29dGalw8mc5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "88db9f9f-959e-45c2-f313-7a6a2ee740b8"
      },
      "source": [
        "print(f'scorul pe datele de validare: {premodel_multinomial_NB.score(word_count_validation, validation_labels)}')\n",
        "val_prediction=premodel_multinomial_NB.predict(word_count_validation)\n",
        "print(f'scorul F1 pe datele de validare: {f1_score(validation_labels, val_prediction)}')\n",
        "print(f'matricea de confuzie pe datele de validare: {confusion_matrix(validation_labels, val_prediction)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "scorul pe datele de validare: 0.7198795180722891\n",
            "scorul F1 pe datele de validare: 0.7296511627906976\n",
            "matricea de confuzie pe datele de validare: [[ 908  393]\n",
            " [ 351 1004]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRj8wxsq7iaJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a85c55e5-e3af-437f-d555-c34e2d22720f"
      },
      "source": [
        "print(word_count_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7757, 37101)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZWPiA8Fm_ke",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a38091e-4599-43f0-dda8-edb93f05b202"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "#fit prior is false because classes are uniformly distributed both on train and on validation\n",
        "premodel_multinomial_NB=MultinomialNB(alpha=0.01, fit_prior=False)\n",
        "premodel_multinomial_NB.fit(word_count_train_ngrams, train_labels)\n",
        "print(premodel_multinomial_NB.score(word_count_train_ngrams, train_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9739590047698853\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P59CehVdnBH2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ecd4a2d8-f115-4925-88eb-a3f79b285305"
      },
      "source": [
        "print(premodel_multinomial_NB.score(word_count_validation_ngrams, validation_labels))\n",
        "val_prediction=premodel_multinomial_NB.predict(word_count_validation_ngrams)\n",
        "print(f'scorul F1 pe datele de validare: {f1_score(validation_labels, val_prediction)}')\n",
        "print(f'matricea de confuzie pe datele de validare: {confusion_matrix(validation_labels, val_prediction)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7127259036144579\n",
            "scorul F1 pe datele de validare: 0.7242500903505601\n",
            "matricea de confuzie pe datele de validare: [[ 891  410]\n",
            " [ 353 1002]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvvfFSnhZWTB"
      },
      "source": [
        "import pickle\n",
        "pickle.dump(premodel, open(basepath+'svm_model_train_defaultparams', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgAMWsA3eCNd"
      },
      "source": [
        "premodel = pickle.load(open(basepath+'svm_model_train_defaultparams', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrfn0nXqupx2"
      },
      "source": [
        "test_ids=[key for key in test_data.keys()]\n",
        "word_count_test=torch.Tensor(cv.transform([test_data[key] for key in test_data.keys()]).todense())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeFruLg6eMfv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "257bcf5f-5483-490b-e12d-2956ff26f01f"
      },
      "source": [
        "pred_test=premodel_multinomial_NB.predict(word_count_test)\n",
        "print(pred_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 ... 1 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbHhenNDgHCY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "81d0ccfa-ce57-48f2-be31-700ca7d38b11"
      },
      "source": [
        "arr_test=np.zeros((len(test_ids),  2), dtype=np.int64)\n",
        "for i in range(len(test_data.keys())):\n",
        "  arr_test[i, 0]=np.array(test_ids[i], dtype=np.int64)\n",
        "  arr_test[i, 1]=np.array(pred_test[i], dtype=np.int64)\n",
        "print(arr_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[110499      0]\n",
            " [101319      0]\n",
            " [108883      0]\n",
            " ...\n",
            " [103098      1]\n",
            " [107046      1]\n",
            " [108263      0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae2e4HZ2h-yk"
      },
      "source": [
        "arr_test=np.zeros((len(test_ids),  2), dtype=np.int64)\n",
        "for i in range(len(test_data.keys())):\n",
        "  arr_test[i, 0]=np.array(test_ids[i], dtype=np.int64)\n",
        "  arr_test[i, 1]=np.array(pred_test[i], dtype=np.int64)\n",
        "print(arr_test)\n",
        "np.savetxt(\"predictii.csv\", arr_test, delimiter=\",\", header='id,label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoqVLX76JI82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "545968c3-dae4-4d89-ad4a-747adce8827b"
      },
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "premodel_bernoulli=BernoulliNB(alpha=0.001, binarize=0.01, fit_prior=False, class_prior=None)\n",
        "premodel_bernoulli.fit(word_count_train, train_labels)\n",
        "print(premodel_bernoulli.score(word_count_train, train_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.932190279747325\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqMltaydNlFK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c9c6e94a-01d7-48ce-fa7c-0bb59b55b515"
      },
      "source": [
        "print(premodel_bernoulli.score(word_count_validation, validation_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.697289156626506\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTMnzuFmNr9Z"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}